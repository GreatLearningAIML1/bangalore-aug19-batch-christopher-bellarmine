{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Christopher_R6_Internal_AIML_Lab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zUZjPnVXGz0Z"
      },
      "source": [
        "# The Iris Dataset\n",
        "The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters.\n",
        "\n",
        "The dataset contains a set of 150 records under five attributes - petal length, petal width, sepal length, sepal width and species."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RMbmpriavLE9"
      },
      "source": [
        "### Specifying the TensorFlow version\n",
        "Running `import tensorflow` will import the default version (currently 1.x). You can use 2.x by running a cell with the `tensorflow_version` magic **before** you run `import tensorflow`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fu8bUU__oa7h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1268a548-c601-42e4-9464-501d359828a6"
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bLz1Ckvfvn6D"
      },
      "source": [
        "### Import TensorFlow\n",
        "Once you have specified a version via this magic, you can run `import tensorflow` as normal and verify which version was imported as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CWrzVTLOvn6M",
        "outputId": "e4d93a95-f183-43b4-897c-aee453977de0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_uYeJgkNuXNC"
      },
      "source": [
        "### Set random seed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lcASNsewsfQX",
        "colab": {}
      },
      "source": [
        "tf.random.set_seed(47)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5-vVQBBqg7DI"
      },
      "source": [
        "## Question 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kE0EDKvQhEIe"
      },
      "source": [
        "### Import dataset\n",
        "- Import iris dataset\n",
        "- Import the dataset using sklearn library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IOOWpD26Haq3",
        "colab": {}
      },
      "source": [
        "# Import pandas \n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Bds4JgOleorf",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "iris_data = load_iris()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DEzgL7sGfDA2",
        "outputId": "c4c71c33-3390-468b-f19b-8b7010887a63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "iris_data"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...',\n",
              " 'data': array([[5.1, 3.5, 1.4, 0.2],\n",
              "        [4.9, 3. , 1.4, 0.2],\n",
              "        [4.7, 3.2, 1.3, 0.2],\n",
              "        [4.6, 3.1, 1.5, 0.2],\n",
              "        [5. , 3.6, 1.4, 0.2],\n",
              "        [5.4, 3.9, 1.7, 0.4],\n",
              "        [4.6, 3.4, 1.4, 0.3],\n",
              "        [5. , 3.4, 1.5, 0.2],\n",
              "        [4.4, 2.9, 1.4, 0.2],\n",
              "        [4.9, 3.1, 1.5, 0.1],\n",
              "        [5.4, 3.7, 1.5, 0.2],\n",
              "        [4.8, 3.4, 1.6, 0.2],\n",
              "        [4.8, 3. , 1.4, 0.1],\n",
              "        [4.3, 3. , 1.1, 0.1],\n",
              "        [5.8, 4. , 1.2, 0.2],\n",
              "        [5.7, 4.4, 1.5, 0.4],\n",
              "        [5.4, 3.9, 1.3, 0.4],\n",
              "        [5.1, 3.5, 1.4, 0.3],\n",
              "        [5.7, 3.8, 1.7, 0.3],\n",
              "        [5.1, 3.8, 1.5, 0.3],\n",
              "        [5.4, 3.4, 1.7, 0.2],\n",
              "        [5.1, 3.7, 1.5, 0.4],\n",
              "        [4.6, 3.6, 1. , 0.2],\n",
              "        [5.1, 3.3, 1.7, 0.5],\n",
              "        [4.8, 3.4, 1.9, 0.2],\n",
              "        [5. , 3. , 1.6, 0.2],\n",
              "        [5. , 3.4, 1.6, 0.4],\n",
              "        [5.2, 3.5, 1.5, 0.2],\n",
              "        [5.2, 3.4, 1.4, 0.2],\n",
              "        [4.7, 3.2, 1.6, 0.2],\n",
              "        [4.8, 3.1, 1.6, 0.2],\n",
              "        [5.4, 3.4, 1.5, 0.4],\n",
              "        [5.2, 4.1, 1.5, 0.1],\n",
              "        [5.5, 4.2, 1.4, 0.2],\n",
              "        [4.9, 3.1, 1.5, 0.2],\n",
              "        [5. , 3.2, 1.2, 0.2],\n",
              "        [5.5, 3.5, 1.3, 0.2],\n",
              "        [4.9, 3.6, 1.4, 0.1],\n",
              "        [4.4, 3. , 1.3, 0.2],\n",
              "        [5.1, 3.4, 1.5, 0.2],\n",
              "        [5. , 3.5, 1.3, 0.3],\n",
              "        [4.5, 2.3, 1.3, 0.3],\n",
              "        [4.4, 3.2, 1.3, 0.2],\n",
              "        [5. , 3.5, 1.6, 0.6],\n",
              "        [5.1, 3.8, 1.9, 0.4],\n",
              "        [4.8, 3. , 1.4, 0.3],\n",
              "        [5.1, 3.8, 1.6, 0.2],\n",
              "        [4.6, 3.2, 1.4, 0.2],\n",
              "        [5.3, 3.7, 1.5, 0.2],\n",
              "        [5. , 3.3, 1.4, 0.2],\n",
              "        [7. , 3.2, 4.7, 1.4],\n",
              "        [6.4, 3.2, 4.5, 1.5],\n",
              "        [6.9, 3.1, 4.9, 1.5],\n",
              "        [5.5, 2.3, 4. , 1.3],\n",
              "        [6.5, 2.8, 4.6, 1.5],\n",
              "        [5.7, 2.8, 4.5, 1.3],\n",
              "        [6.3, 3.3, 4.7, 1.6],\n",
              "        [4.9, 2.4, 3.3, 1. ],\n",
              "        [6.6, 2.9, 4.6, 1.3],\n",
              "        [5.2, 2.7, 3.9, 1.4],\n",
              "        [5. , 2. , 3.5, 1. ],\n",
              "        [5.9, 3. , 4.2, 1.5],\n",
              "        [6. , 2.2, 4. , 1. ],\n",
              "        [6.1, 2.9, 4.7, 1.4],\n",
              "        [5.6, 2.9, 3.6, 1.3],\n",
              "        [6.7, 3.1, 4.4, 1.4],\n",
              "        [5.6, 3. , 4.5, 1.5],\n",
              "        [5.8, 2.7, 4.1, 1. ],\n",
              "        [6.2, 2.2, 4.5, 1.5],\n",
              "        [5.6, 2.5, 3.9, 1.1],\n",
              "        [5.9, 3.2, 4.8, 1.8],\n",
              "        [6.1, 2.8, 4. , 1.3],\n",
              "        [6.3, 2.5, 4.9, 1.5],\n",
              "        [6.1, 2.8, 4.7, 1.2],\n",
              "        [6.4, 2.9, 4.3, 1.3],\n",
              "        [6.6, 3. , 4.4, 1.4],\n",
              "        [6.8, 2.8, 4.8, 1.4],\n",
              "        [6.7, 3. , 5. , 1.7],\n",
              "        [6. , 2.9, 4.5, 1.5],\n",
              "        [5.7, 2.6, 3.5, 1. ],\n",
              "        [5.5, 2.4, 3.8, 1.1],\n",
              "        [5.5, 2.4, 3.7, 1. ],\n",
              "        [5.8, 2.7, 3.9, 1.2],\n",
              "        [6. , 2.7, 5.1, 1.6],\n",
              "        [5.4, 3. , 4.5, 1.5],\n",
              "        [6. , 3.4, 4.5, 1.6],\n",
              "        [6.7, 3.1, 4.7, 1.5],\n",
              "        [6.3, 2.3, 4.4, 1.3],\n",
              "        [5.6, 3. , 4.1, 1.3],\n",
              "        [5.5, 2.5, 4. , 1.3],\n",
              "        [5.5, 2.6, 4.4, 1.2],\n",
              "        [6.1, 3. , 4.6, 1.4],\n",
              "        [5.8, 2.6, 4. , 1.2],\n",
              "        [5. , 2.3, 3.3, 1. ],\n",
              "        [5.6, 2.7, 4.2, 1.3],\n",
              "        [5.7, 3. , 4.2, 1.2],\n",
              "        [5.7, 2.9, 4.2, 1.3],\n",
              "        [6.2, 2.9, 4.3, 1.3],\n",
              "        [5.1, 2.5, 3. , 1.1],\n",
              "        [5.7, 2.8, 4.1, 1.3],\n",
              "        [6.3, 3.3, 6. , 2.5],\n",
              "        [5.8, 2.7, 5.1, 1.9],\n",
              "        [7.1, 3. , 5.9, 2.1],\n",
              "        [6.3, 2.9, 5.6, 1.8],\n",
              "        [6.5, 3. , 5.8, 2.2],\n",
              "        [7.6, 3. , 6.6, 2.1],\n",
              "        [4.9, 2.5, 4.5, 1.7],\n",
              "        [7.3, 2.9, 6.3, 1.8],\n",
              "        [6.7, 2.5, 5.8, 1.8],\n",
              "        [7.2, 3.6, 6.1, 2.5],\n",
              "        [6.5, 3.2, 5.1, 2. ],\n",
              "        [6.4, 2.7, 5.3, 1.9],\n",
              "        [6.8, 3. , 5.5, 2.1],\n",
              "        [5.7, 2.5, 5. , 2. ],\n",
              "        [5.8, 2.8, 5.1, 2.4],\n",
              "        [6.4, 3.2, 5.3, 2.3],\n",
              "        [6.5, 3. , 5.5, 1.8],\n",
              "        [7.7, 3.8, 6.7, 2.2],\n",
              "        [7.7, 2.6, 6.9, 2.3],\n",
              "        [6. , 2.2, 5. , 1.5],\n",
              "        [6.9, 3.2, 5.7, 2.3],\n",
              "        [5.6, 2.8, 4.9, 2. ],\n",
              "        [7.7, 2.8, 6.7, 2. ],\n",
              "        [6.3, 2.7, 4.9, 1.8],\n",
              "        [6.7, 3.3, 5.7, 2.1],\n",
              "        [7.2, 3.2, 6. , 1.8],\n",
              "        [6.2, 2.8, 4.8, 1.8],\n",
              "        [6.1, 3. , 4.9, 1.8],\n",
              "        [6.4, 2.8, 5.6, 2.1],\n",
              "        [7.2, 3. , 5.8, 1.6],\n",
              "        [7.4, 2.8, 6.1, 1.9],\n",
              "        [7.9, 3.8, 6.4, 2. ],\n",
              "        [6.4, 2.8, 5.6, 2.2],\n",
              "        [6.3, 2.8, 5.1, 1.5],\n",
              "        [6.1, 2.6, 5.6, 1.4],\n",
              "        [7.7, 3. , 6.1, 2.3],\n",
              "        [6.3, 3.4, 5.6, 2.4],\n",
              "        [6.4, 3.1, 5.5, 1.8],\n",
              "        [6. , 3. , 4.8, 1.8],\n",
              "        [6.9, 3.1, 5.4, 2.1],\n",
              "        [6.7, 3.1, 5.6, 2.4],\n",
              "        [6.9, 3.1, 5.1, 2.3],\n",
              "        [5.8, 2.7, 5.1, 1.9],\n",
              "        [6.8, 3.2, 5.9, 2.3],\n",
              "        [6.7, 3.3, 5.7, 2.5],\n",
              "        [6.7, 3. , 5.2, 2.3],\n",
              "        [6.3, 2.5, 5. , 1.9],\n",
              "        [6.5, 3. , 5.2, 2. ],\n",
              "        [6.2, 3.4, 5.4, 2.3],\n",
              "        [5.9, 3. , 5.1, 1.8]]),\n",
              " 'feature_names': ['sepal length (cm)',\n",
              "  'sepal width (cm)',\n",
              "  'petal length (cm)',\n",
              "  'petal width (cm)'],\n",
              " 'filename': '/usr/local/lib/python3.6/dist-packages/sklearn/datasets/data/iris.csv',\n",
              " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
              " 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10')}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ta8YqInTh5v5"
      },
      "source": [
        "## Question 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HERt3drbhX0i"
      },
      "source": [
        "### Get features and label from the dataset in separate variable\n",
        "- you can get the features using .data method\n",
        "- you can get the features using .target method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0cV-_qHAHyvE",
        "colab": {}
      },
      "source": [
        "X = iris_data.data   #independent columns\n",
        "Y = iris_data.target #target column"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "znyRuv5-hYpL",
        "outputId": "c0c0e255-52fb-405f-f1d3-755794a55937",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "X"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.1, 3.5, 1.4, 0.2],\n",
              "       [4.9, 3. , 1.4, 0.2],\n",
              "       [4.7, 3.2, 1.3, 0.2],\n",
              "       [4.6, 3.1, 1.5, 0.2],\n",
              "       [5. , 3.6, 1.4, 0.2],\n",
              "       [5.4, 3.9, 1.7, 0.4],\n",
              "       [4.6, 3.4, 1.4, 0.3],\n",
              "       [5. , 3.4, 1.5, 0.2],\n",
              "       [4.4, 2.9, 1.4, 0.2],\n",
              "       [4.9, 3.1, 1.5, 0.1],\n",
              "       [5.4, 3.7, 1.5, 0.2],\n",
              "       [4.8, 3.4, 1.6, 0.2],\n",
              "       [4.8, 3. , 1.4, 0.1],\n",
              "       [4.3, 3. , 1.1, 0.1],\n",
              "       [5.8, 4. , 1.2, 0.2],\n",
              "       [5.7, 4.4, 1.5, 0.4],\n",
              "       [5.4, 3.9, 1.3, 0.4],\n",
              "       [5.1, 3.5, 1.4, 0.3],\n",
              "       [5.7, 3.8, 1.7, 0.3],\n",
              "       [5.1, 3.8, 1.5, 0.3],\n",
              "       [5.4, 3.4, 1.7, 0.2],\n",
              "       [5.1, 3.7, 1.5, 0.4],\n",
              "       [4.6, 3.6, 1. , 0.2],\n",
              "       [5.1, 3.3, 1.7, 0.5],\n",
              "       [4.8, 3.4, 1.9, 0.2],\n",
              "       [5. , 3. , 1.6, 0.2],\n",
              "       [5. , 3.4, 1.6, 0.4],\n",
              "       [5.2, 3.5, 1.5, 0.2],\n",
              "       [5.2, 3.4, 1.4, 0.2],\n",
              "       [4.7, 3.2, 1.6, 0.2],\n",
              "       [4.8, 3.1, 1.6, 0.2],\n",
              "       [5.4, 3.4, 1.5, 0.4],\n",
              "       [5.2, 4.1, 1.5, 0.1],\n",
              "       [5.5, 4.2, 1.4, 0.2],\n",
              "       [4.9, 3.1, 1.5, 0.2],\n",
              "       [5. , 3.2, 1.2, 0.2],\n",
              "       [5.5, 3.5, 1.3, 0.2],\n",
              "       [4.9, 3.6, 1.4, 0.1],\n",
              "       [4.4, 3. , 1.3, 0.2],\n",
              "       [5.1, 3.4, 1.5, 0.2],\n",
              "       [5. , 3.5, 1.3, 0.3],\n",
              "       [4.5, 2.3, 1.3, 0.3],\n",
              "       [4.4, 3.2, 1.3, 0.2],\n",
              "       [5. , 3.5, 1.6, 0.6],\n",
              "       [5.1, 3.8, 1.9, 0.4],\n",
              "       [4.8, 3. , 1.4, 0.3],\n",
              "       [5.1, 3.8, 1.6, 0.2],\n",
              "       [4.6, 3.2, 1.4, 0.2],\n",
              "       [5.3, 3.7, 1.5, 0.2],\n",
              "       [5. , 3.3, 1.4, 0.2],\n",
              "       [7. , 3.2, 4.7, 1.4],\n",
              "       [6.4, 3.2, 4.5, 1.5],\n",
              "       [6.9, 3.1, 4.9, 1.5],\n",
              "       [5.5, 2.3, 4. , 1.3],\n",
              "       [6.5, 2.8, 4.6, 1.5],\n",
              "       [5.7, 2.8, 4.5, 1.3],\n",
              "       [6.3, 3.3, 4.7, 1.6],\n",
              "       [4.9, 2.4, 3.3, 1. ],\n",
              "       [6.6, 2.9, 4.6, 1.3],\n",
              "       [5.2, 2.7, 3.9, 1.4],\n",
              "       [5. , 2. , 3.5, 1. ],\n",
              "       [5.9, 3. , 4.2, 1.5],\n",
              "       [6. , 2.2, 4. , 1. ],\n",
              "       [6.1, 2.9, 4.7, 1.4],\n",
              "       [5.6, 2.9, 3.6, 1.3],\n",
              "       [6.7, 3.1, 4.4, 1.4],\n",
              "       [5.6, 3. , 4.5, 1.5],\n",
              "       [5.8, 2.7, 4.1, 1. ],\n",
              "       [6.2, 2.2, 4.5, 1.5],\n",
              "       [5.6, 2.5, 3.9, 1.1],\n",
              "       [5.9, 3.2, 4.8, 1.8],\n",
              "       [6.1, 2.8, 4. , 1.3],\n",
              "       [6.3, 2.5, 4.9, 1.5],\n",
              "       [6.1, 2.8, 4.7, 1.2],\n",
              "       [6.4, 2.9, 4.3, 1.3],\n",
              "       [6.6, 3. , 4.4, 1.4],\n",
              "       [6.8, 2.8, 4.8, 1.4],\n",
              "       [6.7, 3. , 5. , 1.7],\n",
              "       [6. , 2.9, 4.5, 1.5],\n",
              "       [5.7, 2.6, 3.5, 1. ],\n",
              "       [5.5, 2.4, 3.8, 1.1],\n",
              "       [5.5, 2.4, 3.7, 1. ],\n",
              "       [5.8, 2.7, 3.9, 1.2],\n",
              "       [6. , 2.7, 5.1, 1.6],\n",
              "       [5.4, 3. , 4.5, 1.5],\n",
              "       [6. , 3.4, 4.5, 1.6],\n",
              "       [6.7, 3.1, 4.7, 1.5],\n",
              "       [6.3, 2.3, 4.4, 1.3],\n",
              "       [5.6, 3. , 4.1, 1.3],\n",
              "       [5.5, 2.5, 4. , 1.3],\n",
              "       [5.5, 2.6, 4.4, 1.2],\n",
              "       [6.1, 3. , 4.6, 1.4],\n",
              "       [5.8, 2.6, 4. , 1.2],\n",
              "       [5. , 2.3, 3.3, 1. ],\n",
              "       [5.6, 2.7, 4.2, 1.3],\n",
              "       [5.7, 3. , 4.2, 1.2],\n",
              "       [5.7, 2.9, 4.2, 1.3],\n",
              "       [6.2, 2.9, 4.3, 1.3],\n",
              "       [5.1, 2.5, 3. , 1.1],\n",
              "       [5.7, 2.8, 4.1, 1.3],\n",
              "       [6.3, 3.3, 6. , 2.5],\n",
              "       [5.8, 2.7, 5.1, 1.9],\n",
              "       [7.1, 3. , 5.9, 2.1],\n",
              "       [6.3, 2.9, 5.6, 1.8],\n",
              "       [6.5, 3. , 5.8, 2.2],\n",
              "       [7.6, 3. , 6.6, 2.1],\n",
              "       [4.9, 2.5, 4.5, 1.7],\n",
              "       [7.3, 2.9, 6.3, 1.8],\n",
              "       [6.7, 2.5, 5.8, 1.8],\n",
              "       [7.2, 3.6, 6.1, 2.5],\n",
              "       [6.5, 3.2, 5.1, 2. ],\n",
              "       [6.4, 2.7, 5.3, 1.9],\n",
              "       [6.8, 3. , 5.5, 2.1],\n",
              "       [5.7, 2.5, 5. , 2. ],\n",
              "       [5.8, 2.8, 5.1, 2.4],\n",
              "       [6.4, 3.2, 5.3, 2.3],\n",
              "       [6.5, 3. , 5.5, 1.8],\n",
              "       [7.7, 3.8, 6.7, 2.2],\n",
              "       [7.7, 2.6, 6.9, 2.3],\n",
              "       [6. , 2.2, 5. , 1.5],\n",
              "       [6.9, 3.2, 5.7, 2.3],\n",
              "       [5.6, 2.8, 4.9, 2. ],\n",
              "       [7.7, 2.8, 6.7, 2. ],\n",
              "       [6.3, 2.7, 4.9, 1.8],\n",
              "       [6.7, 3.3, 5.7, 2.1],\n",
              "       [7.2, 3.2, 6. , 1.8],\n",
              "       [6.2, 2.8, 4.8, 1.8],\n",
              "       [6.1, 3. , 4.9, 1.8],\n",
              "       [6.4, 2.8, 5.6, 2.1],\n",
              "       [7.2, 3. , 5.8, 1.6],\n",
              "       [7.4, 2.8, 6.1, 1.9],\n",
              "       [7.9, 3.8, 6.4, 2. ],\n",
              "       [6.4, 2.8, 5.6, 2.2],\n",
              "       [6.3, 2.8, 5.1, 1.5],\n",
              "       [6.1, 2.6, 5.6, 1.4],\n",
              "       [7.7, 3. , 6.1, 2.3],\n",
              "       [6.3, 3.4, 5.6, 2.4],\n",
              "       [6.4, 3.1, 5.5, 1.8],\n",
              "       [6. , 3. , 4.8, 1.8],\n",
              "       [6.9, 3.1, 5.4, 2.1],\n",
              "       [6.7, 3.1, 5.6, 2.4],\n",
              "       [6.9, 3.1, 5.1, 2.3],\n",
              "       [5.8, 2.7, 5.1, 1.9],\n",
              "       [6.8, 3.2, 5.9, 2.3],\n",
              "       [6.7, 3.3, 5.7, 2.5],\n",
              "       [6.7, 3. , 5.2, 2.3],\n",
              "       [6.3, 2.5, 5. , 1.9],\n",
              "       [6.5, 3. , 5.2, 2. ],\n",
              "       [6.2, 3.4, 5.4, 2.3],\n",
              "       [5.9, 3. , 5.1, 1.8]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0WwlAu8FhiI3",
        "outputId": "648339f1-cd49-4216-d0fc-31188c9e6743",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "Y"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Qg1A2lkUjFak"
      },
      "source": [
        "## Question 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3YErwYLCH0N_"
      },
      "source": [
        "### Create train and test data\n",
        "- use train_test_split to get train and test set\n",
        "- set a random_state\n",
        "- test_size: 0.25"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TYKNJL85h7pQ",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,Y_train,Y_test=train_test_split(X, Y, test_size=0.25, random_state=7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "g0KVP17Ozaix"
      },
      "source": [
        "## Question 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SIjqxbhWv1zv"
      },
      "source": [
        "### One-hot encode the labels\n",
        "- convert class vectors (integers) to binary class matrix\n",
        "- convert labels\n",
        "- number of classes: 3\n",
        "- we are doing this to use categorical_crossentropy as loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R9vv-_gpyLY9",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "OHE = OneHotEncoder(categories='auto')\n",
        "Y1_train = OHE.fit_transform(Y_train.reshape(-1,1))\n",
        "Y1_test = OHE.fit_transform(Y_test.reshape(-1,1))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6nBmBjAGmItQ",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "Y_train = to_categorical(Y_train,3)\n",
        "Y_test = to_categorical(Y_test,3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p5zUpjCVmCU1",
        "outputId": "53172c00-2924-445f-878a-4a516355da43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "Y_train"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R2N7ByQPmPEx",
        "outputId": "ea3ff4a9-66be-45fc-9a98-1ce9be639079",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 655
        }
      },
      "source": [
        "Y_test"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ovjLyYzWkO9s"
      },
      "source": [
        "## Question 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hbIFzoPNSyYo"
      },
      "source": [
        "### Initialize a sequential model\n",
        "- Define a sequential model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4FvSbf1UjHtl",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "model = tf.keras.Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dGMy999vlacX"
      },
      "source": [
        "## Question 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "72ibK5Jxm8iL"
      },
      "source": [
        "### Add a layer\n",
        "- Use Dense Layer  with input shape of 4 (according to the feature set) and number of outputs set to 3\n",
        "- Apply Softmax on Dense Layer outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uZKrBNSRm_o9",
        "colab": {}
      },
      "source": [
        "model.add(layers.Dense(4, activation='relu', input_shape=(4,)))\n",
        "model.add(layers.Dense(3, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "i4uiTH8plmNX"
      },
      "source": [
        "## Question 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yJL8n8vcSyYz"
      },
      "source": [
        "### Compile the model\n",
        "- Use SGD as Optimizer\n",
        "- Use categorical_crossentropy as loss function\n",
        "- Use accuracy as metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Tc_-fjIEk1ve",
        "outputId": "70c51046-fae3-4b07-9be8-6e7b261a27b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        }
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='sgd',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_train, Y_train,epochs=20, batch_size=1, verbose=1)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 112 samples\n",
            "Epoch 1/20\n",
            "112/112 [==============================] - 1s 6ms/sample - loss: 0.9958 - accuracy: 0.5893\n",
            "Epoch 2/20\n",
            "112/112 [==============================] - 0s 1ms/sample - loss: 0.5688 - accuracy: 0.7143\n",
            "Epoch 3/20\n",
            "112/112 [==============================] - 0s 1ms/sample - loss: 0.4488 - accuracy: 0.7946\n",
            "Epoch 4/20\n",
            "112/112 [==============================] - 0s 1ms/sample - loss: 0.3952 - accuracy: 0.8393\n",
            "Epoch 5/20\n",
            "112/112 [==============================] - 0s 1ms/sample - loss: 0.3570 - accuracy: 0.8482\n",
            "Epoch 6/20\n",
            "112/112 [==============================] - 0s 1ms/sample - loss: 0.3456 - accuracy: 0.8036\n",
            "Epoch 7/20\n",
            "112/112 [==============================] - 0s 1ms/sample - loss: 0.3173 - accuracy: 0.8304\n",
            "Epoch 8/20\n",
            "112/112 [==============================] - 0s 1ms/sample - loss: 0.2628 - accuracy: 0.8929\n",
            "Epoch 9/20\n",
            "112/112 [==============================] - 0s 1ms/sample - loss: 0.2619 - accuracy: 0.9018\n",
            "Epoch 10/20\n",
            "112/112 [==============================] - 0s 1ms/sample - loss: 0.2317 - accuracy: 0.9107\n",
            "Epoch 11/20\n",
            "112/112 [==============================] - 0s 1ms/sample - loss: 0.2135 - accuracy: 0.9196\n",
            "Epoch 12/20\n",
            "112/112 [==============================] - 0s 1ms/sample - loss: 0.2021 - accuracy: 0.9375\n",
            "Epoch 13/20\n",
            "112/112 [==============================] - 0s 1ms/sample - loss: 0.2342 - accuracy: 0.9196\n",
            "Epoch 14/20\n",
            "112/112 [==============================] - 0s 1ms/sample - loss: 0.1830 - accuracy: 0.9107\n",
            "Epoch 15/20\n",
            "112/112 [==============================] - 0s 1ms/sample - loss: 0.1769 - accuracy: 0.9196\n",
            "Epoch 16/20\n",
            "112/112 [==============================] - 0s 1ms/sample - loss: 0.1593 - accuracy: 0.9554\n",
            "Epoch 17/20\n",
            "112/112 [==============================] - 0s 1ms/sample - loss: 0.1644 - accuracy: 0.9196\n",
            "Epoch 18/20\n",
            "112/112 [==============================] - 0s 1ms/sample - loss: 0.1651 - accuracy: 0.9107\n",
            "Epoch 19/20\n",
            "112/112 [==============================] - 0s 1ms/sample - loss: 0.1413 - accuracy: 0.9464\n",
            "Epoch 20/20\n",
            "112/112 [==============================] - 0s 1ms/sample - loss: 0.1525 - accuracy: 0.9554\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sihIGbRll_jT"
      },
      "source": [
        "## Question 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "54ZZCfNGlu0i"
      },
      "source": [
        "### Summarize the model\n",
        "- Check model layers\n",
        "- Understand number of trainable parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "elER3F_4ln8n",
        "outputId": "fd949276-fab5-494b-912b-0bb3091d6f38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 4)                 20        \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3)                 15        \n",
            "=================================================================\n",
            "Total params: 35\n",
            "Trainable params: 35\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2PiP7j3Vmj4p"
      },
      "source": [
        "## Question 9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rWdbfFCXmCHt"
      },
      "source": [
        "### Fit the model\n",
        "- Give train data as training features and labels\n",
        "- Epochs: 100\n",
        "- Give validation data as testing features and labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cO1c-5tjmBVZ",
        "outputId": "5536bb0e-4e6c-400a-b994-8e6f221568c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history1 = model.fit(X_train, Y_train, validation_data=(X_test,Y_test) ,epochs=100, batch_size=1, verbose=1)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 112 samples, validate on 38 samples\n",
            "Epoch 1/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1680 - accuracy: 0.9286 - val_loss: 0.2451 - val_accuracy: 0.8684\n",
            "Epoch 2/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1711 - accuracy: 0.9464 - val_loss: 0.1581 - val_accuracy: 0.9474\n",
            "Epoch 3/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1684 - accuracy: 0.9375 - val_loss: 0.1766 - val_accuracy: 0.9211\n",
            "Epoch 4/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1581 - accuracy: 0.9375 - val_loss: 0.1490 - val_accuracy: 0.9474\n",
            "Epoch 5/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1335 - accuracy: 0.9286 - val_loss: 0.2761 - val_accuracy: 0.8684\n",
            "Epoch 6/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1612 - accuracy: 0.9196 - val_loss: 0.2572 - val_accuracy: 0.8684\n",
            "Epoch 7/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1602 - accuracy: 0.9464 - val_loss: 0.1391 - val_accuracy: 0.9474\n",
            "Epoch 8/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1229 - accuracy: 0.9464 - val_loss: 0.1369 - val_accuracy: 0.9474\n",
            "Epoch 9/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1346 - accuracy: 0.9554 - val_loss: 0.1613 - val_accuracy: 0.9211\n",
            "Epoch 10/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1440 - accuracy: 0.9554 - val_loss: 0.1457 - val_accuracy: 0.9474\n",
            "Epoch 11/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1055 - accuracy: 0.9464 - val_loss: 0.2075 - val_accuracy: 0.8947\n",
            "Epoch 12/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1151 - accuracy: 0.9643 - val_loss: 0.1682 - val_accuracy: 0.9211\n",
            "Epoch 13/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1552 - accuracy: 0.9464 - val_loss: 0.1540 - val_accuracy: 0.9211\n",
            "Epoch 14/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1177 - accuracy: 0.9643 - val_loss: 0.1309 - val_accuracy: 0.9474\n",
            "Epoch 15/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1056 - accuracy: 0.9732 - val_loss: 0.1617 - val_accuracy: 0.9211\n",
            "Epoch 16/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1155 - accuracy: 0.9643 - val_loss: 0.2809 - val_accuracy: 0.8684\n",
            "Epoch 17/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1064 - accuracy: 0.9821 - val_loss: 0.1261 - val_accuracy: 0.9474\n",
            "Epoch 18/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1010 - accuracy: 0.9732 - val_loss: 0.1249 - val_accuracy: 0.9474\n",
            "Epoch 19/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1048 - accuracy: 0.9643 - val_loss: 0.1235 - val_accuracy: 0.9474\n",
            "Epoch 20/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1202 - accuracy: 0.9643 - val_loss: 0.2872 - val_accuracy: 0.8684\n",
            "Epoch 21/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1319 - accuracy: 0.9643 - val_loss: 0.1405 - val_accuracy: 0.9211\n",
            "Epoch 22/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1400 - accuracy: 0.9554 - val_loss: 0.1219 - val_accuracy: 0.9474\n",
            "Epoch 23/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1075 - accuracy: 0.9732 - val_loss: 0.2915 - val_accuracy: 0.8684\n",
            "Epoch 24/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0932 - accuracy: 0.9732 - val_loss: 0.1343 - val_accuracy: 0.9474\n",
            "Epoch 25/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0954 - accuracy: 0.9554 - val_loss: 0.3073 - val_accuracy: 0.8684\n",
            "Epoch 26/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0913 - accuracy: 0.9732 - val_loss: 0.1281 - val_accuracy: 0.9211\n",
            "Epoch 27/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1019 - accuracy: 0.9643 - val_loss: 0.1282 - val_accuracy: 0.9474\n",
            "Epoch 28/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1262 - accuracy: 0.9464 - val_loss: 0.1316 - val_accuracy: 0.9474\n",
            "Epoch 29/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1238 - accuracy: 0.9464 - val_loss: 0.3654 - val_accuracy: 0.8684\n",
            "Epoch 30/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0954 - accuracy: 0.9732 - val_loss: 0.1217 - val_accuracy: 0.9737\n",
            "Epoch 31/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0804 - accuracy: 0.9732 - val_loss: 0.2081 - val_accuracy: 0.8947\n",
            "Epoch 32/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1043 - accuracy: 0.9643 - val_loss: 0.1290 - val_accuracy: 0.9211\n",
            "Epoch 33/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1141 - accuracy: 0.9643 - val_loss: 0.2718 - val_accuracy: 0.8684\n",
            "Epoch 34/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1029 - accuracy: 0.9732 - val_loss: 0.1243 - val_accuracy: 0.9211\n",
            "Epoch 35/100\n",
            "112/112 [==============================] - 0s 1ms/sample - loss: 0.0990 - accuracy: 0.9821 - val_loss: 0.1557 - val_accuracy: 0.9474\n",
            "Epoch 36/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0733 - accuracy: 0.9732 - val_loss: 0.2319 - val_accuracy: 0.8947\n",
            "Epoch 37/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0924 - accuracy: 0.9821 - val_loss: 0.2719 - val_accuracy: 0.8947\n",
            "Epoch 38/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0718 - accuracy: 0.9911 - val_loss: 0.1210 - val_accuracy: 0.9211\n",
            "Epoch 39/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1204 - accuracy: 0.9643 - val_loss: 0.1462 - val_accuracy: 0.9474\n",
            "Epoch 40/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0841 - accuracy: 0.9911 - val_loss: 0.1958 - val_accuracy: 0.9211\n",
            "Epoch 41/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1189 - accuracy: 0.9464 - val_loss: 0.1448 - val_accuracy: 0.9474\n",
            "Epoch 42/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1050 - accuracy: 0.9554 - val_loss: 0.1279 - val_accuracy: 0.9211\n",
            "Epoch 43/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1377 - accuracy: 0.9554 - val_loss: 0.2623 - val_accuracy: 0.8947\n",
            "Epoch 44/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0935 - accuracy: 0.9643 - val_loss: 0.2712 - val_accuracy: 0.8947\n",
            "Epoch 45/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1109 - accuracy: 0.9464 - val_loss: 0.1266 - val_accuracy: 0.9211\n",
            "Epoch 46/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0774 - accuracy: 0.9911 - val_loss: 0.1225 - val_accuracy: 0.9474\n",
            "Epoch 47/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1020 - accuracy: 0.9732 - val_loss: 0.2047 - val_accuracy: 0.9211\n",
            "Epoch 48/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0950 - accuracy: 0.9732 - val_loss: 0.1407 - val_accuracy: 0.9474\n",
            "Epoch 49/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1132 - accuracy: 0.9821 - val_loss: 0.1300 - val_accuracy: 0.9474\n",
            "Epoch 50/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1015 - accuracy: 0.9732 - val_loss: 0.1966 - val_accuracy: 0.9211\n",
            "Epoch 51/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0831 - accuracy: 0.9821 - val_loss: 0.2970 - val_accuracy: 0.8684\n",
            "Epoch 52/100\n",
            "112/112 [==============================] - 0s 1ms/sample - loss: 0.1259 - accuracy: 0.9554 - val_loss: 0.3176 - val_accuracy: 0.8684\n",
            "Epoch 53/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0575 - accuracy: 0.9911 - val_loss: 0.2130 - val_accuracy: 0.8947\n",
            "Epoch 54/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0829 - accuracy: 0.9821 - val_loss: 0.1420 - val_accuracy: 0.9474\n",
            "Epoch 55/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0915 - accuracy: 0.9732 - val_loss: 0.2172 - val_accuracy: 0.9211\n",
            "Epoch 56/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0955 - accuracy: 0.9732 - val_loss: 0.1409 - val_accuracy: 0.9474\n",
            "Epoch 57/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0940 - accuracy: 0.9732 - val_loss: 0.1950 - val_accuracy: 0.9211\n",
            "Epoch 58/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0868 - accuracy: 0.9732 - val_loss: 0.1211 - val_accuracy: 0.9474\n",
            "Epoch 59/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0818 - accuracy: 0.9821 - val_loss: 0.1194 - val_accuracy: 0.9474\n",
            "Epoch 60/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0610 - accuracy: 0.9911 - val_loss: 0.1350 - val_accuracy: 0.9474\n",
            "Epoch 61/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0827 - accuracy: 0.9821 - val_loss: 0.2090 - val_accuracy: 0.9211\n",
            "Epoch 62/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0754 - accuracy: 0.9732 - val_loss: 0.1501 - val_accuracy: 0.9474\n",
            "Epoch 63/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1072 - accuracy: 0.9732 - val_loss: 0.1319 - val_accuracy: 0.9474\n",
            "Epoch 64/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0928 - accuracy: 0.9821 - val_loss: 0.1194 - val_accuracy: 0.9474\n",
            "Epoch 65/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0787 - accuracy: 0.9732 - val_loss: 0.1300 - val_accuracy: 0.9474\n",
            "Epoch 66/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0681 - accuracy: 0.9821 - val_loss: 0.2012 - val_accuracy: 0.9211\n",
            "Epoch 67/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0710 - accuracy: 0.9911 - val_loss: 0.1259 - val_accuracy: 0.9474\n",
            "Epoch 68/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0832 - accuracy: 0.9643 - val_loss: 0.1219 - val_accuracy: 0.9211\n",
            "Epoch 69/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0628 - accuracy: 0.9821 - val_loss: 0.1230 - val_accuracy: 0.9211\n",
            "Epoch 70/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0707 - accuracy: 0.9732 - val_loss: 0.1185 - val_accuracy: 0.9211\n",
            "Epoch 71/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0910 - accuracy: 0.9732 - val_loss: 0.1984 - val_accuracy: 0.9211\n",
            "Epoch 72/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0722 - accuracy: 0.9821 - val_loss: 0.1197 - val_accuracy: 0.9211\n",
            "Epoch 73/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0900 - accuracy: 0.9732 - val_loss: 0.1275 - val_accuracy: 0.9211\n",
            "Epoch 74/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0516 - accuracy: 0.9821 - val_loss: 0.2777 - val_accuracy: 0.8947\n",
            "Epoch 75/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1079 - accuracy: 0.9732 - val_loss: 0.3131 - val_accuracy: 0.8947\n",
            "Epoch 76/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1188 - accuracy: 0.9643 - val_loss: 0.1403 - val_accuracy: 0.9474\n",
            "Epoch 77/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0884 - accuracy: 0.9821 - val_loss: 0.1197 - val_accuracy: 0.9211\n",
            "Epoch 78/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0740 - accuracy: 0.9821 - val_loss: 0.3121 - val_accuracy: 0.8947\n",
            "Epoch 79/100\n",
            "112/112 [==============================] - 0s 1ms/sample - loss: 0.1077 - accuracy: 0.9732 - val_loss: 0.1186 - val_accuracy: 0.9211\n",
            "Epoch 80/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1123 - accuracy: 0.9554 - val_loss: 0.1620 - val_accuracy: 0.9211\n",
            "Epoch 81/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0591 - accuracy: 0.9821 - val_loss: 0.1578 - val_accuracy: 0.9211\n",
            "Epoch 82/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.1081 - accuracy: 0.9643 - val_loss: 0.1481 - val_accuracy: 0.9474\n",
            "Epoch 83/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0869 - accuracy: 0.9732 - val_loss: 0.1277 - val_accuracy: 0.9211\n",
            "Epoch 84/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0888 - accuracy: 0.9732 - val_loss: 0.1425 - val_accuracy: 0.9474\n",
            "Epoch 85/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0741 - accuracy: 0.9821 - val_loss: 0.2723 - val_accuracy: 0.8684\n",
            "Epoch 86/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0955 - accuracy: 0.9643 - val_loss: 0.1260 - val_accuracy: 0.9474\n",
            "Epoch 87/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0636 - accuracy: 0.9911 - val_loss: 0.1769 - val_accuracy: 0.9211\n",
            "Epoch 88/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0755 - accuracy: 0.9732 - val_loss: 0.2443 - val_accuracy: 0.8947\n",
            "Epoch 89/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0971 - accuracy: 0.9643 - val_loss: 0.1243 - val_accuracy: 0.9211\n",
            "Epoch 90/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0691 - accuracy: 0.9821 - val_loss: 0.1773 - val_accuracy: 0.9211\n",
            "Epoch 91/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0967 - accuracy: 0.9732 - val_loss: 0.1324 - val_accuracy: 0.9474\n",
            "Epoch 92/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0606 - accuracy: 0.9821 - val_loss: 0.2508 - val_accuracy: 0.8947\n",
            "Epoch 93/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0642 - accuracy: 0.9821 - val_loss: 0.1492 - val_accuracy: 0.9211\n",
            "Epoch 94/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0876 - accuracy: 0.9732 - val_loss: 0.1552 - val_accuracy: 0.8947\n",
            "Epoch 95/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0842 - accuracy: 0.9643 - val_loss: 0.1696 - val_accuracy: 0.9211\n",
            "Epoch 96/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0754 - accuracy: 0.9821 - val_loss: 0.1248 - val_accuracy: 0.9211\n",
            "Epoch 97/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0810 - accuracy: 0.9732 - val_loss: 0.1402 - val_accuracy: 0.9474\n",
            "Epoch 98/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0933 - accuracy: 0.9643 - val_loss: 0.1549 - val_accuracy: 0.9474\n",
            "Epoch 99/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0766 - accuracy: 0.9732 - val_loss: 0.1494 - val_accuracy: 0.9474\n",
            "Epoch 100/100\n",
            "112/112 [==============================] - 0s 2ms/sample - loss: 0.0806 - accuracy: 0.9732 - val_loss: 0.1369 - val_accuracy: 0.9474\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "re9ItAR3yS3J"
      },
      "source": [
        "## Question 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "liw0IFf9yVqH"
      },
      "source": [
        "### Make predictions\n",
        "- Predict labels on one row"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H5sBybi6mlLl",
        "outputId": "b194d788-4e18-47fa-f651-099c269e8dca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 655
        }
      },
      "source": [
        "y_pred = np.round(model.predict(X_test))\n",
        "y_pred"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [1., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Xo2Wq2u3v_35",
        "outputId": "f5511300-1572-462f-9365-3e76150bed0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model.predict(X_test[ 0:1 , : ])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2.3995508e-08, 2.8742576e-02, 9.7125733e-01]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hSUgMq3m0bG7"
      },
      "source": [
        "### Compare the prediction with actual label\n",
        "- Print the same row as done in the previous step but of actual labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "K5WbwVPyz-qQ",
        "outputId": "53ef4652-3231-4e03-f05b-a5b8eb41d2c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "Y_test[0:1 , : ]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JKU1eM-ozehw",
        "outputId": "ab151781-fbed-4877-f21a-e0fd90c1ff51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "score = model.evaluate(X_test, Y_test,verbose=1)\n",
        "\n",
        "print(score)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "38/38 [==============================] - 0s 2ms/sample - loss: 0.1369 - accuracy: 0.9474\n",
            "[0.1369056113456425, 0.94736844]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FrTKwbgE7NFT"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "a1UBYPNp5Tn1"
      },
      "source": [
        "# Stock prices dataset\n",
        "The data is of tock exchange's stock listings for each trading day of 2010 to 2016.\n",
        "\n",
        "## Description\n",
        "A brief description of columns.\n",
        "- open: The opening market price of the equity symbol on the date\n",
        "- high: The highest market price of the equity symbol on the date\n",
        "- low: The lowest recorded market price of the equity symbol on the date\n",
        "- close: The closing recorded price of the equity symbol on the date\n",
        "- symbol: Symbol of the listed company\n",
        "- volume: Total traded volume of the equity symbol on the date\n",
        "- date: Date of record"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ctH_ZW5g-M3g"
      },
      "source": [
        "### Specifying the TensorFlow version\n",
        "Running `import tensorflow` will import the default version (currently 1.x). You can use 2.x by running a cell with the `tensorflow_version` magic **before** you run `import tensorflow`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vQbdODpH-M3r",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nFQWH1tj-M38"
      },
      "source": [
        "### Import TensorFlow\n",
        "Once you have specified a version via this magic, you can run `import tensorflow` as normal and verify which version was imported as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ho5n-xhd-M3_",
        "outputId": "af36d30f-02fc-406b-a07f-4037a016b13b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tgkl0qu6-M4F"
      },
      "source": [
        "### Set random seed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TKgTyuA3-M4G",
        "colab": {}
      },
      "source": [
        "tf.random.set_seed(47)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_88voqAH-O6J"
      },
      "source": [
        "## Question 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dRHCeJqP-evf"
      },
      "source": [
        "### Load the data\n",
        "- load the csv file and read it using pandas\n",
        "- file name is prices.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cKVH5v7r-RmC",
        "outputId": "11e7808b-c433-4cb7-cbca-3d69ecd23174",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "# run this cell to upload file if you are using google colab\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d0c3ece4-1286-4ca7-a58c-1eb5ab03867a\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-d0c3ece4-1286-4ca7-a58c-1eb5ab03867a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving prices.csv to prices.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-gDC6cSW_FSK",
        "outputId": "586d7d50-f862-412f-9c94-d1294ef3ea3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "#prices=pd.read_csv('gdrive/My Drive/Colab Notebooks/prices.csv')\n",
        "prices=pd.read_csv('prices.csv')\n",
        "prices.head()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>symbol</th>\n",
              "      <th>open</th>\n",
              "      <th>close</th>\n",
              "      <th>low</th>\n",
              "      <th>high</th>\n",
              "      <th>volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2016-01-05 00:00:00</td>\n",
              "      <td>WLTW</td>\n",
              "      <td>123.430000</td>\n",
              "      <td>125.839996</td>\n",
              "      <td>122.309998</td>\n",
              "      <td>126.250000</td>\n",
              "      <td>2163600.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2016-01-06 00:00:00</td>\n",
              "      <td>WLTW</td>\n",
              "      <td>125.239998</td>\n",
              "      <td>119.980003</td>\n",
              "      <td>119.940002</td>\n",
              "      <td>125.540001</td>\n",
              "      <td>2386400.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2016-01-07 00:00:00</td>\n",
              "      <td>WLTW</td>\n",
              "      <td>116.379997</td>\n",
              "      <td>114.949997</td>\n",
              "      <td>114.930000</td>\n",
              "      <td>119.739998</td>\n",
              "      <td>2489500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2016-01-08 00:00:00</td>\n",
              "      <td>WLTW</td>\n",
              "      <td>115.480003</td>\n",
              "      <td>116.620003</td>\n",
              "      <td>113.500000</td>\n",
              "      <td>117.440002</td>\n",
              "      <td>2006300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2016-01-11 00:00:00</td>\n",
              "      <td>WLTW</td>\n",
              "      <td>117.010002</td>\n",
              "      <td>114.970001</td>\n",
              "      <td>114.089996</td>\n",
              "      <td>117.330002</td>\n",
              "      <td>1408600.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  date symbol        open  ...         low        high     volume\n",
              "0  2016-01-05 00:00:00   WLTW  123.430000  ...  122.309998  126.250000  2163600.0\n",
              "1  2016-01-06 00:00:00   WLTW  125.239998  ...  119.940002  125.540001  2386400.0\n",
              "2  2016-01-07 00:00:00   WLTW  116.379997  ...  114.930000  119.739998  2489500.0\n",
              "3  2016-01-08 00:00:00   WLTW  115.480003  ...  113.500000  117.440002  2006300.0\n",
              "4  2016-01-11 00:00:00   WLTW  117.010002  ...  114.089996  117.330002  1408600.0\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HlLKVPVH_BCT"
      },
      "source": [
        "## Question 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9J4BlzVA_gZd"
      },
      "source": [
        "### Drop columnns\n",
        "- drop \"date\" and \"symbol\" column from the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IKEK8aEE_Csx",
        "outputId": "f6f81e20-a6a4-4cbf-ac94-8f1e486caf9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "df = prices.drop(['date', 'symbol'], axis = 1) \n",
        "df.head()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>open</th>\n",
              "      <th>close</th>\n",
              "      <th>low</th>\n",
              "      <th>high</th>\n",
              "      <th>volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>123.430000</td>\n",
              "      <td>125.839996</td>\n",
              "      <td>122.309998</td>\n",
              "      <td>126.250000</td>\n",
              "      <td>2163600.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>125.239998</td>\n",
              "      <td>119.980003</td>\n",
              "      <td>119.940002</td>\n",
              "      <td>125.540001</td>\n",
              "      <td>2386400.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>116.379997</td>\n",
              "      <td>114.949997</td>\n",
              "      <td>114.930000</td>\n",
              "      <td>119.739998</td>\n",
              "      <td>2489500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>115.480003</td>\n",
              "      <td>116.620003</td>\n",
              "      <td>113.500000</td>\n",
              "      <td>117.440002</td>\n",
              "      <td>2006300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>117.010002</td>\n",
              "      <td>114.970001</td>\n",
              "      <td>114.089996</td>\n",
              "      <td>117.330002</td>\n",
              "      <td>1408600.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         open       close         low        high     volume\n",
              "0  123.430000  125.839996  122.309998  126.250000  2163600.0\n",
              "1  125.239998  119.980003  119.940002  125.540001  2386400.0\n",
              "2  116.379997  114.949997  114.930000  119.739998  2489500.0\n",
              "3  115.480003  116.620003  113.500000  117.440002  2006300.0\n",
              "4  117.010002  114.970001  114.089996  117.330002  1408600.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cTPhO6v-AiZt"
      },
      "source": [
        "## Question 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SsZXmF3NAkna"
      },
      "source": [
        "### Take initial rows\n",
        "- Take first 1000 rows from the data\n",
        "- This step is done to make the execution faster"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aKs04iIHAjxN",
        "outputId": "682cb500-f9bf-4149-86cc-dde2875ea574",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(851264, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TrS5fBIw1KMH",
        "outputId": "46852183-a270-49b2-8037-bce6dd6b4933",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "df.iloc[0:1000 , :]"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>open</th>\n",
              "      <th>close</th>\n",
              "      <th>low</th>\n",
              "      <th>high</th>\n",
              "      <th>volume</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>123.430000</td>\n",
              "      <td>125.839996</td>\n",
              "      <td>122.309998</td>\n",
              "      <td>126.250000</td>\n",
              "      <td>2163600.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>125.239998</td>\n",
              "      <td>119.980003</td>\n",
              "      <td>119.940002</td>\n",
              "      <td>125.540001</td>\n",
              "      <td>2386400.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>116.379997</td>\n",
              "      <td>114.949997</td>\n",
              "      <td>114.930000</td>\n",
              "      <td>119.739998</td>\n",
              "      <td>2489500.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>115.480003</td>\n",
              "      <td>116.620003</td>\n",
              "      <td>113.500000</td>\n",
              "      <td>117.440002</td>\n",
              "      <td>2006300.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>117.010002</td>\n",
              "      <td>114.970001</td>\n",
              "      <td>114.089996</td>\n",
              "      <td>117.330002</td>\n",
              "      <td>1408600.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>63.310001</td>\n",
              "      <td>63.590000</td>\n",
              "      <td>63.240002</td>\n",
              "      <td>63.639999</td>\n",
              "      <td>2133200.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>27.160000</td>\n",
              "      <td>26.990000</td>\n",
              "      <td>26.680000</td>\n",
              "      <td>27.299999</td>\n",
              "      <td>1982400.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>28.320000</td>\n",
              "      <td>28.770000</td>\n",
              "      <td>28.010000</td>\n",
              "      <td>28.809999</td>\n",
              "      <td>37152800.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>44.000000</td>\n",
              "      <td>44.799999</td>\n",
              "      <td>43.750000</td>\n",
              "      <td>44.810001</td>\n",
              "      <td>6568600.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>36.080002</td>\n",
              "      <td>37.139999</td>\n",
              "      <td>36.009998</td>\n",
              "      <td>37.230000</td>\n",
              "      <td>5604300.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows  5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           open       close         low        high      volume\n",
              "0    123.430000  125.839996  122.309998  126.250000   2163600.0\n",
              "1    125.239998  119.980003  119.940002  125.540001   2386400.0\n",
              "2    116.379997  114.949997  114.930000  119.739998   2489500.0\n",
              "3    115.480003  116.620003  113.500000  117.440002   2006300.0\n",
              "4    117.010002  114.970001  114.089996  117.330002   1408600.0\n",
              "..          ...         ...         ...         ...         ...\n",
              "995   63.310001   63.590000   63.240002   63.639999   2133200.0\n",
              "996   27.160000   26.990000   26.680000   27.299999   1982400.0\n",
              "997   28.320000   28.770000   28.010000   28.809999  37152800.0\n",
              "998   44.000000   44.799999   43.750000   44.810001   6568600.0\n",
              "999   36.080002   37.139999   36.009998   37.230000   5604300.0\n",
              "\n",
              "[1000 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5l7yogRh1yPZ",
        "outputId": "59556cb2-3f17-4547-f88f-cd139d94b572",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "prices_df = df.iloc[0:1000 , :]\n",
        "prices_df.shape"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6vGtnapgBIJm"
      },
      "source": [
        "## Question 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C8u_jlbABTip"
      },
      "source": [
        "### Get features and label from the dataset in separate variable\n",
        "- Take \"open\", \"close\", \"low\", \"high\" columns as features\n",
        "- Take \"volume\" column as label\n",
        "- Normalize label column by dividing it with 1000000"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xQjCMzUXBJbg",
        "colab": {}
      },
      "source": [
        "labels = prices_df['volume']\n",
        "features = prices_df.drop('volume', axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lLx90lHP3S2b",
        "colab": {}
      },
      "source": [
        "labels = labels / 1000000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Lujhavq52ojq",
        "outputId": "feda7f98-24f7-4241-e39e-6e5e0182903b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "labels"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       2.1636\n",
              "1       2.3864\n",
              "2       2.4895\n",
              "3       2.0063\n",
              "4       1.4086\n",
              "        ...   \n",
              "995     2.1332\n",
              "996     1.9824\n",
              "997    37.1528\n",
              "998     6.5686\n",
              "999     5.6043\n",
              "Name: volume, Length: 1000, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RZQ96fDw2pnt",
        "outputId": "d34c766b-a037-409e-e704-d8c38377f18e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "features"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>open</th>\n",
              "      <th>close</th>\n",
              "      <th>low</th>\n",
              "      <th>high</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>123.430000</td>\n",
              "      <td>125.839996</td>\n",
              "      <td>122.309998</td>\n",
              "      <td>126.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>125.239998</td>\n",
              "      <td>119.980003</td>\n",
              "      <td>119.940002</td>\n",
              "      <td>125.540001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>116.379997</td>\n",
              "      <td>114.949997</td>\n",
              "      <td>114.930000</td>\n",
              "      <td>119.739998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>115.480003</td>\n",
              "      <td>116.620003</td>\n",
              "      <td>113.500000</td>\n",
              "      <td>117.440002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>117.010002</td>\n",
              "      <td>114.970001</td>\n",
              "      <td>114.089996</td>\n",
              "      <td>117.330002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>63.310001</td>\n",
              "      <td>63.590000</td>\n",
              "      <td>63.240002</td>\n",
              "      <td>63.639999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>27.160000</td>\n",
              "      <td>26.990000</td>\n",
              "      <td>26.680000</td>\n",
              "      <td>27.299999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>28.320000</td>\n",
              "      <td>28.770000</td>\n",
              "      <td>28.010000</td>\n",
              "      <td>28.809999</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>44.000000</td>\n",
              "      <td>44.799999</td>\n",
              "      <td>43.750000</td>\n",
              "      <td>44.810001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>36.080002</td>\n",
              "      <td>37.139999</td>\n",
              "      <td>36.009998</td>\n",
              "      <td>37.230000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows  4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           open       close         low        high\n",
              "0    123.430000  125.839996  122.309998  126.250000\n",
              "1    125.239998  119.980003  119.940002  125.540001\n",
              "2    116.379997  114.949997  114.930000  119.739998\n",
              "3    115.480003  116.620003  113.500000  117.440002\n",
              "4    117.010002  114.970001  114.089996  117.330002\n",
              "..          ...         ...         ...         ...\n",
              "995   63.310001   63.590000   63.240002   63.639999\n",
              "996   27.160000   26.990000   26.680000   27.299999\n",
              "997   28.320000   28.770000   28.010000   28.809999\n",
              "998   44.000000   44.799999   43.750000   44.810001\n",
              "999   36.080002   37.139999   36.009998   37.230000\n",
              "\n",
              "[1000 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aTAKzlxZBz0z"
      },
      "source": [
        "## Question 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IfY8Km1Zzyt2"
      },
      "source": [
        "### Convert data\n",
        "- Convert features and labels to numpy array\n",
        "- Convert their data type to \"float32\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ko7nnQVbYENh",
        "outputId": "b64493b2-6f0d-42ad-c2e6-9df8bda09f8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "X = np.array(features).astype('float32')\n",
        "Y= np.array(labels).astype('float32')\n",
        "print(X.shape)\n",
        "print(Y.shape)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000, 4)\n",
            "(1000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9bzE5lXn4Xpi",
        "outputId": "c5770b82-a37c-45fb-e8a5-56e33f5f5a26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "X"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[123.43, 125.84, 122.31, 126.25],\n",
              "       [125.24, 119.98, 119.94, 125.54],\n",
              "       [116.38, 114.95, 114.93, 119.74],\n",
              "       ...,\n",
              "       [ 28.32,  28.77,  28.01,  28.81],\n",
              "       [ 44.  ,  44.8 ,  43.75,  44.81],\n",
              "       [ 36.08,  37.14,  36.01,  37.23]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IlxEyxbk4YsL",
        "outputId": "8a06b834-0bb5-4990-9011-69235fb1eed0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "Y"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2.163600e+00, 2.386400e+00, 2.489500e+00, 2.006300e+00,\n",
              "       1.408600e+00, 1.098000e+00, 9.496000e-01, 7.853000e-01,\n",
              "       1.093700e+00, 1.523500e+00, 1.653900e+00, 9.443000e-01,\n",
              "       7.449000e-01, 7.038000e-01, 5.631000e-01, 8.961000e-01,\n",
              "       6.804000e-01, 7.499000e-01, 5.742000e-01, 6.948000e-01,\n",
              "       8.963000e-01, 9.563000e-01, 9.971000e-01, 1.200500e+00,\n",
              "       1.725200e+00, 1.946000e+00, 1.319500e+00, 9.224000e-01,\n",
              "       1.185100e+00, 9.215000e-01, 4.409000e-01, 1.244300e+00,\n",
              "       6.813000e-01, 4.112000e-01, 4.473000e-01, 5.592000e-01,\n",
              "       4.599000e-01, 9.716000e-01, 6.941000e-01, 1.159600e+00,\n",
              "       8.847000e-01, 6.920000e-01, 8.980000e-01, 8.445000e-01,\n",
              "       7.965000e-01, 4.168000e-01, 5.902000e-01, 4.951000e-01,\n",
              "       5.158000e-01, 5.621000e-01, 8.093000e-01, 1.607300e+00,\n",
              "       1.001300e+00, 7.114000e-01, 4.442000e-01, 5.341000e-01,\n",
              "       7.982000e-01, 5.910000e-01, 4.432000e-01, 5.780000e-01,\n",
              "       7.651000e-01, 5.143000e-01, 1.150000e+00, 2.035300e+00,\n",
              "       7.022000e-01, 6.891000e-01, 5.532000e-01, 7.759000e-01,\n",
              "       6.217000e-01, 3.581000e-01, 9.393000e-01, 7.355000e-01,\n",
              "       9.246000e-01, 6.588000e-01, 1.055400e+00, 6.953000e-01,\n",
              "       4.600000e-01, 5.329000e-01, 4.626000e-01, 9.107000e-01,\n",
              "       8.896000e-01, 7.347000e-01, 1.304800e+00, 1.597300e+00,\n",
              "       1.917500e+00, 3.221500e+00, 9.198000e-01, 1.991600e+00,\n",
              "       5.948000e-01, 4.758000e-01, 7.151000e-01, 5.285000e-01,\n",
              "       5.800000e-01, 6.297000e-01, 5.982000e-01, 8.167000e-01,\n",
              "       6.408000e-01, 7.611000e-01, 4.621000e-01, 6.270000e-01,\n",
              "       7.225000e-01, 1.384200e+00, 4.415000e-01, 4.236000e-01,\n",
              "       4.967000e-01, 5.367000e-01, 4.294000e-01, 7.191000e-01,\n",
              "       3.945000e-01, 5.480000e-01, 2.633000e-01, 3.293000e-01,\n",
              "       3.787000e-01, 4.048000e-01, 6.593000e-01, 5.503000e-01,\n",
              "       4.856000e-01, 5.619000e-01, 5.123000e-01, 1.638000e+00,\n",
              "       8.844000e-01, 7.952000e-01, 1.049700e+00, 1.162100e+00,\n",
              "       8.584000e-01, 7.614000e-01, 7.499000e-01, 5.911000e-01,\n",
              "       6.928000e-01, 4.826000e-01, 8.440000e-01, 6.300000e-01,\n",
              "       5.671000e-01, 5.122000e-01, 4.622000e-01, 7.061000e-01,\n",
              "       5.339000e-01, 3.747000e-01, 3.738000e-01, 3.657000e-01,\n",
              "       3.139000e-01, 4.010000e-01, 2.755000e-01, 4.267000e-01,\n",
              "       5.746000e-01, 7.932000e-01, 4.469000e-01, 7.667000e-01,\n",
              "       1.865200e+00, 9.308000e-01, 5.027000e-01, 7.097000e-01,\n",
              "       5.639000e-01, 3.429000e-01, 4.788000e-01, 5.018000e-01,\n",
              "       4.103000e-01, 4.438000e-01, 1.414800e+00, 5.707000e-01,\n",
              "       3.913000e-01, 4.252000e-01, 5.263000e-01, 5.147000e-01,\n",
              "       4.565000e-01, 4.141000e-01, 4.645000e-01, 5.262000e-01,\n",
              "       4.873000e-01, 7.523000e-01, 6.325000e-01, 7.283000e-01,\n",
              "       6.411000e-01, 6.555000e-01, 4.889000e-01, 7.419000e-01,\n",
              "       7.134000e-01, 1.847200e+00, 3.107000e-01, 3.110000e-01,\n",
              "       9.277000e-01, 7.165000e-01, 7.970000e-01, 4.103000e-01,\n",
              "       5.036000e-01, 5.672000e-01, 1.504500e+00, 1.461900e+00,\n",
              "       1.142100e+00, 1.068500e+00, 7.002000e-01, 7.672000e-01,\n",
              "       8.551000e-01, 4.238000e-01, 5.757000e-01, 6.819000e-01,\n",
              "       5.403000e-01, 5.480000e-01, 5.645000e-01, 5.056000e-01,\n",
              "       4.356000e-01, 5.548000e-01, 4.966000e-01, 3.120000e-01,\n",
              "       4.147000e-01, 4.783000e-01, 4.170000e-01, 4.457000e-01,\n",
              "       4.592000e-01, 4.758000e-01, 6.932000e-01, 7.367000e-01,\n",
              "       3.531200e+00, 1.345600e+00, 9.676000e-01, 1.291600e+00,\n",
              "       1.524500e+00, 7.543000e-01, 1.089100e+00, 6.397000e-01,\n",
              "       8.184000e-01, 5.245000e-01, 8.190000e-01, 4.759000e-01,\n",
              "       1.011700e+00, 3.802000e-01, 2.840000e-01, 4.399000e-01,\n",
              "       3.529000e-01, 6.767000e-01, 8.987000e-01, 8.558000e-01,\n",
              "       6.047000e-01, 1.301200e+00, 9.283000e-01, 9.885000e-01,\n",
              "       9.434000e-01, 8.225000e-01, 1.168200e+00, 8.464000e-01,\n",
              "       8.266000e-01, 1.232700e+00, 9.376000e-01, 6.515000e-01,\n",
              "       6.146000e-01, 5.572000e-01, 3.619000e-01, 3.829000e-01,\n",
              "       4.299000e-01, 2.166000e-01, 4.664000e-01, 3.815500e+00,\n",
              "       9.837300e+00, 1.701700e+00, 1.234324e+02, 2.455900e+00,\n",
              "       1.082900e+01, 3.650100e+00, 4.710200e+00, 2.102700e+00,\n",
              "       3.472500e+00, 3.930100e+00, 7.943000e-01, 2.228600e+00,\n",
              "       1.299300e+00, 4.076600e+00, 4.597600e+00, 5.671300e+00,\n",
              "       2.362000e+00, 8.564000e-01, 7.750900e+00, 1.228200e+00,\n",
              "       3.793000e-01, 3.015600e+00, 7.129000e-01, 1.802400e+00,\n",
              "       2.631000e+00, 1.128200e+00, 1.861510e+01, 8.767000e-01,\n",
              "       3.061000e-01, 5.277400e+00, 2.238700e+00, 2.750500e+00,\n",
              "       7.599900e+00, 1.650400e+00, 4.641800e+00, 3.407100e+00,\n",
              "       2.364900e+00, 3.324500e+00, 1.131400e+00, 2.457200e+00,\n",
              "       1.151210e+01, 9.306600e+00, 1.483400e+00, 5.387000e-01,\n",
              "       1.301200e+00, 2.176100e+00, 6.894300e+00, 8.292000e-01,\n",
              "       4.083000e-01, 6.186700e+00, 1.808452e+02, 1.146750e+01,\n",
              "       2.971500e+00, 4.550800e+00, 6.433800e+00, 1.371800e+00,\n",
              "       1.793200e+00, 3.746700e+00, 5.888000e+00, 2.469700e+00,\n",
              "       6.127700e+00, 2.387000e-01, 1.234200e+00, 1.437610e+01,\n",
              "       1.433230e+01, 1.286000e+00, 1.511500e+00, 4.067930e+01,\n",
              "       3.385100e+00, 4.009700e+00, 3.824400e+00, 7.325600e+00,\n",
              "       2.670300e+00, 4.553000e+00, 6.710900e+00, 1.964100e+00,\n",
              "       5.372700e+00, 4.832000e+00, 3.055200e+00, 6.172000e+00,\n",
              "       2.232400e+00, 3.114680e+01, 8.229000e-01, 3.227300e+00,\n",
              "       1.261400e+00, 4.874200e+00, 1.014300e+00, 1.438400e+00,\n",
              "       1.357340e+01, 2.660500e+00, 3.139000e-01, 1.148600e+00,\n",
              "       5.875500e+00, 6.600000e-01, 2.239700e+00, 4.447400e+00,\n",
              "       4.514800e+00, 3.350600e+00, 8.281000e-01, 4.812000e-01,\n",
              "       1.388080e+01, 3.280200e+00, 1.574200e+00, 7.906000e+00,\n",
              "       5.985370e+01, 8.391000e+00, 1.051100e+00, 2.068100e+00,\n",
              "       5.765200e+00, 1.680700e+00, 1.467680e+01, 1.017380e+01,\n",
              "       3.025000e-01, 2.175500e+00, 1.448250e+01, 6.017600e+00,\n",
              "       3.974600e+00, 7.552600e+00, 4.372000e-01, 1.046000e+00,\n",
              "       5.799100e+00, 5.202600e+00, 1.370040e+01, 2.844100e+00,\n",
              "       5.354000e-01, 1.306900e+00, 3.059400e+00, 4.995000e-01,\n",
              "       1.502200e+00, 1.693300e+01, 1.199700e+00, 2.228100e+00,\n",
              "       1.421600e+00, 4.152100e+00, 1.910400e+00, 4.225500e+00,\n",
              "       3.850500e+00, 2.251160e+01, 1.009500e+00, 2.142300e+00,\n",
              "       7.507000e-01, 1.931500e+00, 3.508400e+00, 1.446400e+00,\n",
              "       3.781000e+00, 2.401200e+00, 4.110000e+00, 5.763000e-01,\n",
              "       3.299500e+00, 7.742000e-01, 1.206800e+00, 3.687800e+00,\n",
              "       4.480000e-01, 5.231900e+00, 2.347600e+00, 1.049400e+00,\n",
              "       1.030800e+00, 4.367600e+00, 1.062400e+00, 2.186500e+00,\n",
              "       7.744000e-01, 6.085580e+01, 3.432000e+00, 1.808240e+01,\n",
              "       3.215100e+00, 1.840900e+00, 6.749000e-01, 4.623700e+00,\n",
              "       2.536000e+00, 1.542910e+01, 1.873800e+00, 2.269800e+00,\n",
              "       3.553300e+00, 2.161800e+00, 1.789400e+00, 2.849400e+00,\n",
              "       1.651270e+01, 6.624000e-01, 1.636000e+00, 1.908400e+00,\n",
              "       4.030400e+00, 1.198500e+00, 6.707990e+01, 2.997000e+00,\n",
              "       1.680900e+01, 5.156400e+00, 1.653910e+01, 3.927000e+00,\n",
              "       3.908400e+00, 8.369000e-01, 2.533400e+00, 9.185600e+00,\n",
              "       2.098800e+00, 9.135000e+00, 3.905600e+00, 5.352000e-01,\n",
              "       1.157160e+01, 6.076000e-01, 1.652600e+00, 1.046280e+01,\n",
              "       3.658400e+00, 1.426600e+00, 4.278700e+00, 1.312090e+01,\n",
              "       3.729300e+00, 6.944100e+00, 2.904000e+00, 4.011600e+00,\n",
              "       7.389200e+00, 9.200000e-01, 2.795740e+01, 3.018000e+00,\n",
              "       2.705200e+00, 6.546000e-01, 7.948000e-01, 6.997800e+00,\n",
              "       1.048000e+00, 2.008500e+00, 6.155300e+00, 3.637000e+00,\n",
              "       3.252000e-01, 2.860000e-01, 1.793700e+00, 4.780090e+01,\n",
              "       2.353000e+00, 4.034900e+00, 4.444300e+00, 2.965200e+00,\n",
              "       9.955000e-01, 3.371000e-01, 2.728100e+00, 4.254200e+00,\n",
              "       1.876000e+00, 8.727000e+00, 1.728500e+00, 9.506200e+00,\n",
              "       3.332500e+00, 3.546050e+01, 2.816600e+00, 2.749000e+00,\n",
              "       1.490160e+01, 6.263800e+00, 2.827000e+00, 1.634900e+00,\n",
              "       2.122800e+00, 1.387040e+01, 1.271880e+01, 3.239900e+00,\n",
              "       6.135000e-01, 2.210900e+00, 3.959400e+00, 1.241400e+00,\n",
              "       4.493800e+00, 8.834000e-01, 1.656200e+00, 8.989000e-01,\n",
              "       2.887600e+00, 6.067100e+00, 2.408300e+00, 3.811400e+00,\n",
              "       1.332800e+00, 9.616700e+00, 1.856900e+00, 9.378000e-01,\n",
              "       9.625000e+00, 1.141850e+01, 8.823100e+00, 9.321000e+00,\n",
              "       3.858000e-01, 2.642400e+00, 2.860300e+00, 3.380500e+00,\n",
              "       2.975900e+00, 5.839300e+00, 1.518700e+00, 2.035200e+00,\n",
              "       1.859700e+00, 7.953300e+00, 6.109400e+00, 4.767000e+00,\n",
              "       4.970000e-01, 6.010500e+00, 4.900000e-01, 4.562000e-01,\n",
              "       3.680300e+00, 3.043700e+00, 3.880200e+00, 1.104860e+01,\n",
              "       3.424300e+00, 4.072100e+00, 1.389650e+01, 9.214200e+00,\n",
              "       3.840910e+01, 7.511700e+00, 7.210000e-01, 9.510000e-02,\n",
              "       3.441270e+01, 1.642300e+00, 3.611900e+00, 2.227600e+00,\n",
              "       2.171500e+00, 2.602200e+00, 5.625400e+00, 1.723960e+01,\n",
              "       1.164900e+00, 6.905600e+00, 1.197240e+01, 1.616100e+00,\n",
              "       5.385900e+00, 1.683700e+00, 1.884500e+00, 5.320100e+00,\n",
              "       1.967200e+00, 4.923600e+00, 2.000510e+01, 2.925800e+00,\n",
              "       8.769000e-01, 1.759600e+00, 1.670800e+00, 2.679500e+01,\n",
              "       1.278200e+00, 3.624100e+00, 3.731300e+00, 2.932200e+00,\n",
              "       2.040000e+00, 2.631700e+00, 2.041800e+00, 8.632000e-01,\n",
              "       1.519900e+00, 5.130400e+00, 6.585900e+00, 5.208600e+01,\n",
              "       3.470900e+00, 9.190800e+00, 4.619400e+00, 9.610000e-01,\n",
              "       6.121300e+00, 1.880200e+00, 1.169700e+00, 7.844500e+00,\n",
              "       4.890300e+00, 7.125000e-01, 8.735000e-01, 2.413400e+00,\n",
              "       1.126800e+00, 5.465000e-01, 3.001500e+00, 1.579100e+00,\n",
              "       5.405000e-01, 2.460200e+00, 1.233000e+00, 1.524100e+00,\n",
              "       1.457020e+01, 6.407000e-01, 3.252000e+00, 2.792600e+00,\n",
              "       5.103000e-01, 1.175480e+01, 1.018900e+00, 2.060800e+00,\n",
              "       3.964700e+00, 8.997000e-01, 9.040000e-01, 9.279000e-01,\n",
              "       1.574360e+01, 2.244000e+00, 1.425500e+00, 1.897000e+00,\n",
              "       1.637000e+01, 8.668000e-01, 1.402360e+01, 3.100900e+00,\n",
              "       9.204000e-01, 1.338000e+00, 2.947000e-01, 7.737000e-01,\n",
              "       5.771300e+00, 2.415600e+00, 2.680000e-01, 1.323500e+00,\n",
              "       5.119300e+00, 3.450400e+00, 1.572200e+00, 4.008800e+00,\n",
              "       4.659000e-01, 2.081200e+00, 6.668600e+00, 3.440300e+00,\n",
              "       1.111690e+01, 1.418700e+00, 2.033000e+00, 4.911500e+00,\n",
              "       3.473200e+00, 3.982800e+00, 8.322300e+00, 2.459700e+00,\n",
              "       2.913660e+01, 1.302400e+00, 2.928700e+00, 1.793700e+00,\n",
              "       8.490500e+00, 4.589100e+00, 1.935200e+00, 1.257500e+01,\n",
              "       1.734900e+00, 7.182800e+00, 2.330200e+00, 3.716000e+00,\n",
              "       1.316400e+00, 3.355000e+00, 8.833800e+00, 1.795200e+00,\n",
              "       7.026100e+00, 1.036930e+01, 3.630600e+00, 4.284000e+00,\n",
              "       8.785900e+00, 1.835200e+00, 1.553600e+00, 5.947000e-01,\n",
              "       1.219950e+01, 3.110300e+00, 5.894200e+00, 3.897200e+00,\n",
              "       3.433800e+00, 1.692500e+00, 1.289170e+01, 6.104100e+00,\n",
              "       2.018000e+01, 7.806000e-01, 2.042000e+00, 3.132800e+00,\n",
              "       1.545450e+01, 1.128000e+00, 1.863800e+00, 3.900000e-01,\n",
              "       2.652000e+00, 1.744900e+00, 1.196900e+00, 1.617660e+01,\n",
              "       9.544000e-01, 8.171000e+00, 3.118200e+00, 1.541800e+00,\n",
              "       3.933570e+01, 5.275000e+00, 1.010100e+00, 2.058800e+00,\n",
              "       7.020200e+00, 2.075310e+01, 4.277900e+00, 1.832400e+00,\n",
              "       2.071000e+00, 4.741400e+00, 6.275000e-01, 2.670400e+00,\n",
              "       2.555900e+00, 2.824700e+00, 2.780910e+01, 1.051400e+00,\n",
              "       1.347270e+01, 1.658740e+01, 2.962300e+00, 7.824000e-01,\n",
              "       3.974600e+00, 1.938700e+00, 4.186000e+00, 2.521200e+01,\n",
              "       1.932400e+00, 1.504762e+02, 2.476800e+00, 1.056210e+01,\n",
              "       2.613000e+00, 7.108800e+00, 2.040100e+00, 3.458700e+00,\n",
              "       3.252400e+00, 3.693000e-01, 3.008800e+00, 1.422200e+00,\n",
              "       5.112400e+00, 5.093200e+00, 4.573600e+00, 3.965300e+00,\n",
              "       5.834000e-01, 8.920500e+00, 1.377600e+00, 5.457000e-01,\n",
              "       5.421900e+00, 3.989000e-01, 3.052800e+00, 5.342100e+00,\n",
              "       9.276000e-01, 1.517320e+01, 7.141000e-01, 3.246000e-01,\n",
              "       7.882800e+00, 2.782200e+00, 2.575800e+00, 8.851900e+00,\n",
              "       2.532200e+00, 8.385500e+00, 4.986100e+00, 2.464500e+00,\n",
              "       4.087500e+00, 7.901000e-01, 2.705400e+00, 2.220100e+01,\n",
              "       1.136610e+01, 1.341000e+00, 3.011000e-01, 2.708800e+00,\n",
              "       1.843600e+00, 1.064120e+01, 7.000000e-01, 1.224700e+00,\n",
              "       8.867800e+00, 2.095213e+02, 5.993700e+00, 4.406600e+00,\n",
              "       7.023400e+00, 6.979200e+00, 1.093600e+00, 1.122200e+00,\n",
              "       3.276600e+00, 1.066340e+01, 4.899400e+00, 7.189300e+00,\n",
              "       1.988000e-01, 1.526800e+00, 1.697360e+01, 8.594200e+00,\n",
              "       2.654000e+00, 2.173700e+00, 6.686170e+01, 3.437400e+00,\n",
              "       5.517100e+00, 3.023700e+00, 5.697200e+00, 9.280400e+00,\n",
              "       4.510400e+00, 5.441000e+00, 2.116000e+00, 3.199900e+00,\n",
              "       6.705800e+00, 7.324400e+00, 6.662500e+00, 1.936000e+00,\n",
              "       2.869260e+01, 2.352700e+00, 1.000000e-02, 4.564300e+00,\n",
              "       1.412800e+00, 5.296800e+00, 3.386900e+00, 1.336700e+00,\n",
              "       1.774650e+01, 2.835000e+00, 5.110000e-01, 1.592300e+00,\n",
              "       4.288600e+00, 8.488000e-01, 4.227100e+00, 8.029400e+00,\n",
              "       6.313200e+00, 3.223400e+00, 5.152000e-01, 3.109000e-01,\n",
              "       1.008450e+01, 2.775800e+00, 2.497100e+00, 7.942400e+00,\n",
              "       4.512450e+01, 1.197120e+01, 1.455600e+00, 3.450100e+00,\n",
              "       8.993400e+00, 2.149700e+00, 7.512000e+00, 1.059370e+01,\n",
              "       1.200700e+00, 2.802200e+00, 2.506600e+01, 9.206100e+00,\n",
              "       3.007400e+00, 7.766400e+00, 2.836000e-01, 1.535200e+00,\n",
              "       1.025450e+01, 4.502200e+00, 1.030770e+01, 3.193000e+00,\n",
              "       8.574000e-01, 8.415000e-01, 3.456600e+00, 4.135000e-01,\n",
              "       1.027600e+00, 1.878380e+01, 9.571000e-01, 2.735200e+00,\n",
              "       2.127200e+00, 3.872300e+00, 1.541000e+00, 3.711400e+00,\n",
              "       6.632500e+00, 2.668310e+01, 1.098400e+00, 2.856000e+00,\n",
              "       9.488000e-01, 2.485100e+00, 3.246000e+00, 2.269800e+00,\n",
              "       2.707500e+00, 2.344800e+00, 4.450000e+00, 6.819000e-01,\n",
              "       4.545400e+00, 7.623000e-01, 2.014200e+00, 4.364200e+00,\n",
              "       5.423000e-01, 5.734800e+00, 2.946600e+00, 1.157100e+00,\n",
              "       9.292000e-01, 5.222400e+00, 1.393500e+00, 2.431800e+00,\n",
              "       6.426000e-01, 2.156202e+02, 2.898800e+00, 1.731300e+01,\n",
              "       2.493300e+00, 2.922100e+00, 2.143400e+00, 4.868800e+00,\n",
              "       2.674000e+00, 1.270070e+01, 2.914300e+00, 1.414100e+00,\n",
              "       2.981200e+00, 2.363100e+00, 1.064000e+00, 2.672000e+00,\n",
              "       2.219590e+01, 7.788000e-01, 2.520900e+00, 2.206600e+00,\n",
              "       3.272900e+00, 1.535400e+00, 6.455060e+01, 2.315000e+00,\n",
              "       2.078820e+01, 7.439200e+00, 1.806090e+01, 6.031900e+00,\n",
              "       6.003300e+00, 9.428000e-01, 1.288600e+00, 1.829710e+01,\n",
              "       1.719300e+00, 1.165940e+01, 6.883000e+00, 5.945000e-01,\n",
              "       1.898970e+01, 8.226000e-01, 1.646900e+00, 2.122660e+01,\n",
              "       2.300400e+00, 1.173300e+00, 4.180600e+00, 1.559430e+01,\n",
              "       3.195800e+00, 1.242400e+01, 2.805400e+00, 2.211800e+00,\n",
              "       6.479200e+00, 3.039400e+00, 2.883050e+01, 8.338900e+00,\n",
              "       1.854400e+00, 8.921000e-01, 5.095000e-01, 6.070600e+00,\n",
              "       3.828900e+00, 1.860700e+00, 6.841400e+00, 7.804000e+00,\n",
              "       2.236000e-01, 3.489000e-01, 3.039700e+00, 5.235770e+01,\n",
              "       3.142100e+00, 5.249500e+00, 6.134700e+00, 2.522700e+00,\n",
              "       1.120200e+00, 4.101000e-01, 2.247700e+00, 2.711400e+00,\n",
              "       2.186900e+00, 4.925700e+00, 1.510300e+00, 1.067310e+01,\n",
              "       9.555900e+00, 4.120830e+01, 5.012300e+00, 1.472000e+00,\n",
              "       1.666080e+01, 5.996200e+00, 2.136200e+00, 2.465900e+00,\n",
              "       1.689300e+00, 2.317240e+01, 1.990400e+01, 2.539600e+00,\n",
              "       1.129300e+00, 1.975600e+00, 9.097500e+00, 2.242100e+00,\n",
              "       6.205100e+00, 8.646000e-01, 7.092000e-01, 1.129600e+00,\n",
              "       3.449300e+00, 7.517100e+00, 2.356500e+00, 4.839200e+00,\n",
              "       3.684600e+00, 1.442840e+01, 1.463300e+00, 7.496000e-01,\n",
              "       1.335510e+01, 1.153830e+01, 9.931300e+00, 1.290600e+01,\n",
              "       3.699000e-01, 2.701000e+00, 2.627800e+00, 3.258700e+00,\n",
              "       5.206100e+00, 7.099000e+00, 7.795500e+00, 2.133200e+00,\n",
              "       1.982400e+00, 3.715280e+01, 6.568600e+00, 5.604300e+00],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3TWpN0nVTpUx"
      },
      "source": [
        "## Question 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WQ1FKEs-4btX"
      },
      "source": [
        "### Normalize data\n",
        "- Normalize features\n",
        "- Use tf.math.l2_normalize to normalize features\n",
        "- You can read more about it here https://www.tensorflow.org/api_docs/python/tf/math/l2_normalize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V0Tfe00X78wB",
        "colab": {}
      },
      "source": [
        "X=tf.math.l2_normalize(\n",
        "    X,\n",
        "    axis=None,\n",
        "    epsilon=1e-12,\n",
        "    name=None\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wmXUGc2oTspa"
      },
      "source": [
        "## Question 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VJelDMpzxs0L"
      },
      "source": [
        "### Define weight and bias\n",
        "- Initialize weight and bias with tf.zeros\n",
        "- tf.zeros is an initializer that generates tensors initialized to 0\n",
        "- Specify the value for shape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8o9RPWVTxs0O",
        "colab": {}
      },
      "source": [
        "W = tf.zeros(shape=(4, 1))\n",
        "b = tf.zeros(shape=(1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8a0wr94aTyjg"
      },
      "source": [
        "## Question 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zMXXYdOSxs0Q"
      },
      "source": [
        "### Get prediction\n",
        "- Define a function to get prediction\n",
        "- Approach: prediction = (X * W) + b; here is X is features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U8Cty1y0xs0S",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def prediction(features, W, b):\n",
        "    y_pred = tf.add(tf.matmul(features, W), b)\n",
        "    return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lQmS3Tauxs0V"
      },
      "source": [
        "### Calculate loss\n",
        "- Calculate loss using predictions\n",
        "- Define a function to calculate loss\n",
        "- We are calculating mean squared error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-FRXmDd5xs0X",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def loss(y_actual, y_predicted):\n",
        "    diff = y_actual - y_predicted\n",
        "    sqr = tf.square(diff)\n",
        "    avg = tf.reduce_mean(sqr)\n",
        "    return avg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZbBpnOtfT0wd"
      },
      "source": [
        "## Question 9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bkOzAUUsTmF_"
      },
      "source": [
        "### Define a function to train the model\n",
        "1.   Record all the mathematical steps to calculate Loss\n",
        "2.   Calculate Gradients of Loss w.r.t weights and bias\n",
        "3.   Update Weights and Bias based on gradients and learning rate to minimize loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2R4uieGYLYtM",
        "colab": {}
      },
      "source": [
        "def train(x, y_actual, w, b, learning_rate=0.01):\n",
        "    \n",
        "    # Record mathematical operations on 'tape' to calculate loss\n",
        "    with tf.GradientTape() as t:\n",
        "        t.watch([w,b])\n",
        "        current_prediction = prediction(x, w, b)\n",
        "        current_loss = loss(y_actual, current_prediction)\n",
        "    \n",
        "    # Calculate Gradients for Loss with respect to Weights and Bias\n",
        "    dw, db = t.gradient(current_loss,[w, b])\n",
        "    \n",
        "    # Update Weights and Bias\n",
        "    w = w - learning_rate * dw\n",
        "    b = b - learning_rate * db\n",
        "    \n",
        "    return w, b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AW4SEP8kT2ls"
      },
      "source": [
        "## Question 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yeN0deOvT81N"
      },
      "source": [
        "### Train the model for 100 epochs \n",
        "- Observe the training loss at every iteration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Jjkn4gUgLevE",
        "outputId": "76c3af04-b49d-44af-eefe-e7a39c3e1309",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for i in range(100):    \n",
        "    W, b = train(X, Y, W, b)\n",
        "    print('Current Training Loss on iteration', i, loss(Y, prediction(X, W, b)))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current Training Loss on iteration 0 tf.Tensor(236.16759, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 1 tf.Tensor(235.09299, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 2 tf.Tensor(234.06102, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 3 tf.Tensor(233.06998, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 4 tf.Tensor(232.11781, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 5 tf.Tensor(231.20381, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 6 tf.Tensor(230.32585, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 7 tf.Tensor(229.48265, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 8 tf.Tensor(228.67297, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 9 tf.Tensor(227.89539, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 10 tf.Tensor(227.14832, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 11 tf.Tensor(226.43114, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 12 tf.Tensor(225.74234, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 13 tf.Tensor(225.0808, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 14 tf.Tensor(224.44553, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 15 tf.Tensor(223.835, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 16 tf.Tensor(223.24931, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 17 tf.Tensor(222.68655, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 18 tf.Tensor(222.14595, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 19 tf.Tensor(221.62688, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 20 tf.Tensor(221.12842, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 21 tf.Tensor(220.6498, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 22 tf.Tensor(220.18993, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 23 tf.Tensor(219.74841, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 24 tf.Tensor(219.32425, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 25 tf.Tensor(218.91705, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 26 tf.Tensor(218.52579, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 27 tf.Tensor(218.15027, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 28 tf.Tensor(217.78957, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 29 tf.Tensor(217.44315, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 30 tf.Tensor(217.11034, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 31 tf.Tensor(216.79077, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 32 tf.Tensor(216.48412, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 33 tf.Tensor(216.18932, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 34 tf.Tensor(215.90611, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 35 tf.Tensor(215.63437, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 36 tf.Tensor(215.37334, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 37 tf.Tensor(215.12263, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 38 tf.Tensor(214.88185, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 39 tf.Tensor(214.65053, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 40 tf.Tensor(214.42859, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 41 tf.Tensor(214.21527, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 42 tf.Tensor(214.01044, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 43 tf.Tensor(213.81369, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 44 tf.Tensor(213.62482, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 45 tf.Tensor(213.44333, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 46 tf.Tensor(213.2691, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 47 tf.Tensor(213.10179, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 48 tf.Tensor(212.94106, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 49 tf.Tensor(212.78687, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 50 tf.Tensor(212.63857, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 51 tf.Tensor(212.4961, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 52 tf.Tensor(212.35948, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 53 tf.Tensor(212.22816, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 54 tf.Tensor(212.10205, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 55 tf.Tensor(211.98093, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 56 tf.Tensor(211.86461, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 57 tf.Tensor(211.75293, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 58 tf.Tensor(211.64558, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 59 tf.Tensor(211.54266, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 60 tf.Tensor(211.44374, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 61 tf.Tensor(211.34874, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 62 tf.Tensor(211.2575, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 63 tf.Tensor(211.16982, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 64 tf.Tensor(211.08572, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 65 tf.Tensor(211.00484, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 66 tf.Tensor(210.92729, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 67 tf.Tensor(210.85278, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 68 tf.Tensor(210.78104, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 69 tf.Tensor(210.71225, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 70 tf.Tensor(210.64633, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 71 tf.Tensor(210.58289, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 72 tf.Tensor(210.52199, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 73 tf.Tensor(210.46347, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 74 tf.Tensor(210.4073, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 75 tf.Tensor(210.3533, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 76 tf.Tensor(210.30153, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 77 tf.Tensor(210.2518, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 78 tf.Tensor(210.204, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 79 tf.Tensor(210.15811, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 80 tf.Tensor(210.1141, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 81 tf.Tensor(210.0717, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 82 tf.Tensor(210.03108, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 83 tf.Tensor(209.99205, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 84 tf.Tensor(209.95453, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 85 tf.Tensor(209.91852, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 86 tf.Tensor(209.88396, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 87 tf.Tensor(209.85075, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 88 tf.Tensor(209.81891, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 89 tf.Tensor(209.78818, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 90 tf.Tensor(209.75874, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 91 tf.Tensor(209.73053, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 92 tf.Tensor(209.70349, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 93 tf.Tensor(209.67744, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 94 tf.Tensor(209.65225, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 95 tf.Tensor(209.62833, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 96 tf.Tensor(209.6052, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 97 tf.Tensor(209.58302, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 98 tf.Tensor(209.56174, shape=(), dtype=float32)\n",
            "Current Training Loss on iteration 99 tf.Tensor(209.5413, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vanvD93FV0_k"
      },
      "source": [
        "### Observe values of Weight\n",
        "- Print the updated values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QSqpy4gtWaOD",
        "outputId": "30200d17-f494-48a3-f011-0e530816afcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "print(W)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[0.0547828 ]\n",
            " [0.05488191]\n",
            " [0.05422735]\n",
            " [0.05533117]], shape=(4, 1), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "y9KpRupYUEwy"
      },
      "source": [
        "### Observe values of Bias\n",
        "- Print the updated values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bhEWkGqHWohg",
        "outputId": "36f15673-27ba-401d-dc62-14e42959461e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(b)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([4.607657], shape=(1,), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGLnHAOC4wDr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}