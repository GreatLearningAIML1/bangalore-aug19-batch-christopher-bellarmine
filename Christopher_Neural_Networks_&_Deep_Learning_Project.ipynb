{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Christopher_Neural_Networks_&_Deep_Learning_Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lz9bffGcyhn8",
        "outputId": "dbc00b4b-9f5b-4644-d291-765d9435acd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "# Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "X8PZN4ee1mJI",
        "colab": {}
      },
      "source": [
        "# Setting the current working directory\n",
        "import os; os.chdir('drive/My Drive/Colab Notebooks')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6DsvU1eB2dJ9"
      },
      "source": [
        "<a id='import'></a>\n",
        "### Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YdPPqfcR17r5",
        "outputId": "4fb32597-09e2-4b30-d287-93353ff35403",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        }
      },
      "source": [
        "# Imports\n",
        "import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns, h5py\n",
        "import matplotlib.style as style; style.use('fivethirtyeight')\n",
        "%matplotlib inline\n",
        "\n",
        "# Metrics and preprocessing\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, precision_recall_curve, auc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# TF and Keras\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.layers import Activation, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "# Checking if GPU is found\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "tf.reset_default_graph()\n",
        "tf.set_random_seed(42)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "X4jslkxXdFB1"
      },
      "source": [
        "<a id='load'></a>\n",
        "### Load train, val and test datasets from h5py file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kdolI00z7KOf",
        "outputId": "72eafd88-2453-4154-8393-697e64f644ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "# Read the h5 file\n",
        "h5_SVH = h5py.File('SVHN_single_grey1.h5', 'r')\n",
        "\n",
        "# Load the training, validation and test sets\n",
        "X_train = h5_SVH['X_train'][:]\n",
        "y_train_o = h5_SVH['y_train'][:]\n",
        "X_val = h5_SVH['X_val'][:]\n",
        "y_val_o = h5_SVH['y_val'][:]\n",
        "X_test = h5_SVH['X_test'][:]\n",
        "y_test_o = h5_SVH['y_test'][:]\n",
        "\n",
        "# Close this file\n",
        "\n",
        "h5_SVH.close()\n",
        "\n",
        "print('Training set', X_train.shape, y_train_o.shape)\n",
        "print('Validation set', X_val.shape, y_val_o.shape)\n",
        "print('Test set', X_test.shape, y_test_o.shape)\n",
        "\n",
        "print('\\n')\n",
        "print('Unique labels in y_train :', np.unique(y_train_o))\n",
        "print('Unique labels in y_val   :', np.unique(y_val_o))\n",
        "print('Unique labels in y_test  :', np.unique(y_test_o))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set (42000, 32, 32) (42000,)\n",
            "Validation set (60000, 32, 32) (60000,)\n",
            "Test set (18000, 32, 32) (18000,)\n",
            "\n",
            "\n",
            "Unique labels in y_train : [0 1 2 3 4 5 6 7 8 9]\n",
            "Unique labels in y_val   : [0 1 2 3 4 5 6 7 8 9]\n",
            "Unique labels in y_test  : [0 1 2 3 4 5 6 7 8 9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OwanQxdD9mBC"
      },
      "source": [
        "<a id='o1'></a>\n",
        "#### Observation 1 - Sets Shape\n",
        "* Length of training sets: 42k, validation sets: 60k, test sets: 18k\n",
        "* Size of the images: 32*32\n",
        "* Number of class: 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "g8Zur6Qxs92_"
      },
      "source": [
        "<a id='visualize'></a>\n",
        "### Visualizing first 10 images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wuXKfSv58lbK",
        "outputId": "4383ada9-51ac-4722-a698-ad866b9194f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "# Visualizing first 10 images in the dataset and their labels\n",
        "plt.figure(figsize = (15, 4.5))\n",
        "for i in range(10):  \n",
        "    plt.subplot(1, 10, i+1)\n",
        "    plt.imshow(X_train[i].reshape((32, 32)),cmap = plt.cm.binary)\n",
        "    plt.axis('off')\n",
        "plt.subplots_adjust(wspace = -0.1, hspace = -0.1)\n",
        "plt.show()\n",
        "\n",
        "print('Label for each of the above image: %s' % (y_train_o[0 : 10]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7kAAAB1CAYAAACLZSaSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 1080x324 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Label for each of the above image: [2 6 7 4 4 0 3 0 7 3]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Td7LigoTynL2",
        "outputId": "94570b7b-184c-41bd-9a5d-68c4178c02e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "print('Checking first image and label in training set'); print('--'*40)\n",
        "plt.imshow(X_train[0], cmap = plt.cm.binary)    \n",
        "plt.show()\n",
        "print('Label:', y_train_o[0])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checking first image and label in training set\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD1CAYAAAB9TzjVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYmjghETM/wIo2e2Bn1dKLgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Label: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Bcpa-MFxy0Zt",
        "outputId": "bea93993-280b-4ec6-a526-205bc5cb670c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "print('Checking first image and label in validation set'); print('--'*40)\n",
        "plt.imshow(X_val[0], cmap = plt.cm.binary)    \n",
        "plt.show()\n",
        "print('Label:', y_val_o[0])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checking first image and label in validation set\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD1CAYAAAB9TzjVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYeTgQlQYObgQFebfAfmL0QQbDJBwAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Label: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "r1OIeig-zHap",
        "outputId": "dcd69079-226f-4125-ae09-94cac6a5cdaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "print('Checking first image and label in test set'); print('--'*40)\n",
        "plt.imshow(X_test[0], cmap = plt.cm.binary)    \n",
        "plt.show()\n",
        "print('Label:', y_test_o[0])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checking first image and label in test set\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD1CAYAAAB9TzjVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Label: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fEQ-CNcVdr-c"
      },
      "source": [
        "<a id='flatten'></a>\n",
        "### Flatten and normalize the images for Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t4cfwZ9ldlO0",
        "outputId": "c3bcd8cf-9647-4270-9e57-3fad4f88cc26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "print('Reshaping X data: (n, 32, 32) => (n, 1024)'); print('--'*40)\n",
        "X_train = X_train.reshape((X_train.shape[0], -1))\n",
        "X_val = X_val.reshape((X_val.shape[0], -1))\n",
        "X_test = X_test.reshape((X_test.shape[0], -1))\n",
        "\n",
        "print('Making sure that the values are float so that we can get decimal points after division'); print('--'*40)\n",
        "X_train = X_train.astype('float32')\n",
        "X_val = X_val.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "print('Normalizing the RGB codes by dividing it to the max RGB value'); print('--'*40)\n",
        "X_train /= 255\n",
        "X_val /= 255\n",
        "X_test /= 255\n",
        "\n",
        "print('Converting y data into categorical (one-hot encoding)'); print('--'*40)\n",
        "y_train = to_categorical(y_train_o)\n",
        "y_val = to_categorical(y_val_o)\n",
        "y_test = to_categorical(y_test_o)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reshaping X data: (n, 32, 32) => (n, 1024)\n",
            "--------------------------------------------------------------------------------\n",
            "Making sure that the values are float so that we can get decimal points after division\n",
            "--------------------------------------------------------------------------------\n",
            "Normalizing the RGB codes by dividing it to the max RGB value\n",
            "--------------------------------------------------------------------------------\n",
            "Converting y data into categorical (one-hot encoding)\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "evUXte-cnfRN",
        "outputId": "59ae4ac2-02ed-4990-f970-c7ca607c139e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "print('X_train shape:', X_train.shape)\n",
        "print('X_val shape:', X_val.shape)\n",
        "print('X_test shape:', X_test.shape)\n",
        "\n",
        "print('\\n')\n",
        "print('y_train shape:', y_train.shape)\n",
        "print('y_val shape:', y_val.shape)\n",
        "print('y_test shape:', y_test.shape)\n",
        "\n",
        "print('\\n')\n",
        "print('Number of images in X_train', X_train.shape[0])\n",
        "print('Number of images in X_val', X_val.shape[0])\n",
        "print('Number of images in X_test', X_test.shape[0])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape: (42000, 1024)\n",
            "X_val shape: (60000, 1024)\n",
            "X_test shape: (18000, 1024)\n",
            "\n",
            "\n",
            "y_train shape: (42000, 10)\n",
            "y_val shape: (60000, 10)\n",
            "y_test shape: (18000, 10)\n",
            "\n",
            "\n",
            "Number of images in X_train 42000\n",
            "Number of images in X_val 60000\n",
            "Number of images in X_test 18000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fGKy-ZD1YAWv"
      },
      "source": [
        "<a id='Baby'></a>\n",
        "### Modelling - Baby sitting the learning process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JX35qWq_AyLZ"
      },
      "source": [
        "#### Fully connected linear layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nyQvFvwMX_pI",
        "colab": {}
      },
      "source": [
        "class Linear():\n",
        "    def __init__(self, in_size, out_size):\n",
        "        self.W = np.random.randn(in_size, out_size) * 0.01\n",
        "        self.b = np.zeros((1, out_size))\n",
        "        self.params = [self.W, self.b]\n",
        "        self.gradW = None\n",
        "        self.gradB = None\n",
        "        self.gradInput = None        \n",
        "\n",
        "    def forward(self, X):\n",
        "        self.X = X\n",
        "        self.output = np.dot(X, self.W) + self.b\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, nextgrad):\n",
        "        self.gradW = np.dot(self.X.T, nextgrad)\n",
        "        self.gradB = np.sum(nextgrad, axis=0)\n",
        "        self.gradInput = np.dot(nextgrad, self.W.T)\n",
        "        return self.gradInput, [self.gradW, self.gradB]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CQegqIQXA1Wi"
      },
      "source": [
        "#### ReLU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A0HxN2PnaaaM",
        "colab": {}
      },
      "source": [
        "class ReLU():\n",
        "    def __init__(self):\n",
        "        self.params = []\n",
        "        self.gradInput = None\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.output = np.maximum(X, 0)\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, nextgrad):\n",
        "        self.gradInput = nextgrad.copy()\n",
        "        self.gradInput[self.output <=0] = 0\n",
        "        return self.gradInput, []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5CwuYJdhA7sz"
      },
      "source": [
        "#### Softmax function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5AxCBE9fagiP",
        "colab": {}
      },
      "source": [
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x6X9_AZeA-61"
      },
      "source": [
        "#### Cross entropy loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tbjl_6qYaq80",
        "colab": {}
      },
      "source": [
        "class CrossEntropy:\n",
        "    def forward(self, X, y):\n",
        "        self.m = y.shape[0]\n",
        "        self.p = softmax(X)\n",
        "        cross_entropy = -np.log(self.p[range(self.m), y]+1e-16)\n",
        "        loss = np.sum(cross_entropy) / self.m\n",
        "        return loss\n",
        "    \n",
        "    def backward(self, X, y):\n",
        "        y_idx = y.argmax()        \n",
        "        grad = softmax(X)\n",
        "        grad[range(self.m), y] -= 1\n",
        "        grad /= self.m\n",
        "        return grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "g-dLPKqbBCuT"
      },
      "source": [
        "#### NN class that enables the forward prop and backward propagation of the entire network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aOUQbEfIawH0",
        "colab": {}
      },
      "source": [
        "class NN():\n",
        "    def __init__(self, lossfunc = CrossEntropy(), mode = 'train'):\n",
        "        self.params = []\n",
        "        self.layers = []\n",
        "        self.loss_func = lossfunc\n",
        "        self.grads = []\n",
        "        self.mode = mode\n",
        "        \n",
        "    def add_layer(self, layer):\n",
        "        self.layers.append(layer)\n",
        "        self.params.append(layer.params)\n",
        "\n",
        "    def forward(self, X):\n",
        "        for layer in self.layers:\n",
        "            X = layer.forward(X)\n",
        "        return X\n",
        "    \n",
        "    def backward(self, nextgrad):\n",
        "        self.clear_grad_param()\n",
        "        for layer in reversed(self.layers):\n",
        "            nextgrad, grad = layer.backward(nextgrad)\n",
        "            self.grads.append(grad)\n",
        "        return self.grads\n",
        "    \n",
        "    def train_step(self, X, y):\n",
        "        out = self.forward(X)\n",
        "        loss = self.loss_func.forward(out,y)\n",
        "        nextgrad = self.loss_func.backward(out,y)\n",
        "        grads = self.backward(nextgrad)\n",
        "        return loss, grads\n",
        "    \n",
        "    def predict(self, X):\n",
        "        X = self.forward(X)\n",
        "        p = softmax(X)\n",
        "        return np.argmax(p, axis=1)\n",
        "    \n",
        "    def predict_scores(self, X):\n",
        "        X = self.forward(X)\n",
        "        p = softmax(X)\n",
        "        return p\n",
        "    \n",
        "    def clear_grad_param(self):\n",
        "        self.grads = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mHLu_qtoBGYg"
      },
      "source": [
        "#### Update function SGD with momentum"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XpEjX7qqbN9q",
        "colab": {}
      },
      "source": [
        "def update(velocity, params, grads, learning_rate=0.01, mu=0.9):\n",
        "    for v, p, g, in zip(velocity, params, reversed(grads)):\n",
        "        for i in range(len(g)):\n",
        "            v[i] = (mu * v[i]) - (learning_rate * g[i])\n",
        "            p[i] += v[i]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YQnT5wBFBJUH"
      },
      "source": [
        "#### Get minibatches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xYxPXK7Fbfgk",
        "colab": {}
      },
      "source": [
        "def minibatch(X, y, minibatch_size):\n",
        "    n = X.shape[0]\n",
        "    minibatches = []\n",
        "    permutation = np.random.permutation(X.shape[0])\n",
        "    X = X[permutation]\n",
        "    y = y[permutation]\n",
        "    \n",
        "    for i in range(0, n , minibatch_size):\n",
        "        X_batch = X[i:i + minibatch_size, :]\n",
        "        y_batch = y[i:i + minibatch_size, ]\n",
        "        minibatches.append((X_batch, y_batch))\n",
        "        \n",
        "    return minibatches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ScfG6m51BQfz"
      },
      "source": [
        "#### The training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y6F3W-wfbpqC",
        "colab": {}
      },
      "source": [
        "def train(net, X_train, y_train, minibatch_size, epoch, learning_rate, mu = 0.9, X_val = None, y_val = None, Lambda = 0, verb = True):\n",
        "    val_loss_epoch = []\n",
        "    minibatches = minibatch(X_train, y_train, minibatch_size)\n",
        "    minibatches_val = minibatch(X_val, y_val, minibatch_size)\n",
        "    \n",
        "    for i in range(epoch):\n",
        "        loss_batch = []\n",
        "        val_loss_batch = []\n",
        "        velocity = []\n",
        "        for param_layer in net.params:\n",
        "            p = [np.zeros_like(param) for param in list(param_layer)]\n",
        "            velocity.append(p)\n",
        "            \n",
        "        # iterate over mini batches\n",
        "        for X_mini, y_mini in minibatches:\n",
        "            loss, grads = net.train_step(X_mini, y_mini)\n",
        "            loss_batch.append(loss)\n",
        "            update(velocity, net.params, grads, learning_rate=learning_rate, mu=mu)\n",
        "\n",
        "        for X_mini_val, y_mini_val in minibatches_val:\n",
        "            val_loss, _ = net.train_step(X_mini, y_mini)\n",
        "            val_loss_batch.append(val_loss)\n",
        "        \n",
        "        # accuracy of model at end of epoch after all mini batch updates\n",
        "        m_train = X_train.shape[0]\n",
        "        m_val = X_val.shape[0]\n",
        "        y_train_pred = []\n",
        "        y_val_pred = []\n",
        "        y_train1 = []\n",
        "        y_vall = []\n",
        "        for ii in range(0, m_train, minibatch_size):\n",
        "            X_tr = X_train[ii:ii + minibatch_size, : ]\n",
        "            y_tr = y_train[ii:ii + minibatch_size,]\n",
        "            y_train1 = np.append(y_train1, y_tr)\n",
        "            y_train_pred = np.append(y_train_pred, net.predict(X_tr))\n",
        "\n",
        "        for ii in range(0, m_val, minibatch_size):\n",
        "            X_va = X_val[ii:ii + minibatch_size, : ]\n",
        "            y_va = y_val[ii:ii + minibatch_size,]\n",
        "            y_vall = np.append(y_vall, y_va)\n",
        "            y_val_pred = np.append(y_val_pred, net.predict(X_va))\n",
        "            \n",
        "        train_acc = check_accuracy(y_train1, y_train_pred)\n",
        "        val_acc = check_accuracy(y_vall, y_val_pred)\n",
        "        \n",
        "        ## weights\n",
        "        w = np.array(net.params[0][0])\n",
        "        \n",
        "        ## adding regularization to cost\n",
        "        mean_train_loss = (sum(loss_batch) / float(len(loss_batch)))\n",
        "        mean_val_loss = sum(val_loss_batch) / float(len(val_loss_batch))\n",
        "        \n",
        "        val_loss_epoch.append(mean_val_loss)\n",
        "        if verb:\n",
        "            if i%50==0:\n",
        "                print(\"Epoch {3}/{4}: Loss = {0} | Training Accuracy = {1}\".format(mean_train_loss, train_acc, val_acc, i, epoch))\n",
        "    return net, val_acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kFKQ9JIPBU2L"
      },
      "source": [
        "#### Checking the accuracy of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gAAOfhBebz8d",
        "colab": {}
      },
      "source": [
        "def check_accuracy(y_true, y_pred):\n",
        "    return np.mean(y_pred == y_true)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iagKp2kvBaZD"
      },
      "source": [
        "#### Invoking all that we have created until now"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vA1-5ojib7XH",
        "colab": {}
      },
      "source": [
        "# Invoking the model\n",
        "## input size\n",
        "input_dim = X_train.shape[1]\n",
        "\n",
        "def train_and_test_loop(iterations, lr, Lambda, verb = True):\n",
        "    ## hyperparameters\n",
        "    iterations = iterations\n",
        "    learning_rate = lr\n",
        "    hidden_nodes1 = 10\n",
        "    output_nodes = 10\n",
        "\n",
        "    ## define neural net\n",
        "    nn = NN()\n",
        "    nn.add_layer(Linear(input_dim, hidden_nodes1))\n",
        "\n",
        "    nn, val_acc = train(nn, X_train, y_train_o, minibatch_size = 200, epoch = iterations, learning_rate = learning_rate,\\\n",
        "                      X_val = X_test, y_val = y_test_o, Lambda = Lambda, verb = verb)\n",
        "    return val_acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_JhBJRfvBfiC"
      },
      "source": [
        "#### Double Check that the loss is reasonable : Disable the regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fjbjgrHl6LBv",
        "outputId": "446deb18-928c-403a-d236-cd97ca61b468",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "lr = 0.00001\n",
        "Lambda = 0\n",
        "train_and_test_loop(1, lr, Lambda)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/1: Loss = 2.309373743409679 | Training Accuracy = 0.09297619047619048\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.09216666666666666"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wFbKNxLpBl7i"
      },
      "source": [
        "#### Now, lets crank up the Lambda(Regularization) and check what it does to our loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_OtyvtW39bmm",
        "outputId": "6abfcf8f-c243-44ba-ecff-7f4109466c2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "lr = 0.00001\n",
        "Lambda = 1e3\n",
        "train_and_test_loop(1, lr, Lambda)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/1: Loss = 2.3119503381058504 | Training Accuracy = 0.10057142857142858\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.10316666666666667"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7Zq7TGKfBped"
      },
      "source": [
        "#### Now, lets overfit to a small subset of our dataset, in this case 20 images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "882INFrA9wSF",
        "outputId": "083cddd6-a787-40c6-8720-62b5d6f4d51e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train_subset = X_train[0:20]\n",
        "y_train_subset = y_train_o[0:20]\n",
        "\n",
        "X_train = X_train_subset\n",
        "y_train_o = y_train_subset\n",
        "\n",
        "X_train.shape, y_train_o.shape"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((20, 1024), (20,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fmS0Eicu-T00"
      },
      "source": [
        "#### Make sure that you can overfit very small portion of the training data\n",
        "So, set a small learning rate and turn regularization off\n",
        "In the code below:\n",
        "* Take the first 20 examples\n",
        "* turn off regularization(reg=0.0)\n",
        "* use simple vanilla 'sgd'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "adxWZ2NO-OrY",
        "outputId": "920cf945-235e-485f-b31e-5e07c9f1cfaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%time\n",
        "lr = 0.001\n",
        "Lambda = 0\n",
        "train_and_test_loop(5000, lr, Lambda)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 6 µs, sys: 2 µs, total: 8 µs\n",
            "Wall time: 8.11 µs\n",
            "Epoch 0/5000: Loss = 2.2912052054756744 | Training Accuracy = 0.2\n",
            "Epoch 50/5000: Loss = 1.9326575049595554 | Training Accuracy = 0.25\n",
            "Epoch 100/5000: Loss = 1.8482099256335585 | Training Accuracy = 0.35\n",
            "Epoch 150/5000: Loss = 1.7943376433438991 | Training Accuracy = 0.35\n",
            "Epoch 200/5000: Loss = 1.7496415711149464 | Training Accuracy = 0.5\n",
            "Epoch 250/5000: Loss = 1.709425690282347 | Training Accuracy = 0.5\n",
            "Epoch 300/5000: Loss = 1.6720401663002735 | Training Accuracy = 0.5\n",
            "Epoch 350/5000: Loss = 1.6367355519905478 | Training Accuracy = 0.5\n",
            "Epoch 400/5000: Loss = 1.6031114234034636 | Training Accuracy = 0.55\n",
            "Epoch 450/5000: Loss = 1.5709261368685572 | Training Accuracy = 0.55\n",
            "Epoch 500/5000: Loss = 1.5400184464453295 | Training Accuracy = 0.55\n",
            "Epoch 550/5000: Loss = 1.5102714307411595 | Training Accuracy = 0.55\n",
            "Epoch 600/5000: Loss = 1.4815945100541206 | Training Accuracy = 0.6\n",
            "Epoch 650/5000: Loss = 1.453913869536486 | Training Accuracy = 0.65\n",
            "Epoch 700/5000: Loss = 1.4271670500006648 | Training Accuracy = 0.65\n",
            "Epoch 750/5000: Loss = 1.4012997231512678 | Training Accuracy = 0.65\n",
            "Epoch 800/5000: Loss = 1.3762636706479814 | Training Accuracy = 0.65\n",
            "Epoch 850/5000: Loss = 1.3520154579503223 | Training Accuracy = 0.65\n",
            "Epoch 900/5000: Loss = 1.3285155266189008 | Training Accuracy = 0.65\n",
            "Epoch 950/5000: Loss = 1.3057275487126094 | Training Accuracy = 0.65\n",
            "Epoch 1000/5000: Loss = 1.283617951318297 | Training Accuracy = 0.65\n",
            "Epoch 1050/5000: Loss = 1.2621555551637713 | Training Accuracy = 0.65\n",
            "Epoch 1100/5000: Loss = 1.2413112920266884 | Training Accuracy = 0.65\n",
            "Epoch 1150/5000: Loss = 1.2210579780617627 | Training Accuracy = 0.7\n",
            "Epoch 1200/5000: Loss = 1.2013701278176696 | Training Accuracy = 0.7\n",
            "Epoch 1250/5000: Loss = 1.1822237985627106 | Training Accuracy = 0.7\n",
            "Epoch 1300/5000: Loss = 1.1635964576888842 | Training Accuracy = 0.7\n",
            "Epoch 1350/5000: Loss = 1.1454668680583528 | Training Accuracy = 0.7\n",
            "Epoch 1400/5000: Loss = 1.1278149875770538 | Training Accuracy = 0.75\n",
            "Epoch 1450/5000: Loss = 1.1106218802617107 | Training Accuracy = 0.75\n",
            "Epoch 1500/5000: Loss = 1.0938696367558396 | Training Accuracy = 0.75\n",
            "Epoch 1550/5000: Loss = 1.0775413027416487 | Training Accuracy = 0.8\n",
            "Epoch 1600/5000: Loss = 1.0616208140497239 | Training Accuracy = 0.8\n",
            "Epoch 1650/5000: Loss = 1.0460929375280255 | Training Accuracy = 0.8\n",
            "Epoch 1700/5000: Loss = 1.0309432169238413 | Training Accuracy = 0.85\n",
            "Epoch 1750/5000: Loss = 1.0161579231760531 | Training Accuracy = 0.85\n",
            "Epoch 1800/5000: Loss = 1.0017240086237404 | Training Accuracy = 0.85\n",
            "Epoch 1850/5000: Loss = 0.9876290647201978 | Training Accuracy = 0.85\n",
            "Epoch 1900/5000: Loss = 0.9738612829056246 | Training Accuracy = 0.85\n",
            "Epoch 1950/5000: Loss = 0.9604094183419398 | Training Accuracy = 0.85\n",
            "Epoch 2000/5000: Loss = 0.947262756252923 | Training Accuracy = 0.85\n",
            "Epoch 2050/5000: Loss = 0.9344110806447903 | Training Accuracy = 0.85\n",
            "Epoch 2100/5000: Loss = 0.9218446452082816 | Training Accuracy = 0.85\n",
            "Epoch 2150/5000: Loss = 0.909554146224808 | Training Accuracy = 0.85\n",
            "Epoch 2200/5000: Loss = 0.8975306973172117 | Training Accuracy = 0.85\n",
            "Epoch 2250/5000: Loss = 0.8857658059010165 | Training Accuracy = 0.9\n",
            "Epoch 2300/5000: Loss = 0.874251351205289 | Training Accuracy = 0.95\n",
            "Epoch 2350/5000: Loss = 0.8629795637437875 | Training Accuracy = 0.95\n",
            "Epoch 2400/5000: Loss = 0.8519430061273301 | Training Accuracy = 0.95\n",
            "Epoch 2450/5000: Loss = 0.8411345551174337 | Training Accuracy = 0.95\n",
            "Epoch 2500/5000: Loss = 0.8305473848295225 | Training Accuracy = 0.95\n",
            "Epoch 2550/5000: Loss = 0.8201749510014373 | Training Accuracy = 0.95\n",
            "Epoch 2600/5000: Loss = 0.8100109762497739 | Training Accuracy = 0.95\n",
            "Epoch 2650/5000: Loss = 0.8000494362427755 | Training Accuracy = 0.95\n",
            "Epoch 2700/5000: Loss = 0.790284546724166 | Training Accuracy = 0.95\n",
            "Epoch 2750/5000: Loss = 0.780710751327524 | Training Accuracy = 0.95\n",
            "Epoch 2800/5000: Loss = 0.7713227101255695 | Training Accuracy = 1.0\n",
            "Epoch 2850/5000: Loss = 0.7621152888631139 | Training Accuracy = 1.0\n",
            "Epoch 2900/5000: Loss = 0.7530835488264574 | Training Accuracy = 1.0\n",
            "Epoch 2950/5000: Loss = 0.7442227373056988 | Training Accuracy = 1.0\n",
            "Epoch 3000/5000: Loss = 0.7355282786098275 | Training Accuracy = 1.0\n",
            "Epoch 3050/5000: Loss = 0.7269957655975602 | Training Accuracy = 1.0\n",
            "Epoch 3100/5000: Loss = 0.718620951689737 | Training Accuracy = 1.0\n",
            "Epoch 3150/5000: Loss = 0.7103997433317112 | Training Accuracy = 1.0\n",
            "Epoch 3200/5000: Loss = 0.7023281928765341 | Training Accuracy = 1.0\n",
            "Epoch 3250/5000: Loss = 0.694402491861954 | Training Accuracy = 1.0\n",
            "Epoch 3300/5000: Loss = 0.6866189646562215 | Training Accuracy = 1.0\n",
            "Epoch 3350/5000: Loss = 0.6789740624495529 | Training Accuracy = 1.0\n",
            "Epoch 3400/5000: Loss = 0.6714643575697716 | Training Accuracy = 1.0\n",
            "Epoch 3450/5000: Loss = 0.6640865381021855 | Training Accuracy = 1.0\n",
            "Epoch 3500/5000: Loss = 0.6568374027951762 | Training Accuracy = 1.0\n",
            "Epoch 3550/5000: Loss = 0.6497138562342712 | Training Accuracy = 1.0\n",
            "Epoch 3600/5000: Loss = 0.6427129042686541 | Training Accuracy = 1.0\n",
            "Epoch 3650/5000: Loss = 0.6358316496751579 | Training Accuracy = 1.0\n",
            "Epoch 3700/5000: Loss = 0.6290672880458087 | Training Accuracy = 1.0\n",
            "Epoch 3750/5000: Loss = 0.6224171038858871 | Training Accuracy = 1.0\n",
            "Epoch 3800/5000: Loss = 0.6158784669103516 | Training Accuracy = 1.0\n",
            "Epoch 3850/5000: Loss = 0.6094488285272428 | Training Accuracy = 1.0\n",
            "Epoch 3900/5000: Loss = 0.6031257184974194 | Training Accuracy = 1.0\n",
            "Epoch 3950/5000: Loss = 0.5969067417606481 | Training Accuracy = 1.0\n",
            "Epoch 4000/5000: Loss = 0.5907895754187047 | Training Accuracy = 1.0\n",
            "Epoch 4050/5000: Loss = 0.5847719658667081 | Training Accuracy = 1.0\n",
            "Epoch 4100/5000: Loss = 0.5788517260644575 | Training Accuracy = 1.0\n",
            "Epoch 4150/5000: Loss = 0.5730267329400418 | Training Accuracy = 1.0\n",
            "Epoch 4200/5000: Loss = 0.5672949249184563 | Training Accuracy = 1.0\n",
            "Epoch 4250/5000: Loss = 0.5616542995683957 | Training Accuracy = 1.0\n",
            "Epoch 4300/5000: Loss = 0.5561029113607994 | Training Accuracy = 1.0\n",
            "Epoch 4350/5000: Loss = 0.5506388695331027 | Training Accuracy = 1.0\n",
            "Epoch 4400/5000: Loss = 0.5452603360535051 | Training Accuracy = 1.0\n",
            "Epoch 4450/5000: Loss = 0.5399655236798948 | Training Accuracy = 1.0\n",
            "Epoch 4500/5000: Loss = 0.5347526941083817 | Training Accuracy = 1.0\n",
            "Epoch 4550/5000: Loss = 0.5296201562066767 | Training Accuracy = 1.0\n",
            "Epoch 4600/5000: Loss = 0.5245662643278454 | Training Accuracy = 1.0\n",
            "Epoch 4650/5000: Loss = 0.5195894167001904 | Training Accuracy = 1.0\n",
            "Epoch 4700/5000: Loss = 0.514688053889292 | Training Accuracy = 1.0\n",
            "Epoch 4750/5000: Loss = 0.5098606573284322 | Training Accuracy = 1.0\n",
            "Epoch 4800/5000: Loss = 0.5051057479138623 | Training Accuracy = 1.0\n",
            "Epoch 4850/5000: Loss = 0.5004218846615616 | Training Accuracy = 1.0\n",
            "Epoch 4900/5000: Loss = 0.49580766342232385 | Training Accuracy = 1.0\n",
            "Epoch 4950/5000: Loss = 0.49126171565219223 | Training Accuracy = 1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.13677777777777778"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BleQhuF6ByTQ"
      },
      "source": [
        "#### Loading the original dataset again"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C1iZh7go--Tx",
        "outputId": "e1f9520e-ec0b-4ce1-9ada-145d01fb68c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "h5_SVH = h5py.File('SVHN_single_grey1.h5', 'r')\n",
        "# Load the training, validation and test sets\n",
        "X_train = h5_SVH['X_train'][:]\n",
        "y_train_o = h5_SVH['y_train'][:]\n",
        "X_val = h5_SVH['X_val'][:]\n",
        "y_val_o = h5_SVH['y_val'][:]\n",
        "X_test = h5_SVH['X_test'][:]\n",
        "y_test_o = h5_SVH['y_test'][:]\n",
        "\n",
        "print('Reshaping X data: (n, 32, 32) => (n, 1024)'); print('--'*40)\n",
        "X_train = X_train.reshape((X_train.shape[0], -1))\n",
        "X_val = X_val.reshape((X_val.shape[0], -1))\n",
        "X_test = X_test.reshape((X_test.shape[0], -1))\n",
        "\n",
        "print('Making sure that the values are float so that we can get decimal points after division'); print('--'*40)\n",
        "X_train = X_train.astype('float32')\n",
        "X_val = X_val.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "\n",
        "print('Normalizing the RGB codes by dividing it to the max RGB value'); print('--'*40)\n",
        "X_train /= 255\n",
        "X_val /= 255\n",
        "X_test /= 255\n",
        "\n",
        "print('Converting y data into categorical (one-hot encoding)'); print('--'*40)\n",
        "y_train = to_categorical(y_train_o)\n",
        "y_val = to_categorical(y_val_o)\n",
        "y_test = to_categorical(y_test_o)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reshaping X data: (n, 32, 32) => (n, 1024)\n",
            "--------------------------------------------------------------------------------\n",
            "Making sure that the values are float so that we can get decimal points after division\n",
            "--------------------------------------------------------------------------------\n",
            "Normalizing the RGB codes by dividing it to the max RGB value\n",
            "--------------------------------------------------------------------------------\n",
            "Converting y data into categorical (one-hot encoding)\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8qZ1E-lE_vD9"
      },
      "source": [
        "#### Start with small regularization and find learning rate that makes the loss go down.\n",
        "* we start with Lambda(small regularization) = 1e-7\n",
        "* we start with a small learning rate = 1e-7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V7q3hsLT_oDn",
        "outputId": "6c11d7a0-cb80-4336-d811-6d1e825deb7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "lr = 1e-7\n",
        "Lambda = 1e-7\n",
        "train_and_test_loop(500, lr, Lambda)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/500: Loss = 2.313636055830537 | Training Accuracy = 0.10047619047619048\n",
            "Epoch 50/500: Loss = 2.3098854192808265 | Training Accuracy = 0.10071428571428571\n",
            "Epoch 100/500: Loss = 2.307415493568172 | Training Accuracy = 0.10316666666666667\n",
            "Epoch 150/500: Loss = 2.3057910045718257 | Training Accuracy = 0.10704761904761904\n",
            "Epoch 200/500: Loss = 2.3047235654049802 | Training Accuracy = 0.10909523809523809\n",
            "Epoch 250/500: Loss = 2.304022015886695 | Training Accuracy = 0.1105\n",
            "Epoch 300/500: Loss = 2.3035599597681045 | Training Accuracy = 0.11019047619047619\n",
            "Epoch 350/500: Loss = 2.3032541306688623 | Training Accuracy = 0.10907142857142857\n",
            "Epoch 400/500: Loss = 2.3030498914481656 | Training Accuracy = 0.10919047619047619\n",
            "Epoch 450/500: Loss = 2.302911526056488 | Training Accuracy = 0.10861904761904762\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.10733333333333334"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OtPHH89wB8YE"
      },
      "source": [
        "#### Lets try to train now with a value of learning rate 0.001"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7hvRY3b__90J",
        "outputId": "9a362110-d04b-4df4-cf45-73cc746d2c07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        }
      },
      "source": [
        "lr = 0.001\n",
        "Lambda = 1e-7\n",
        "train_and_test_loop(500, lr, Lambda)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/500: Loss = 2.304716830397155 | Training Accuracy = 0.1029047619047619\n",
            "Epoch 50/500: Loss = 2.2593131034088025 | Training Accuracy = 0.20676190476190476\n",
            "Epoch 100/500: Loss = 2.2508055885871188 | Training Accuracy = 0.2173095238095238\n",
            "Epoch 150/500: Loss = 2.2464950595129447 | Training Accuracy = 0.22304761904761905\n",
            "Epoch 200/500: Loss = 2.243615003186904 | Training Accuracy = 0.22583333333333333\n",
            "Epoch 250/500: Loss = 2.24145019171443 | Training Accuracy = 0.2287857142857143\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-a45767c81522>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mLambda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_and_test_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-21-85c43d915170>\u001b[0m in \u001b[0;36mtrain_and_test_loop\u001b[0;34m(iterations, lr, Lambda, verb)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_nodes1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_o\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m                      \u001b[0mX_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_test_o\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLambda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mverb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-fb96e988df47>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, X_train, y_train, minibatch_size, epoch, learning_rate, mu, X_val, y_val, Lambda, verb)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mX_mini_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_mini_val\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mminibatches_val\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_mini\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_mini\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mval_loss_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-0d95edd3a992>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mnextgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnextgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-0d95edd3a992>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, nextgrad)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_grad_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mnextgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnextgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-f1b39b838df0>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, nextgrad)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnextgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnextgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradInput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnextgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradB\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gU8B62CzAdTA"
      },
      "source": [
        "#### Hyperparameter Optimization\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qp_2H1IwCUIY"
      },
      "source": [
        "#### Running a finer search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GZN36etSCUsA",
        "outputId": "4c98da91-ef64-43bc-a088-59bca2e7108a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "import math\n",
        "for k in range(1, 10):\n",
        "    lr = math.pow(10, np.random.uniform(-3.0, -2.0))\n",
        "    Lambda = math.pow(10, np.random.uniform(-5, 2))\n",
        "    best_acc = train_and_test_loop(100, lr, Lambda, False)\n",
        "    print(\"Try {0}/{1}: Best_val_acc: {2}, lr: {3}, Lambda: {4}\\n\".format(k, 10, best_acc, lr, Lambda))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Try 1/10: Best_val_acc: 0.20077777777777778, lr: 0.0015152323209262192, Lambda: 2.420088513058786\n",
            "\n",
            "Try 2/10: Best_val_acc: 0.1995, lr: 0.0030309330192900014, Lambda: 0.005383061454363937\n",
            "\n",
            "Try 3/10: Best_val_acc: 0.2065, lr: 0.0012797477597798065, Lambda: 10.3755728511842\n",
            "\n",
            "Try 4/10: Best_val_acc: 0.20005555555555554, lr: 0.004519064979439422, Lambda: 33.89483252747831\n",
            "\n",
            "Try 5/10: Best_val_acc: 0.2011111111111111, lr: 0.005815733096986648, Lambda: 4.509999539600583e-05\n",
            "\n",
            "Try 6/10: Best_val_acc: 0.18733333333333332, lr: 0.005783968170324197, Lambda: 0.6858952147621221\n",
            "\n",
            "Try 7/10: Best_val_acc: 0.2021111111111111, lr: 0.0017842001687856277, Lambda: 0.00021786260784602122\n",
            "\n",
            "Try 8/10: Best_val_acc: 0.20822222222222223, lr: 0.0032010583399681015, Lambda: 0.0001013405412068582\n",
            "\n",
            "Try 9/10: Best_val_acc: 0.20105555555555554, lr: 0.0021921377401540316, Lambda: 0.04994502659125139\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lzm9IwhkvrIm"
      },
      "source": [
        "##### Observation 2 - Baby sitting the neural network for SVHN\n",
        "* Best accuracy achieved using this method after hyperparameter optimization: 21%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uwYNz4zM-gob"
      },
      "source": [
        "<a id='BasicNN'></a>\n",
        "### Modelling - Neural Network API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vrFnLj4hP787"
      },
      "source": [
        "#### NN model, sigmoid activations, SGD optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "clFSpvEc-fkg",
        "outputId": "2f971997-e012-4b33-9cb2-a0c73276c15e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "print('NN model with sigmoid activations'); print('--'*40)\n",
        "# Initialize the neural network classifier\n",
        "model1 = Sequential()\n",
        "\n",
        "# Input Layer - adding input layer and activation functions sigmoid\n",
        "model1.add(Dense(128, input_shape = (1024, )))\n",
        "# Adding activation function\n",
        "model1.add(Activation('sigmoid'))\n",
        "\n",
        "#Hidden Layer 1 - adding first hidden layer\n",
        "model1.add(Dense(64))\n",
        "# Adding activation function\n",
        "model1.add(Activation('sigmoid'))\n",
        "\n",
        "# Output Layer - adding output layer which is of 10 nodes (digits)\n",
        "model1.add(Dense(10))\n",
        "# Adding activation function - softmax for multiclass classification\n",
        "model1.add(Activation('softmax'))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NN model with sigmoid activations\n",
            "--------------------------------------------------------------------------------\n",
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vYDJ37MPlLiL",
        "outputId": "88e810be-a131-4d07-c01d-895e7a5c4f10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "model1.summary()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 128)               131200    \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                650       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 140,106\n",
            "Trainable params: 140,106\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RkURmgxTQYhx",
        "outputId": "b4ed62bc-bf90-4282-adcb-c6235991c16d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# compiling the neural network classifier, sgd optimizer\n",
        "sgd = optimizers.SGD(lr = 0.01)\n",
        "model1.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# Fitting the neural network for training\n",
        "history = model1.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 42000 samples, validate on 60000 samples\n",
            "Epoch 1/100\n",
            "42000/42000 [==============================] - 3s 74us/sample - loss: 2.3176 - acc: 0.1024 - val_loss: 2.3029 - val_acc: 0.1036\n",
            "Epoch 2/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.3032 - acc: 0.0999 - val_loss: 2.3028 - val_acc: 0.1026\n",
            "Epoch 3/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.3030 - acc: 0.1005 - val_loss: 2.3031 - val_acc: 0.1010\n",
            "Epoch 4/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.3030 - acc: 0.1010 - val_loss: 2.3027 - val_acc: 0.1002\n",
            "Epoch 5/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.3029 - acc: 0.1040 - val_loss: 2.3029 - val_acc: 0.0990\n",
            "Epoch 6/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.3028 - acc: 0.1000 - val_loss: 2.3027 - val_acc: 0.1041\n",
            "Epoch 7/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.3028 - acc: 0.1032 - val_loss: 2.3027 - val_acc: 0.1001\n",
            "Epoch 8/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.3028 - acc: 0.1001 - val_loss: 2.3025 - val_acc: 0.1014\n",
            "Epoch 9/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.3027 - acc: 0.0976 - val_loss: 2.3025 - val_acc: 0.1062\n",
            "Epoch 10/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.3026 - acc: 0.1011 - val_loss: 2.3026 - val_acc: 0.1013\n",
            "Epoch 11/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.3027 - acc: 0.1019 - val_loss: 2.3024 - val_acc: 0.1047\n",
            "Epoch 12/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.3025 - acc: 0.1027 - val_loss: 2.3026 - val_acc: 0.1011\n",
            "Epoch 13/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.3025 - acc: 0.1023 - val_loss: 2.3025 - val_acc: 0.1013\n",
            "Epoch 14/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.3025 - acc: 0.1020 - val_loss: 2.3022 - val_acc: 0.1032\n",
            "Epoch 15/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.3025 - acc: 0.1025 - val_loss: 2.3022 - val_acc: 0.1083\n",
            "Epoch 16/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.3024 - acc: 0.1020 - val_loss: 2.3022 - val_acc: 0.0999\n",
            "Epoch 17/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.3024 - acc: 0.1045 - val_loss: 2.3022 - val_acc: 0.1015\n",
            "Epoch 18/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.3024 - acc: 0.1020 - val_loss: 2.3022 - val_acc: 0.0994\n",
            "Epoch 19/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.3022 - acc: 0.1037 - val_loss: 2.3022 - val_acc: 0.1112\n",
            "Epoch 20/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.3023 - acc: 0.1036 - val_loss: 2.3021 - val_acc: 0.1015\n",
            "Epoch 21/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 2.3022 - acc: 0.1053 - val_loss: 2.3021 - val_acc: 0.1076\n",
            "Epoch 22/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.3022 - acc: 0.1047 - val_loss: 2.3020 - val_acc: 0.1045\n",
            "Epoch 23/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 2.3021 - acc: 0.1035 - val_loss: 2.3020 - val_acc: 0.1029\n",
            "Epoch 24/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 2.3020 - acc: 0.1043 - val_loss: 2.3022 - val_acc: 0.1022\n",
            "Epoch 25/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 2.3021 - acc: 0.1028 - val_loss: 2.3019 - val_acc: 0.1067\n",
            "Epoch 26/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 2.3020 - acc: 0.1027 - val_loss: 2.3019 - val_acc: 0.1049\n",
            "Epoch 27/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 2.3019 - acc: 0.1054 - val_loss: 2.3021 - val_acc: 0.1054\n",
            "Epoch 28/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 2.3020 - acc: 0.1016 - val_loss: 2.3019 - val_acc: 0.1077\n",
            "Epoch 29/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 2.3018 - acc: 0.1052 - val_loss: 2.3019 - val_acc: 0.1068\n",
            "Epoch 30/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 2.3018 - acc: 0.1053 - val_loss: 2.3020 - val_acc: 0.1021\n",
            "Epoch 31/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.3018 - acc: 0.1060 - val_loss: 2.3017 - val_acc: 0.1055\n",
            "Epoch 32/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.3018 - acc: 0.1050 - val_loss: 2.3017 - val_acc: 0.1084\n",
            "Epoch 33/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.3018 - acc: 0.1068 - val_loss: 2.3017 - val_acc: 0.1017\n",
            "Epoch 34/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.3017 - acc: 0.1057 - val_loss: 2.3017 - val_acc: 0.1032\n",
            "Epoch 35/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.3017 - acc: 0.1043 - val_loss: 2.3015 - val_acc: 0.1086\n",
            "Epoch 36/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.3017 - acc: 0.1067 - val_loss: 2.3015 - val_acc: 0.1010\n",
            "Epoch 37/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.3016 - acc: 0.1053 - val_loss: 2.3014 - val_acc: 0.1116\n",
            "Epoch 38/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.3016 - acc: 0.1079 - val_loss: 2.3015 - val_acc: 0.1079\n",
            "Epoch 39/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.3015 - acc: 0.1061 - val_loss: 2.3016 - val_acc: 0.1152\n",
            "Epoch 40/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.3015 - acc: 0.1083 - val_loss: 2.3015 - val_acc: 0.1087\n",
            "Epoch 41/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.3014 - acc: 0.1083 - val_loss: 2.3015 - val_acc: 0.1034\n",
            "Epoch 42/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.3014 - acc: 0.1065 - val_loss: 2.3013 - val_acc: 0.1068\n",
            "Epoch 43/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.3014 - acc: 0.1080 - val_loss: 2.3013 - val_acc: 0.1089\n",
            "Epoch 44/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.3013 - acc: 0.1071 - val_loss: 2.3015 - val_acc: 0.1105\n",
            "Epoch 45/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.3014 - acc: 0.1075 - val_loss: 2.3012 - val_acc: 0.1106\n",
            "Epoch 46/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.3012 - acc: 0.1081 - val_loss: 2.3013 - val_acc: 0.1091\n",
            "Epoch 47/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.3013 - acc: 0.1089 - val_loss: 2.3011 - val_acc: 0.1087\n",
            "Epoch 48/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.3012 - acc: 0.1080 - val_loss: 2.3011 - val_acc: 0.1075\n",
            "Epoch 49/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.3012 - acc: 0.1102 - val_loss: 2.3011 - val_acc: 0.1095\n",
            "Epoch 50/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.3012 - acc: 0.1111 - val_loss: 2.3010 - val_acc: 0.1133\n",
            "Epoch 51/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.3011 - acc: 0.1081 - val_loss: 2.3010 - val_acc: 0.1106\n",
            "Epoch 52/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.3011 - acc: 0.1086 - val_loss: 2.3010 - val_acc: 0.1107\n",
            "Epoch 53/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.3011 - acc: 0.1096 - val_loss: 2.3010 - val_acc: 0.1063\n",
            "Epoch 54/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.3010 - acc: 0.1082 - val_loss: 2.3009 - val_acc: 0.1181\n",
            "Epoch 55/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.3010 - acc: 0.1140 - val_loss: 2.3009 - val_acc: 0.1074\n",
            "Epoch 56/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.3009 - acc: 0.1089 - val_loss: 2.3010 - val_acc: 0.1094\n",
            "Epoch 57/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 2.3009 - acc: 0.1101 - val_loss: 2.3007 - val_acc: 0.1168\n",
            "Epoch 58/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.3008 - acc: 0.1103 - val_loss: 2.3009 - val_acc: 0.1152\n",
            "Epoch 59/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.3008 - acc: 0.1135 - val_loss: 2.3009 - val_acc: 0.1112\n",
            "Epoch 60/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.3008 - acc: 0.1128 - val_loss: 2.3009 - val_acc: 0.1105\n",
            "Epoch 61/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.3008 - acc: 0.1115 - val_loss: 2.3007 - val_acc: 0.1134\n",
            "Epoch 62/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.3008 - acc: 0.1139 - val_loss: 2.3006 - val_acc: 0.1192\n",
            "Epoch 63/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.3007 - acc: 0.1128 - val_loss: 2.3006 - val_acc: 0.1122\n",
            "Epoch 64/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.3006 - acc: 0.1112 - val_loss: 2.3006 - val_acc: 0.1118\n",
            "Epoch 65/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.3006 - acc: 0.1132 - val_loss: 2.3005 - val_acc: 0.1101\n",
            "Epoch 66/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.3006 - acc: 0.1130 - val_loss: 2.3006 - val_acc: 0.1058\n",
            "Epoch 67/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.3006 - acc: 0.1127 - val_loss: 2.3005 - val_acc: 0.1133\n",
            "Epoch 68/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.3005 - acc: 0.1150 - val_loss: 2.3005 - val_acc: 0.1108\n",
            "Epoch 69/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.3005 - acc: 0.1166 - val_loss: 2.3004 - val_acc: 0.1151\n",
            "Epoch 70/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.3005 - acc: 0.1149 - val_loss: 2.3004 - val_acc: 0.1282\n",
            "Epoch 71/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.3004 - acc: 0.1162 - val_loss: 2.3004 - val_acc: 0.1181\n",
            "Epoch 72/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.3004 - acc: 0.1133 - val_loss: 2.3004 - val_acc: 0.1127\n",
            "Epoch 73/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.3004 - acc: 0.1166 - val_loss: 2.3002 - val_acc: 0.1094\n",
            "Epoch 74/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.3003 - acc: 0.1144 - val_loss: 2.3003 - val_acc: 0.1141\n",
            "Epoch 75/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.3002 - acc: 0.1166 - val_loss: 2.3003 - val_acc: 0.1157\n",
            "Epoch 76/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.3002 - acc: 0.1192 - val_loss: 2.3002 - val_acc: 0.1116\n",
            "Epoch 77/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.3002 - acc: 0.1148 - val_loss: 2.3001 - val_acc: 0.1114\n",
            "Epoch 78/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.3001 - acc: 0.1135 - val_loss: 2.3000 - val_acc: 0.1245\n",
            "Epoch 79/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.3001 - acc: 0.1171 - val_loss: 2.3001 - val_acc: 0.1203\n",
            "Epoch 80/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.3001 - acc: 0.1142 - val_loss: 2.3000 - val_acc: 0.1208\n",
            "Epoch 81/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.3000 - acc: 0.1180 - val_loss: 2.3001 - val_acc: 0.1024\n",
            "Epoch 82/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.3001 - acc: 0.1184 - val_loss: 2.2999 - val_acc: 0.1222\n",
            "Epoch 83/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.3000 - acc: 0.1160 - val_loss: 2.2999 - val_acc: 0.1213\n",
            "Epoch 84/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2999 - acc: 0.1205 - val_loss: 2.2999 - val_acc: 0.1088\n",
            "Epoch 85/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2998 - acc: 0.1143 - val_loss: 2.2999 - val_acc: 0.1267\n",
            "Epoch 86/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.2999 - acc: 0.1193 - val_loss: 2.2998 - val_acc: 0.1144\n",
            "Epoch 87/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.2998 - acc: 0.1192 - val_loss: 2.2997 - val_acc: 0.1170\n",
            "Epoch 88/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2997 - acc: 0.1179 - val_loss: 2.2997 - val_acc: 0.1139\n",
            "Epoch 89/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.2998 - acc: 0.1161 - val_loss: 2.2997 - val_acc: 0.1082\n",
            "Epoch 90/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2997 - acc: 0.1172 - val_loss: 2.2998 - val_acc: 0.1248\n",
            "Epoch 91/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2996 - acc: 0.1184 - val_loss: 2.2998 - val_acc: 0.1212\n",
            "Epoch 92/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2997 - acc: 0.1202 - val_loss: 2.2996 - val_acc: 0.1311\n",
            "Epoch 93/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.2997 - acc: 0.1207 - val_loss: 2.2995 - val_acc: 0.1238\n",
            "Epoch 94/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.2996 - acc: 0.1212 - val_loss: 2.2995 - val_acc: 0.1262\n",
            "Epoch 95/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.2995 - acc: 0.1211 - val_loss: 2.2995 - val_acc: 0.1112\n",
            "Epoch 96/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.2995 - acc: 0.1195 - val_loss: 2.2995 - val_acc: 0.1203\n",
            "Epoch 97/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2994 - acc: 0.1223 - val_loss: 2.2995 - val_acc: 0.1279\n",
            "Epoch 98/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2995 - acc: 0.1203 - val_loss: 2.2994 - val_acc: 0.1268\n",
            "Epoch 99/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.2993 - acc: 0.1212 - val_loss: 2.2995 - val_acc: 0.1134\n",
            "Epoch 100/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2993 - acc: 0.1193 - val_loss: 2.2994 - val_acc: 0.1175\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "airVUr8-FQJ8",
        "outputId": "678f156b-3918-4267-f77c-2651240f46a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "print('Evaluate NN model with sigmoid activations'); print('--'*40)\n",
        "results1 = model1.evaluate(X_val, y_val)\n",
        "print('Validation accuracy: {}'.format(round(results1[1]*100, 2), '%'))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluate NN model with sigmoid activations\n",
            "--------------------------------------------------------------------------------\n",
            "60000/60000 [==============================] - 4s 64us/sample - loss: 2.2994 - acc: 0.1175\n",
            "Validation accuracy: 11.75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "o2rIo2V1Q10H"
      },
      "source": [
        "#### NN model, sigmoid activations, SGD optimizer, changing learning rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YTbX96-vGjFj",
        "outputId": "9923166d-14d1-437b-9c1a-cf8141e239ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print('NN model with sigmoid activations - changing learning rate'); print('--'*40)\n",
        "# compiling the neural network classifier, sgd optimizer\n",
        "sgd = optimizers.SGD(lr = 0.001)\n",
        "model1.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# Fitting the neural network for training\n",
        "history = model1.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NN model with sigmoid activations - changing learning rate\n",
            "--------------------------------------------------------------------------------\n",
            "Train on 42000 samples, validate on 60000 samples\n",
            "Epoch 1/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 2.2991 - acc: 0.1180 - val_loss: 2.2992 - val_acc: 0.1237\n",
            "Epoch 2/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.2990 - acc: 0.1224 - val_loss: 2.2992 - val_acc: 0.1276\n",
            "Epoch 3/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2990 - acc: 0.1294 - val_loss: 2.2992 - val_acc: 0.1280\n",
            "Epoch 4/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2990 - acc: 0.1285 - val_loss: 2.2991 - val_acc: 0.1281\n",
            "Epoch 5/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.2990 - acc: 0.1302 - val_loss: 2.2991 - val_acc: 0.1282\n",
            "Epoch 6/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2990 - acc: 0.1299 - val_loss: 2.2991 - val_acc: 0.1282\n",
            "Epoch 7/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.2990 - acc: 0.1310 - val_loss: 2.2991 - val_acc: 0.1279\n",
            "Epoch 8/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2990 - acc: 0.1290 - val_loss: 2.2991 - val_acc: 0.1275\n",
            "Epoch 9/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2990 - acc: 0.1293 - val_loss: 2.2991 - val_acc: 0.1275\n",
            "Epoch 10/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2990 - acc: 0.1302 - val_loss: 2.2991 - val_acc: 0.1281\n",
            "Epoch 11/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.2990 - acc: 0.1284 - val_loss: 2.2991 - val_acc: 0.1284\n",
            "Epoch 12/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.2990 - acc: 0.1297 - val_loss: 2.2991 - val_acc: 0.1285\n",
            "Epoch 13/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.2989 - acc: 0.1285 - val_loss: 2.2991 - val_acc: 0.1288\n",
            "Epoch 14/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2989 - acc: 0.1285 - val_loss: 2.2991 - val_acc: 0.1289\n",
            "Epoch 15/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2989 - acc: 0.1299 - val_loss: 2.2991 - val_acc: 0.1287\n",
            "Epoch 16/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2989 - acc: 0.1294 - val_loss: 2.2991 - val_acc: 0.1287\n",
            "Epoch 17/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.2989 - acc: 0.1300 - val_loss: 2.2991 - val_acc: 0.1282\n",
            "Epoch 18/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.2989 - acc: 0.1295 - val_loss: 2.2991 - val_acc: 0.1289\n",
            "Epoch 19/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2989 - acc: 0.1309 - val_loss: 2.2991 - val_acc: 0.1290\n",
            "Epoch 20/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2989 - acc: 0.1295 - val_loss: 2.2991 - val_acc: 0.1286\n",
            "Epoch 21/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.2989 - acc: 0.1308 - val_loss: 2.2991 - val_acc: 0.1284\n",
            "Epoch 22/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.2989 - acc: 0.1274 - val_loss: 2.2991 - val_acc: 0.1295\n",
            "Epoch 23/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.2989 - acc: 0.1294 - val_loss: 2.2991 - val_acc: 0.1294\n",
            "Epoch 24/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2989 - acc: 0.1292 - val_loss: 2.2991 - val_acc: 0.1297\n",
            "Epoch 25/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2989 - acc: 0.1310 - val_loss: 2.2991 - val_acc: 0.1289\n",
            "Epoch 26/100\n",
            "42000/42000 [==============================] - 1s 32us/sample - loss: 2.2989 - acc: 0.1312 - val_loss: 2.2991 - val_acc: 0.1288\n",
            "Epoch 27/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2989 - acc: 0.1295 - val_loss: 2.2991 - val_acc: 0.1285\n",
            "Epoch 28/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2989 - acc: 0.1273 - val_loss: 2.2990 - val_acc: 0.1294\n",
            "Epoch 29/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.2989 - acc: 0.1304 - val_loss: 2.2990 - val_acc: 0.1287\n",
            "Epoch 30/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2989 - acc: 0.1296 - val_loss: 2.2990 - val_acc: 0.1291\n",
            "Epoch 31/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2989 - acc: 0.1312 - val_loss: 2.2990 - val_acc: 0.1287\n",
            "Epoch 32/100\n",
            "42000/42000 [==============================] - 1s 32us/sample - loss: 2.2989 - acc: 0.1301 - val_loss: 2.2990 - val_acc: 0.1286\n",
            "Epoch 33/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2989 - acc: 0.1301 - val_loss: 2.2990 - val_acc: 0.1284\n",
            "Epoch 34/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2989 - acc: 0.1300 - val_loss: 2.2990 - val_acc: 0.1285\n",
            "Epoch 35/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2989 - acc: 0.1290 - val_loss: 2.2990 - val_acc: 0.1287\n",
            "Epoch 36/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 2.2989 - acc: 0.1280 - val_loss: 2.2990 - val_acc: 0.1296\n",
            "Epoch 37/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.2988 - acc: 0.1305 - val_loss: 2.2990 - val_acc: 0.1287\n",
            "Epoch 38/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2988 - acc: 0.1307 - val_loss: 2.2990 - val_acc: 0.1281\n",
            "Epoch 39/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.2988 - acc: 0.1305 - val_loss: 2.2990 - val_acc: 0.1284\n",
            "Epoch 40/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2988 - acc: 0.1300 - val_loss: 2.2990 - val_acc: 0.1288\n",
            "Epoch 41/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.2988 - acc: 0.1289 - val_loss: 2.2990 - val_acc: 0.1291\n",
            "Epoch 42/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2988 - acc: 0.1300 - val_loss: 2.2990 - val_acc: 0.1288\n",
            "Epoch 43/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2988 - acc: 0.1293 - val_loss: 2.2990 - val_acc: 0.1301\n",
            "Epoch 44/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.2988 - acc: 0.1295 - val_loss: 2.2990 - val_acc: 0.1299\n",
            "Epoch 45/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.2988 - acc: 0.1317 - val_loss: 2.2990 - val_acc: 0.1299\n",
            "Epoch 46/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2988 - acc: 0.1300 - val_loss: 2.2990 - val_acc: 0.1294\n",
            "Epoch 47/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2988 - acc: 0.1312 - val_loss: 2.2990 - val_acc: 0.1291\n",
            "Epoch 48/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.2988 - acc: 0.1295 - val_loss: 2.2990 - val_acc: 0.1293\n",
            "Epoch 49/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2988 - acc: 0.1299 - val_loss: 2.2990 - val_acc: 0.1297\n",
            "Epoch 50/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2988 - acc: 0.1304 - val_loss: 2.2990 - val_acc: 0.1298\n",
            "Epoch 51/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2988 - acc: 0.1325 - val_loss: 2.2990 - val_acc: 0.1300\n",
            "Epoch 52/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.2988 - acc: 0.1306 - val_loss: 2.2990 - val_acc: 0.1297\n",
            "Epoch 53/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2988 - acc: 0.1309 - val_loss: 2.2990 - val_acc: 0.1294\n",
            "Epoch 54/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.2988 - acc: 0.1291 - val_loss: 2.2990 - val_acc: 0.1296\n",
            "Epoch 55/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2988 - acc: 0.1309 - val_loss: 2.2989 - val_acc: 0.1294\n",
            "Epoch 56/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2988 - acc: 0.1313 - val_loss: 2.2989 - val_acc: 0.1296\n",
            "Epoch 57/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.2988 - acc: 0.1312 - val_loss: 2.2989 - val_acc: 0.1291\n",
            "Epoch 58/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.2988 - acc: 0.1316 - val_loss: 2.2989 - val_acc: 0.1290\n",
            "Epoch 59/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.2988 - acc: 0.1294 - val_loss: 2.2989 - val_acc: 0.1293\n",
            "Epoch 60/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.2988 - acc: 0.1308 - val_loss: 2.2989 - val_acc: 0.1294\n",
            "Epoch 61/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2987 - acc: 0.1309 - val_loss: 2.2989 - val_acc: 0.1296\n",
            "Epoch 62/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2987 - acc: 0.1308 - val_loss: 2.2989 - val_acc: 0.1296\n",
            "Epoch 63/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.2987 - acc: 0.1321 - val_loss: 2.2989 - val_acc: 0.1288\n",
            "Epoch 64/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.2987 - acc: 0.1322 - val_loss: 2.2989 - val_acc: 0.1285\n",
            "Epoch 65/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.2987 - acc: 0.1297 - val_loss: 2.2989 - val_acc: 0.1290\n",
            "Epoch 66/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2987 - acc: 0.1298 - val_loss: 2.2989 - val_acc: 0.1291\n",
            "Epoch 67/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.2987 - acc: 0.1302 - val_loss: 2.2989 - val_acc: 0.1294\n",
            "Epoch 68/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2987 - acc: 0.1317 - val_loss: 2.2989 - val_acc: 0.1293\n",
            "Epoch 69/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2987 - acc: 0.1313 - val_loss: 2.2989 - val_acc: 0.1296\n",
            "Epoch 70/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2987 - acc: 0.1312 - val_loss: 2.2989 - val_acc: 0.1291\n",
            "Epoch 71/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2987 - acc: 0.1302 - val_loss: 2.2989 - val_acc: 0.1295\n",
            "Epoch 72/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2987 - acc: 0.1291 - val_loss: 2.2989 - val_acc: 0.1301\n",
            "Epoch 73/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2987 - acc: 0.1309 - val_loss: 2.2989 - val_acc: 0.1305\n",
            "Epoch 74/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.2987 - acc: 0.1324 - val_loss: 2.2989 - val_acc: 0.1304\n",
            "Epoch 75/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.2987 - acc: 0.1310 - val_loss: 2.2989 - val_acc: 0.1302\n",
            "Epoch 76/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2987 - acc: 0.1310 - val_loss: 2.2989 - val_acc: 0.1297\n",
            "Epoch 77/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2987 - acc: 0.1318 - val_loss: 2.2989 - val_acc: 0.1298\n",
            "Epoch 78/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.2987 - acc: 0.1336 - val_loss: 2.2989 - val_acc: 0.1292\n",
            "Epoch 79/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2987 - acc: 0.1303 - val_loss: 2.2989 - val_acc: 0.1290\n",
            "Epoch 80/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2987 - acc: 0.1288 - val_loss: 2.2988 - val_acc: 0.1294\n",
            "Epoch 81/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.2987 - acc: 0.1314 - val_loss: 2.2988 - val_acc: 0.1295\n",
            "Epoch 82/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.2987 - acc: 0.1310 - val_loss: 2.2988 - val_acc: 0.1298\n",
            "Epoch 83/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2987 - acc: 0.1284 - val_loss: 2.2988 - val_acc: 0.1305\n",
            "Epoch 84/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2987 - acc: 0.1320 - val_loss: 2.2988 - val_acc: 0.1293\n",
            "Epoch 85/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2986 - acc: 0.1314 - val_loss: 2.2988 - val_acc: 0.1297\n",
            "Epoch 86/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.2986 - acc: 0.1301 - val_loss: 2.2988 - val_acc: 0.1304\n",
            "Epoch 87/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2986 - acc: 0.1291 - val_loss: 2.2988 - val_acc: 0.1311\n",
            "Epoch 88/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2986 - acc: 0.1318 - val_loss: 2.2988 - val_acc: 0.1310\n",
            "Epoch 89/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.2986 - acc: 0.1313 - val_loss: 2.2988 - val_acc: 0.1312\n",
            "Epoch 90/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.2986 - acc: 0.1323 - val_loss: 2.2988 - val_acc: 0.1308\n",
            "Epoch 91/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2986 - acc: 0.1293 - val_loss: 2.2988 - val_acc: 0.1316\n",
            "Epoch 92/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2986 - acc: 0.1313 - val_loss: 2.2988 - val_acc: 0.1320\n",
            "Epoch 93/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2986 - acc: 0.1317 - val_loss: 2.2988 - val_acc: 0.1317\n",
            "Epoch 94/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.2986 - acc: 0.1318 - val_loss: 2.2988 - val_acc: 0.1317\n",
            "Epoch 95/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2986 - acc: 0.1328 - val_loss: 2.2988 - val_acc: 0.1310\n",
            "Epoch 96/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2986 - acc: 0.1321 - val_loss: 2.2988 - val_acc: 0.1311\n",
            "Epoch 97/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.2986 - acc: 0.1310 - val_loss: 2.2988 - val_acc: 0.1307\n",
            "Epoch 98/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2986 - acc: 0.1326 - val_loss: 2.2988 - val_acc: 0.1305\n",
            "Epoch 99/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2986 - acc: 0.1340 - val_loss: 2.2988 - val_acc: 0.1294\n",
            "Epoch 100/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.2986 - acc: 0.1306 - val_loss: 2.2988 - val_acc: 0.1300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "scbm_YOeJkap",
        "outputId": "b7f3c049-aa51-4922-ae12-36122069fb9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "print('Evaluate NN model with sigmoid activations - changing learning rate'); print('--'*40)\n",
        "results1 = model1.evaluate(X_val, y_val)\n",
        "print('Validation accuracy: {}'.format(round(results1[1]*100, 2), '%'))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluate NN model with sigmoid activations - changing learning rate\n",
            "--------------------------------------------------------------------------------\n",
            "60000/60000 [==============================] - 4s 65us/sample - loss: 2.2988 - acc: 0.1300\n",
            "Validation accuracy: 13.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7SUgKHYEmt1X"
      },
      "source": [
        "<a id='o3'></a>\n",
        "##### Observation 3 - NN model with sigmoid activations\n",
        "* Validation score is very low, changing learning rate further reduces it.\n",
        "* Optimizing the network in order to better learn the patterns in the dataset.\n",
        "* Best model out of the above is the one with lower learning rate using SGD optimizer and sigmoid activations.\n",
        "* Next, let's use relu activations and see if the score improves."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u3qkfHovRFig"
      },
      "source": [
        "#### NN model, relu activations, SGD optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BRsNW7ss_iL9",
        "outputId": "c3212139-62e6-43df-9431-6ac98328b8c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "%time\n",
        "print('NN model with relu activations and sgd optimizers'); print('--'*40)\n",
        "# Initialize the neural network classifier\n",
        "model2 = Sequential()\n",
        "\n",
        "# Input Layer - adding input layer and activation functions relu\n",
        "model2.add(Dense(128, input_shape = (1024, )))\n",
        "# Adding activation function\n",
        "model2.add(Activation('relu'))\n",
        "\n",
        "#Hidden Layer 1 - adding first hidden layer\n",
        "model2.add(Dense(64))\n",
        "# Adding activation function\n",
        "model2.add(Activation('relu'))\n",
        "\n",
        "# Output Layer - adding output layer which is of 10 nodes (digits)\n",
        "model2.add(Dense(10))\n",
        "# Adding activation function - softmax for multiclass classification\n",
        "model2.add(Activation('softmax'))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
            "Wall time: 9.06 µs\n",
            "NN model with relu activations and sgd optimizers\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x4uIPsDZqi8n",
        "outputId": "1c2a69bf-5c32-466d-a918-490112731e6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "model2.summary()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 128)               131200    \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 10)                650       \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 140,106\n",
            "Trainable params: 140,106\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OJzn5I1ORL8q",
        "outputId": "e9a11055-648f-467b-ba1e-f4393188dd4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# compiling the neural network classifier, sgd optimizer\n",
        "sgd = optimizers.SGD(lr = 0.01)\n",
        "model2.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# Fitting the neural network for training\n",
        "history = model2.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 42000 samples, validate on 60000 samples\n",
            "Epoch 1/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 2.3004 - acc: 0.1163 - val_loss: 2.2902 - val_acc: 0.1356\n",
            "Epoch 2/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2848 - acc: 0.1479 - val_loss: 2.2766 - val_acc: 0.1672\n",
            "Epoch 3/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2697 - acc: 0.1807 - val_loss: 2.2616 - val_acc: 0.1969\n",
            "Epoch 4/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2521 - acc: 0.2163 - val_loss: 2.2425 - val_acc: 0.2213\n",
            "Epoch 5/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2312 - acc: 0.2530 - val_loss: 2.2190 - val_acc: 0.2751\n",
            "Epoch 6/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.2056 - acc: 0.2900 - val_loss: 2.1894 - val_acc: 0.3113\n",
            "Epoch 7/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.1736 - acc: 0.3232 - val_loss: 2.1542 - val_acc: 0.3513\n",
            "Epoch 8/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.1348 - acc: 0.3520 - val_loss: 2.1103 - val_acc: 0.3841\n",
            "Epoch 9/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.0882 - acc: 0.3811 - val_loss: 2.0600 - val_acc: 0.3993\n",
            "Epoch 10/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 2.0329 - acc: 0.4085 - val_loss: 2.0017 - val_acc: 0.4051\n",
            "Epoch 11/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.9704 - acc: 0.4298 - val_loss: 1.9342 - val_acc: 0.4510\n",
            "Epoch 12/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.9016 - acc: 0.4525 - val_loss: 1.8676 - val_acc: 0.4548\n",
            "Epoch 13/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 1.8304 - acc: 0.4731 - val_loss: 1.7982 - val_acc: 0.4873\n",
            "Epoch 14/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 1.7603 - acc: 0.4927 - val_loss: 1.7245 - val_acc: 0.5071\n",
            "Epoch 15/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 1.6937 - acc: 0.5133 - val_loss: 1.6638 - val_acc: 0.5286\n",
            "Epoch 16/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.6312 - acc: 0.5284 - val_loss: 1.5974 - val_acc: 0.5420\n",
            "Epoch 17/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.5737 - acc: 0.5434 - val_loss: 1.5450 - val_acc: 0.5552\n",
            "Epoch 18/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 1.5225 - acc: 0.5567 - val_loss: 1.5005 - val_acc: 0.5644\n",
            "Epoch 19/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.4768 - acc: 0.5687 - val_loss: 1.4511 - val_acc: 0.5780\n",
            "Epoch 20/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 1.4362 - acc: 0.5784 - val_loss: 1.4068 - val_acc: 0.5869\n",
            "Epoch 21/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 1.3979 - acc: 0.5902 - val_loss: 1.3811 - val_acc: 0.5893\n",
            "Epoch 22/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 1.3660 - acc: 0.5958 - val_loss: 1.3373 - val_acc: 0.6122\n",
            "Epoch 23/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.3368 - acc: 0.6029 - val_loss: 1.3135 - val_acc: 0.6130\n",
            "Epoch 24/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.3074 - acc: 0.6127 - val_loss: 1.3033 - val_acc: 0.6103\n",
            "Epoch 25/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 1.2836 - acc: 0.6185 - val_loss: 1.2918 - val_acc: 0.6068\n",
            "Epoch 26/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 1.2590 - acc: 0.6252 - val_loss: 1.2371 - val_acc: 0.6325\n",
            "Epoch 27/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 1.2366 - acc: 0.6317 - val_loss: 1.2179 - val_acc: 0.6401\n",
            "Epoch 28/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 1.2160 - acc: 0.6379 - val_loss: 1.1994 - val_acc: 0.6440\n",
            "Epoch 29/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 1.1988 - acc: 0.6433 - val_loss: 1.1953 - val_acc: 0.6419\n",
            "Epoch 30/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 1.1824 - acc: 0.6464 - val_loss: 1.1690 - val_acc: 0.6517\n",
            "Epoch 31/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.1652 - acc: 0.6521 - val_loss: 1.1497 - val_acc: 0.6600\n",
            "Epoch 32/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 1.1515 - acc: 0.6570 - val_loss: 1.1423 - val_acc: 0.6617\n",
            "Epoch 33/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.1373 - acc: 0.6613 - val_loss: 1.1358 - val_acc: 0.6570\n",
            "Epoch 34/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.1254 - acc: 0.6629 - val_loss: 1.1160 - val_acc: 0.6675\n",
            "Epoch 35/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.1122 - acc: 0.6677 - val_loss: 1.1036 - val_acc: 0.6741\n",
            "Epoch 36/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.1014 - acc: 0.6717 - val_loss: 1.0953 - val_acc: 0.6702\n",
            "Epoch 37/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.0884 - acc: 0.6755 - val_loss: 1.0820 - val_acc: 0.6776\n",
            "Epoch 38/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 1.0799 - acc: 0.6762 - val_loss: 1.0686 - val_acc: 0.6851\n",
            "Epoch 39/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 1.0680 - acc: 0.6800 - val_loss: 1.0608 - val_acc: 0.6844\n",
            "Epoch 40/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 1.0582 - acc: 0.6832 - val_loss: 1.0497 - val_acc: 0.6843\n",
            "Epoch 41/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.0481 - acc: 0.6875 - val_loss: 1.0354 - val_acc: 0.6911\n",
            "Epoch 42/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.0371 - acc: 0.6902 - val_loss: 1.0356 - val_acc: 0.6912\n",
            "Epoch 43/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 1.0294 - acc: 0.6920 - val_loss: 1.0290 - val_acc: 0.6940\n",
            "Epoch 44/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 1.0208 - acc: 0.6949 - val_loss: 1.0328 - val_acc: 0.6867\n",
            "Epoch 45/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.0147 - acc: 0.6965 - val_loss: 1.0000 - val_acc: 0.7024\n",
            "Epoch 46/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 1.0045 - acc: 0.7001 - val_loss: 1.0142 - val_acc: 0.6961\n",
            "Epoch 47/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.9933 - acc: 0.7042 - val_loss: 1.0023 - val_acc: 0.6992\n",
            "Epoch 48/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 0.9871 - acc: 0.7050 - val_loss: 0.9777 - val_acc: 0.7099\n",
            "Epoch 49/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 0.9777 - acc: 0.7075 - val_loss: 0.9910 - val_acc: 0.7020\n",
            "Epoch 50/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.9721 - acc: 0.7106 - val_loss: 0.9667 - val_acc: 0.7085\n",
            "Epoch 51/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 0.9643 - acc: 0.7111 - val_loss: 0.9758 - val_acc: 0.7081\n",
            "Epoch 52/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 0.9573 - acc: 0.7137 - val_loss: 0.9551 - val_acc: 0.7161\n",
            "Epoch 53/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.9493 - acc: 0.7174 - val_loss: 0.9400 - val_acc: 0.7212\n",
            "Epoch 54/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 0.9417 - acc: 0.7208 - val_loss: 0.9426 - val_acc: 0.7197\n",
            "Epoch 55/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 0.9345 - acc: 0.7203 - val_loss: 0.9365 - val_acc: 0.7203\n",
            "Epoch 56/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 0.9261 - acc: 0.7246 - val_loss: 0.9246 - val_acc: 0.7242\n",
            "Epoch 57/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 0.9226 - acc: 0.7250 - val_loss: 0.9220 - val_acc: 0.7254\n",
            "Epoch 58/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 0.9148 - acc: 0.7295 - val_loss: 0.9152 - val_acc: 0.7267\n",
            "Epoch 59/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 0.9106 - acc: 0.7298 - val_loss: 0.9253 - val_acc: 0.7215\n",
            "Epoch 60/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 0.9030 - acc: 0.7302 - val_loss: 0.8985 - val_acc: 0.7346\n",
            "Epoch 61/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.8946 - acc: 0.7330 - val_loss: 0.8931 - val_acc: 0.7358\n",
            "Epoch 62/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 0.8926 - acc: 0.7351 - val_loss: 0.8934 - val_acc: 0.7356\n",
            "Epoch 63/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.8810 - acc: 0.7383 - val_loss: 0.8808 - val_acc: 0.7390\n",
            "Epoch 64/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 0.8770 - acc: 0.7410 - val_loss: 0.8821 - val_acc: 0.7358\n",
            "Epoch 65/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.8739 - acc: 0.7389 - val_loss: 0.8744 - val_acc: 0.7398\n",
            "Epoch 66/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 0.8664 - acc: 0.7432 - val_loss: 0.8677 - val_acc: 0.7442\n",
            "Epoch 67/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 0.8625 - acc: 0.7436 - val_loss: 0.8555 - val_acc: 0.7485\n",
            "Epoch 68/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.8541 - acc: 0.7450 - val_loss: 0.8515 - val_acc: 0.7484\n",
            "Epoch 69/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.8506 - acc: 0.7466 - val_loss: 0.8520 - val_acc: 0.7492\n",
            "Epoch 70/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 0.8462 - acc: 0.7484 - val_loss: 0.8395 - val_acc: 0.7538\n",
            "Epoch 71/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 0.8419 - acc: 0.7494 - val_loss: 0.8413 - val_acc: 0.7510\n",
            "Epoch 72/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 0.8346 - acc: 0.7523 - val_loss: 0.8623 - val_acc: 0.7418\n",
            "Epoch 73/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 0.8295 - acc: 0.7530 - val_loss: 0.8418 - val_acc: 0.7505\n",
            "Epoch 74/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.8271 - acc: 0.7547 - val_loss: 0.8326 - val_acc: 0.7554\n",
            "Epoch 75/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.8228 - acc: 0.7561 - val_loss: 0.8161 - val_acc: 0.7595\n",
            "Epoch 76/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 0.8142 - acc: 0.7594 - val_loss: 0.8157 - val_acc: 0.7612\n",
            "Epoch 77/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.8156 - acc: 0.7579 - val_loss: 0.8241 - val_acc: 0.7542\n",
            "Epoch 78/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.8076 - acc: 0.7606 - val_loss: 0.8105 - val_acc: 0.7627\n",
            "Epoch 79/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 0.8029 - acc: 0.7622 - val_loss: 0.8196 - val_acc: 0.7564\n",
            "Epoch 80/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 0.8005 - acc: 0.7624 - val_loss: 0.7981 - val_acc: 0.7659\n",
            "Epoch 81/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.7943 - acc: 0.7656 - val_loss: 0.7990 - val_acc: 0.7641\n",
            "Epoch 82/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.7903 - acc: 0.7668 - val_loss: 0.7881 - val_acc: 0.7673\n",
            "Epoch 83/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.7880 - acc: 0.7666 - val_loss: 0.7828 - val_acc: 0.7691\n",
            "Epoch 84/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 0.7819 - acc: 0.7687 - val_loss: 0.7794 - val_acc: 0.7714\n",
            "Epoch 85/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.7773 - acc: 0.7698 - val_loss: 0.8435 - val_acc: 0.7475\n",
            "Epoch 86/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 0.7769 - acc: 0.7700 - val_loss: 0.7965 - val_acc: 0.7634\n",
            "Epoch 87/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.7721 - acc: 0.7712 - val_loss: 0.7923 - val_acc: 0.7655\n",
            "Epoch 88/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.7682 - acc: 0.7729 - val_loss: 0.7747 - val_acc: 0.7726\n",
            "Epoch 89/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 0.7674 - acc: 0.7736 - val_loss: 0.7590 - val_acc: 0.7779\n",
            "Epoch 90/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.7590 - acc: 0.7744 - val_loss: 0.7687 - val_acc: 0.7718\n",
            "Epoch 91/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.7574 - acc: 0.7764 - val_loss: 0.7600 - val_acc: 0.7765\n",
            "Epoch 92/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 0.7509 - acc: 0.7767 - val_loss: 0.8030 - val_acc: 0.7617\n",
            "Epoch 93/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.7510 - acc: 0.7787 - val_loss: 0.7500 - val_acc: 0.7796\n",
            "Epoch 94/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.7451 - acc: 0.7780 - val_loss: 0.7624 - val_acc: 0.7750\n",
            "Epoch 95/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.7427 - acc: 0.7808 - val_loss: 0.7653 - val_acc: 0.7746\n",
            "Epoch 96/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.7429 - acc: 0.7801 - val_loss: 0.7540 - val_acc: 0.7779\n",
            "Epoch 97/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 0.7404 - acc: 0.7798 - val_loss: 0.7501 - val_acc: 0.7775\n",
            "Epoch 98/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 0.7331 - acc: 0.7823 - val_loss: 0.7695 - val_acc: 0.7700\n",
            "Epoch 99/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.7288 - acc: 0.7831 - val_loss: 0.7490 - val_acc: 0.7800\n",
            "Epoch 100/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.7347 - acc: 0.7832 - val_loss: 0.7887 - val_acc: 0.7647\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RGwKAGtRDEm8",
        "outputId": "5ec6e7e4-aed4-445c-8396-6b3e7a1248ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "print('Evaluate NN model with relu activations'); print('--'*40)\n",
        "results2 = model2.evaluate(X_val, y_val)\n",
        "print('Validation accuracy: {}'.format(round(results2[1]*100, 2), '%'))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluate NN model with relu activations\n",
            "--------------------------------------------------------------------------------\n",
            "60000/60000 [==============================] - 4s 66us/sample - loss: 0.7887 - acc: 0.7647\n",
            "Validation accuracy: 76.47\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4p0ZX0GTRRyG"
      },
      "source": [
        "#### NN model, relu activations, SGD optimizer, changing learning rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DWIBrdDdGnDY",
        "outputId": "89876955-5da7-4dac-df18-1a40bc4949b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%time\n",
        "print('NN model with relu activations and sgd optimizers - changing learning rate'); print('--'*40)\n",
        "# compiling the neural network classifier, sgd optimizer\n",
        "sgd = optimizers.SGD(lr = 0.001)\n",
        "model2.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# Fitting the neural network for training\n",
        "history = model2.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
            "Wall time: 5.72 µs\n",
            "NN model with relu activations and sgd optimizers - changing learning rate\n",
            "--------------------------------------------------------------------------------\n",
            "Train on 42000 samples, validate on 60000 samples\n",
            "Epoch 1/100\n",
            "42000/42000 [==============================] - 1s 36us/sample - loss: 0.6972 - acc: 0.7959 - val_loss: 0.7161 - val_acc: 0.7925\n",
            "Epoch 2/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.6939 - acc: 0.7978 - val_loss: 0.7160 - val_acc: 0.7917\n",
            "Epoch 3/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6934 - acc: 0.7981 - val_loss: 0.7145 - val_acc: 0.7926\n",
            "Epoch 4/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 0.6929 - acc: 0.7978 - val_loss: 0.7142 - val_acc: 0.7931\n",
            "Epoch 5/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6924 - acc: 0.7974 - val_loss: 0.7142 - val_acc: 0.7928\n",
            "Epoch 6/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6918 - acc: 0.7989 - val_loss: 0.7143 - val_acc: 0.7924\n",
            "Epoch 7/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6916 - acc: 0.7985 - val_loss: 0.7140 - val_acc: 0.7929\n",
            "Epoch 8/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6913 - acc: 0.7987 - val_loss: 0.7127 - val_acc: 0.7929\n",
            "Epoch 9/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 0.6908 - acc: 0.7982 - val_loss: 0.7132 - val_acc: 0.7933\n",
            "Epoch 10/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6902 - acc: 0.7987 - val_loss: 0.7120 - val_acc: 0.7932\n",
            "Epoch 11/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6900 - acc: 0.7994 - val_loss: 0.7120 - val_acc: 0.7939\n",
            "Epoch 12/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 0.6897 - acc: 0.7988 - val_loss: 0.7114 - val_acc: 0.7935\n",
            "Epoch 13/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6892 - acc: 0.7996 - val_loss: 0.7110 - val_acc: 0.7935\n",
            "Epoch 14/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6888 - acc: 0.7995 - val_loss: 0.7116 - val_acc: 0.7931\n",
            "Epoch 15/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6884 - acc: 0.8003 - val_loss: 0.7100 - val_acc: 0.7944\n",
            "Epoch 16/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6881 - acc: 0.7993 - val_loss: 0.7104 - val_acc: 0.7944\n",
            "Epoch 17/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6877 - acc: 0.7993 - val_loss: 0.7103 - val_acc: 0.7942\n",
            "Epoch 18/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6875 - acc: 0.8001 - val_loss: 0.7097 - val_acc: 0.7943\n",
            "Epoch 19/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6870 - acc: 0.8005 - val_loss: 0.7094 - val_acc: 0.7946\n",
            "Epoch 20/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6868 - acc: 0.8006 - val_loss: 0.7098 - val_acc: 0.7939\n",
            "Epoch 21/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6865 - acc: 0.7998 - val_loss: 0.7084 - val_acc: 0.7951\n",
            "Epoch 22/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6861 - acc: 0.8000 - val_loss: 0.7089 - val_acc: 0.7951\n",
            "Epoch 23/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6856 - acc: 0.8004 - val_loss: 0.7089 - val_acc: 0.7941\n",
            "Epoch 24/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.6854 - acc: 0.8009 - val_loss: 0.7080 - val_acc: 0.7944\n",
            "Epoch 25/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6847 - acc: 0.8006 - val_loss: 0.7072 - val_acc: 0.7947\n",
            "Epoch 26/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6846 - acc: 0.8006 - val_loss: 0.7073 - val_acc: 0.7954\n",
            "Epoch 27/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6843 - acc: 0.8007 - val_loss: 0.7069 - val_acc: 0.7949\n",
            "Epoch 28/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6840 - acc: 0.8003 - val_loss: 0.7065 - val_acc: 0.7957\n",
            "Epoch 29/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6837 - acc: 0.8006 - val_loss: 0.7068 - val_acc: 0.7952\n",
            "Epoch 30/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6832 - acc: 0.8010 - val_loss: 0.7060 - val_acc: 0.7958\n",
            "Epoch 31/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 0.6828 - acc: 0.8012 - val_loss: 0.7063 - val_acc: 0.7956\n",
            "Epoch 32/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6826 - acc: 0.8012 - val_loss: 0.7050 - val_acc: 0.7960\n",
            "Epoch 33/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6820 - acc: 0.8020 - val_loss: 0.7057 - val_acc: 0.7959\n",
            "Epoch 34/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6816 - acc: 0.8016 - val_loss: 0.7043 - val_acc: 0.7960\n",
            "Epoch 35/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6816 - acc: 0.8012 - val_loss: 0.7042 - val_acc: 0.7955\n",
            "Epoch 36/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 0.6812 - acc: 0.8015 - val_loss: 0.7051 - val_acc: 0.7965\n",
            "Epoch 37/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 0.6810 - acc: 0.8023 - val_loss: 0.7035 - val_acc: 0.7961\n",
            "Epoch 38/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6805 - acc: 0.8018 - val_loss: 0.7038 - val_acc: 0.7962\n",
            "Epoch 39/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6801 - acc: 0.8022 - val_loss: 0.7029 - val_acc: 0.7968\n",
            "Epoch 40/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 0.6796 - acc: 0.8027 - val_loss: 0.7042 - val_acc: 0.7954\n",
            "Epoch 41/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6796 - acc: 0.8028 - val_loss: 0.7025 - val_acc: 0.7963\n",
            "Epoch 42/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 0.6791 - acc: 0.8021 - val_loss: 0.7020 - val_acc: 0.7973\n",
            "Epoch 43/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6788 - acc: 0.8025 - val_loss: 0.7018 - val_acc: 0.7966\n",
            "Epoch 44/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6784 - acc: 0.8024 - val_loss: 0.7024 - val_acc: 0.7962\n",
            "Epoch 45/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6779 - acc: 0.8026 - val_loss: 0.7020 - val_acc: 0.7971\n",
            "Epoch 46/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6781 - acc: 0.8030 - val_loss: 0.7012 - val_acc: 0.7969\n",
            "Epoch 47/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6775 - acc: 0.8029 - val_loss: 0.7006 - val_acc: 0.7972\n",
            "Epoch 48/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6770 - acc: 0.8024 - val_loss: 0.7001 - val_acc: 0.7972\n",
            "Epoch 49/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 0.6767 - acc: 0.8030 - val_loss: 0.7003 - val_acc: 0.7976\n",
            "Epoch 50/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 0.6766 - acc: 0.8031 - val_loss: 0.7001 - val_acc: 0.7968\n",
            "Epoch 51/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6760 - acc: 0.8038 - val_loss: 0.6994 - val_acc: 0.7980\n",
            "Epoch 52/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6757 - acc: 0.8030 - val_loss: 0.6999 - val_acc: 0.7975\n",
            "Epoch 53/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.6754 - acc: 0.8037 - val_loss: 0.7002 - val_acc: 0.7969\n",
            "Epoch 54/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6750 - acc: 0.8040 - val_loss: 0.6988 - val_acc: 0.7974\n",
            "Epoch 55/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6747 - acc: 0.8037 - val_loss: 0.6993 - val_acc: 0.7977\n",
            "Epoch 56/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 0.6743 - acc: 0.8034 - val_loss: 0.6999 - val_acc: 0.7974\n",
            "Epoch 57/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 0.6742 - acc: 0.8037 - val_loss: 0.6980 - val_acc: 0.7973\n",
            "Epoch 58/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6738 - acc: 0.8036 - val_loss: 0.6979 - val_acc: 0.7982\n",
            "Epoch 59/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6736 - acc: 0.8036 - val_loss: 0.6968 - val_acc: 0.7987\n",
            "Epoch 60/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6733 - acc: 0.8040 - val_loss: 0.6969 - val_acc: 0.7979\n",
            "Epoch 61/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.6727 - acc: 0.8038 - val_loss: 0.6963 - val_acc: 0.7981\n",
            "Epoch 62/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6724 - acc: 0.8042 - val_loss: 0.6964 - val_acc: 0.7980\n",
            "Epoch 63/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6724 - acc: 0.8050 - val_loss: 0.6961 - val_acc: 0.7985\n",
            "Epoch 64/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6718 - acc: 0.8044 - val_loss: 0.6959 - val_acc: 0.7991\n",
            "Epoch 65/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6716 - acc: 0.8047 - val_loss: 0.6954 - val_acc: 0.7997\n",
            "Epoch 66/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.6712 - acc: 0.8047 - val_loss: 0.6949 - val_acc: 0.7986\n",
            "Epoch 67/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6709 - acc: 0.8049 - val_loss: 0.6950 - val_acc: 0.7988\n",
            "Epoch 68/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6705 - acc: 0.8057 - val_loss: 0.6951 - val_acc: 0.7989\n",
            "Epoch 69/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6702 - acc: 0.8042 - val_loss: 0.6947 - val_acc: 0.7987\n",
            "Epoch 70/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6701 - acc: 0.8050 - val_loss: 0.6944 - val_acc: 0.7990\n",
            "Epoch 71/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6698 - acc: 0.8046 - val_loss: 0.6943 - val_acc: 0.7990\n",
            "Epoch 72/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6692 - acc: 0.8055 - val_loss: 0.6936 - val_acc: 0.7987\n",
            "Epoch 73/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6691 - acc: 0.8058 - val_loss: 0.6929 - val_acc: 0.7999\n",
            "Epoch 74/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6685 - acc: 0.8057 - val_loss: 0.6928 - val_acc: 0.7997\n",
            "Epoch 75/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6685 - acc: 0.8053 - val_loss: 0.6931 - val_acc: 0.7995\n",
            "Epoch 76/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.6677 - acc: 0.8065 - val_loss: 0.6936 - val_acc: 0.7986\n",
            "Epoch 77/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6674 - acc: 0.8054 - val_loss: 0.6923 - val_acc: 0.7991\n",
            "Epoch 78/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6675 - acc: 0.8063 - val_loss: 0.6918 - val_acc: 0.7996\n",
            "Epoch 79/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 0.6670 - acc: 0.8068 - val_loss: 0.6913 - val_acc: 0.7997\n",
            "Epoch 80/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6668 - acc: 0.8067 - val_loss: 0.6908 - val_acc: 0.8005\n",
            "Epoch 81/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6664 - acc: 0.8065 - val_loss: 0.6905 - val_acc: 0.7998\n",
            "Epoch 82/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6662 - acc: 0.8060 - val_loss: 0.6901 - val_acc: 0.8009\n",
            "Epoch 83/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.6658 - acc: 0.8069 - val_loss: 0.6901 - val_acc: 0.7997\n",
            "Epoch 84/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6656 - acc: 0.8058 - val_loss: 0.6903 - val_acc: 0.8005\n",
            "Epoch 85/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6652 - acc: 0.8062 - val_loss: 0.6896 - val_acc: 0.8000\n",
            "Epoch 86/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6650 - acc: 0.8063 - val_loss: 0.6892 - val_acc: 0.8007\n",
            "Epoch 87/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6644 - acc: 0.8069 - val_loss: 0.6898 - val_acc: 0.7999\n",
            "Epoch 88/100\n",
            "42000/42000 [==============================] - 1s 33us/sample - loss: 0.6644 - acc: 0.8068 - val_loss: 0.6882 - val_acc: 0.8004\n",
            "Epoch 89/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6638 - acc: 0.8072 - val_loss: 0.6890 - val_acc: 0.8012\n",
            "Epoch 90/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6633 - acc: 0.8073 - val_loss: 0.6878 - val_acc: 0.8013\n",
            "Epoch 91/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6632 - acc: 0.8072 - val_loss: 0.6880 - val_acc: 0.8015\n",
            "Epoch 92/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6631 - acc: 0.8073 - val_loss: 0.6890 - val_acc: 0.8000\n",
            "Epoch 93/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6623 - acc: 0.8075 - val_loss: 0.6877 - val_acc: 0.8012\n",
            "Epoch 94/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6624 - acc: 0.8071 - val_loss: 0.6875 - val_acc: 0.8015\n",
            "Epoch 95/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6622 - acc: 0.8071 - val_loss: 0.6863 - val_acc: 0.8021\n",
            "Epoch 96/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.6616 - acc: 0.8064 - val_loss: 0.6863 - val_acc: 0.8013\n",
            "Epoch 97/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6613 - acc: 0.8077 - val_loss: 0.6866 - val_acc: 0.8019\n",
            "Epoch 98/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.6610 - acc: 0.8074 - val_loss: 0.6867 - val_acc: 0.8006\n",
            "Epoch 99/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6605 - acc: 0.8079 - val_loss: 0.6862 - val_acc: 0.8016\n",
            "Epoch 100/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6605 - acc: 0.8074 - val_loss: 0.6857 - val_acc: 0.8026\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9O9KhqH5HMWW",
        "outputId": "62ac8991-4e7f-446e-897b-8e7fbb350931",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "print('Evaluate NN model with relu activations'); print('--'*40)\n",
        "results2 = model2.evaluate(X_val, y_val)\n",
        "print('Validation accuracy: {}'.format(round(results2[1]*100, 2), '%'))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluate NN model with relu activations\n",
            "--------------------------------------------------------------------------------\n",
            "60000/60000 [==============================] - 4s 66us/sample - loss: 0.6857 - acc: 0.8026\n",
            "Validation accuracy: 80.26\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CKJFAr9JRfJJ"
      },
      "source": [
        "#### NN model, relu activations, adam optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9Q8PtyA-Jw74",
        "outputId": "f2eeec72-92e0-44d9-b373-c71a0b9c2592",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%time\n",
        "print('NN model with relu activations and adam optimizer'); print('--'*40)\n",
        "# compiling the neural network classifier, adam optimizer\n",
        "adam = optimizers.Adam(lr = 0.01)\n",
        "model2.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# Fitting the neural network for training\n",
        "history = model2.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
            "Wall time: 6.91 µs\n",
            "NN model with relu activations and adam optimizer\n",
            "--------------------------------------------------------------------------------\n",
            "Train on 42000 samples, validate on 60000 samples\n",
            "Epoch 1/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 3.4388 - acc: 0.1415 - val_loss: 2.2079 - val_acc: 0.1669\n",
            "Epoch 2/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 2.1457 - acc: 0.1876 - val_loss: 2.1208 - val_acc: 0.1866\n",
            "Epoch 3/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 2.0520 - acc: 0.2217 - val_loss: 2.0034 - val_acc: 0.2390\n",
            "Epoch 4/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 2.0003 - acc: 0.2417 - val_loss: 1.9829 - val_acc: 0.2522\n",
            "Epoch 5/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.9847 - acc: 0.2495 - val_loss: 1.9680 - val_acc: 0.2583\n",
            "Epoch 6/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.9529 - acc: 0.2733 - val_loss: 1.8472 - val_acc: 0.3402\n",
            "Epoch 7/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.7928 - acc: 0.3628 - val_loss: 1.6587 - val_acc: 0.4146\n",
            "Epoch 8/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.6868 - acc: 0.4008 - val_loss: 1.6035 - val_acc: 0.4359\n",
            "Epoch 9/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.6311 - acc: 0.4229 - val_loss: 1.5752 - val_acc: 0.4355\n",
            "Epoch 10/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.5817 - acc: 0.4430 - val_loss: 1.5229 - val_acc: 0.4695\n",
            "Epoch 11/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.5774 - acc: 0.4453 - val_loss: 1.5093 - val_acc: 0.4838\n",
            "Epoch 12/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.5341 - acc: 0.4675 - val_loss: 1.5085 - val_acc: 0.4774\n",
            "Epoch 13/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.5145 - acc: 0.4775 - val_loss: 1.6087 - val_acc: 0.4377\n",
            "Epoch 14/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.4909 - acc: 0.4900 - val_loss: 1.4441 - val_acc: 0.5077\n",
            "Epoch 15/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.5009 - acc: 0.4833 - val_loss: 1.5004 - val_acc: 0.4881\n",
            "Epoch 16/100\n",
            "42000/42000 [==============================] - 1s 36us/sample - loss: 1.4728 - acc: 0.4991 - val_loss: 1.4640 - val_acc: 0.5032\n",
            "Epoch 17/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.4694 - acc: 0.4989 - val_loss: 1.5071 - val_acc: 0.4837\n",
            "Epoch 18/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.4584 - acc: 0.5072 - val_loss: 1.4648 - val_acc: 0.5031\n",
            "Epoch 19/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.4515 - acc: 0.5095 - val_loss: 1.4345 - val_acc: 0.5170\n",
            "Epoch 20/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.4783 - acc: 0.4974 - val_loss: 1.4457 - val_acc: 0.5131\n",
            "Epoch 21/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.4571 - acc: 0.5085 - val_loss: 1.4256 - val_acc: 0.5208\n",
            "Epoch 22/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.4398 - acc: 0.5157 - val_loss: 1.4845 - val_acc: 0.4884\n",
            "Epoch 23/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.4552 - acc: 0.5081 - val_loss: 1.4265 - val_acc: 0.5206\n",
            "Epoch 24/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.4320 - acc: 0.5178 - val_loss: 1.4044 - val_acc: 0.5291\n",
            "Epoch 25/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.4622 - acc: 0.5040 - val_loss: 1.5103 - val_acc: 0.4882\n",
            "Epoch 26/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.4341 - acc: 0.5163 - val_loss: 1.4085 - val_acc: 0.5261\n",
            "Epoch 27/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.4534 - acc: 0.5077 - val_loss: 1.3916 - val_acc: 0.5356\n",
            "Epoch 28/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.4315 - acc: 0.5174 - val_loss: 1.4067 - val_acc: 0.5271\n",
            "Epoch 29/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.4359 - acc: 0.5162 - val_loss: 1.3984 - val_acc: 0.5339\n",
            "Epoch 30/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.4371 - acc: 0.5165 - val_loss: 1.4966 - val_acc: 0.4955\n",
            "Epoch 31/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.4300 - acc: 0.5201 - val_loss: 1.3932 - val_acc: 0.5344\n",
            "Epoch 32/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.4321 - acc: 0.5191 - val_loss: 1.4106 - val_acc: 0.5280\n",
            "Epoch 33/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.4357 - acc: 0.5171 - val_loss: 1.4122 - val_acc: 0.5288\n",
            "Epoch 34/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.4125 - acc: 0.5257 - val_loss: 1.4076 - val_acc: 0.5299\n",
            "Epoch 35/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.4354 - acc: 0.5201 - val_loss: 1.4268 - val_acc: 0.5214\n",
            "Epoch 36/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.4341 - acc: 0.5179 - val_loss: 1.4110 - val_acc: 0.5246\n",
            "Epoch 37/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.4232 - acc: 0.5214 - val_loss: 1.4705 - val_acc: 0.5030\n",
            "Epoch 38/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.4197 - acc: 0.5233 - val_loss: 1.4148 - val_acc: 0.5270\n",
            "Epoch 39/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.4113 - acc: 0.5270 - val_loss: 1.4325 - val_acc: 0.5171\n",
            "Epoch 40/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.4407 - acc: 0.5143 - val_loss: 1.4027 - val_acc: 0.5316\n",
            "Epoch 41/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.4286 - acc: 0.5215 - val_loss: 1.4043 - val_acc: 0.5274\n",
            "Epoch 42/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.4231 - acc: 0.5205 - val_loss: 1.4333 - val_acc: 0.5208\n",
            "Epoch 43/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.4127 - acc: 0.5281 - val_loss: 1.5796 - val_acc: 0.4553\n",
            "Epoch 44/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 1.4262 - acc: 0.5201 - val_loss: 1.4211 - val_acc: 0.5216\n",
            "Epoch 45/100\n",
            "42000/42000 [==============================] - 2s 38us/sample - loss: 1.4118 - acc: 0.5276 - val_loss: 1.4146 - val_acc: 0.5260\n",
            "Epoch 46/100\n",
            "42000/42000 [==============================] - 2s 38us/sample - loss: 1.4302 - acc: 0.5199 - val_loss: 1.4167 - val_acc: 0.5287\n",
            "Epoch 47/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 1.4349 - acc: 0.5160 - val_loss: 1.4819 - val_acc: 0.5010\n",
            "Epoch 48/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 1.4280 - acc: 0.5223 - val_loss: 1.4176 - val_acc: 0.5236\n",
            "Epoch 49/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 1.4226 - acc: 0.5211 - val_loss: 1.3991 - val_acc: 0.5318\n",
            "Epoch 50/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 1.4304 - acc: 0.5196 - val_loss: 1.4190 - val_acc: 0.5282\n",
            "Epoch 51/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.4280 - acc: 0.5220 - val_loss: 1.3826 - val_acc: 0.5409\n",
            "Epoch 52/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.4199 - acc: 0.5246 - val_loss: 1.4424 - val_acc: 0.5132\n",
            "Epoch 53/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.4297 - acc: 0.5206 - val_loss: 1.4060 - val_acc: 0.5295\n",
            "Epoch 54/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.4170 - acc: 0.5234 - val_loss: 1.3851 - val_acc: 0.5393\n",
            "Epoch 55/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.4108 - acc: 0.5285 - val_loss: 1.4151 - val_acc: 0.5291\n",
            "Epoch 56/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.4238 - acc: 0.5235 - val_loss: 1.4169 - val_acc: 0.5283\n",
            "Epoch 57/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.4230 - acc: 0.5239 - val_loss: 1.3931 - val_acc: 0.5376\n",
            "Epoch 58/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.4278 - acc: 0.5198 - val_loss: 1.4392 - val_acc: 0.5150\n",
            "Epoch 59/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.4122 - acc: 0.5262 - val_loss: 1.4460 - val_acc: 0.5155\n",
            "Epoch 60/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.4123 - acc: 0.5278 - val_loss: 1.3857 - val_acc: 0.5383\n",
            "Epoch 61/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.4247 - acc: 0.5228 - val_loss: 1.3878 - val_acc: 0.5362\n",
            "Epoch 62/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.4144 - acc: 0.5263 - val_loss: 1.4072 - val_acc: 0.5282\n",
            "Epoch 63/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.4050 - acc: 0.5282 - val_loss: 1.3884 - val_acc: 0.5352\n",
            "Epoch 64/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.4148 - acc: 0.5269 - val_loss: 1.3979 - val_acc: 0.5334\n",
            "Epoch 65/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.4170 - acc: 0.5277 - val_loss: 1.4233 - val_acc: 0.5211\n",
            "Epoch 66/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.4399 - acc: 0.5147 - val_loss: 1.4137 - val_acc: 0.5215\n",
            "Epoch 67/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.4131 - acc: 0.5275 - val_loss: 1.4684 - val_acc: 0.5048\n",
            "Epoch 68/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.4116 - acc: 0.5298 - val_loss: 1.4120 - val_acc: 0.5312\n",
            "Epoch 69/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.4187 - acc: 0.5258 - val_loss: 1.3929 - val_acc: 0.5356\n",
            "Epoch 70/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.4103 - acc: 0.5284 - val_loss: 1.3737 - val_acc: 0.5451\n",
            "Epoch 71/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.4129 - acc: 0.5286 - val_loss: 1.3803 - val_acc: 0.5423\n",
            "Epoch 72/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.4204 - acc: 0.5231 - val_loss: 1.3870 - val_acc: 0.5371\n",
            "Epoch 73/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.4202 - acc: 0.5251 - val_loss: 1.4315 - val_acc: 0.5177\n",
            "Epoch 74/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.4198 - acc: 0.5244 - val_loss: 1.4195 - val_acc: 0.5225\n",
            "Epoch 75/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.4102 - acc: 0.5262 - val_loss: 1.4227 - val_acc: 0.5271\n",
            "Epoch 76/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.4166 - acc: 0.5245 - val_loss: 1.3863 - val_acc: 0.5386\n",
            "Epoch 77/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.4047 - acc: 0.5311 - val_loss: 1.3721 - val_acc: 0.5448\n",
            "Epoch 78/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.3980 - acc: 0.5340 - val_loss: 1.3840 - val_acc: 0.5403\n",
            "Epoch 79/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.4244 - acc: 0.5229 - val_loss: 1.4608 - val_acc: 0.5154\n",
            "Epoch 80/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.4136 - acc: 0.5297 - val_loss: 1.3956 - val_acc: 0.5326\n",
            "Epoch 81/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.4036 - acc: 0.5310 - val_loss: 1.3858 - val_acc: 0.5396\n",
            "Epoch 82/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.4016 - acc: 0.5322 - val_loss: 1.3925 - val_acc: 0.5376\n",
            "Epoch 83/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.4102 - acc: 0.5270 - val_loss: 1.4327 - val_acc: 0.5238\n",
            "Epoch 84/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.4230 - acc: 0.5263 - val_loss: 1.4264 - val_acc: 0.5217\n",
            "Epoch 85/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.4085 - acc: 0.5299 - val_loss: 1.4280 - val_acc: 0.5218\n",
            "Epoch 86/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.4261 - acc: 0.5226 - val_loss: 1.4097 - val_acc: 0.5260\n",
            "Epoch 87/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.4276 - acc: 0.5219 - val_loss: 1.3937 - val_acc: 0.5373\n",
            "Epoch 88/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.4197 - acc: 0.5243 - val_loss: 1.5029 - val_acc: 0.4927\n",
            "Epoch 89/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.4053 - acc: 0.5311 - val_loss: 1.3806 - val_acc: 0.5384\n",
            "Epoch 90/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.4192 - acc: 0.5256 - val_loss: 1.4594 - val_acc: 0.5104\n",
            "Epoch 91/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.4175 - acc: 0.5273 - val_loss: 1.4107 - val_acc: 0.5258\n",
            "Epoch 92/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.4263 - acc: 0.5234 - val_loss: 1.4172 - val_acc: 0.5322\n",
            "Epoch 93/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.4211 - acc: 0.5250 - val_loss: 1.4141 - val_acc: 0.5262\n",
            "Epoch 94/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.4048 - acc: 0.5333 - val_loss: 1.4179 - val_acc: 0.5243\n",
            "Epoch 95/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.4135 - acc: 0.5267 - val_loss: 1.3923 - val_acc: 0.5361\n",
            "Epoch 96/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.4042 - acc: 0.5330 - val_loss: 1.4053 - val_acc: 0.5318\n",
            "Epoch 97/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.4094 - acc: 0.5309 - val_loss: 1.3823 - val_acc: 0.5431\n",
            "Epoch 98/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.4045 - acc: 0.5324 - val_loss: 1.4404 - val_acc: 0.5164\n",
            "Epoch 99/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.4162 - acc: 0.5276 - val_loss: 1.3838 - val_acc: 0.5419\n",
            "Epoch 100/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.4040 - acc: 0.5336 - val_loss: 1.3807 - val_acc: 0.5409\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pAICaMQJKFE5",
        "outputId": "07bff301-1de4-4663-ebc8-537831f35c57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "print('Evaluate NN model with relu activations'); print('--'*40)\n",
        "results2 = model2.evaluate(X_val, y_val)\n",
        "print('Validation accuracy: {}'.format(round(results2[1]*100, 2), '%'))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluate NN model with relu activations\n",
            "--------------------------------------------------------------------------------\n",
            "60000/60000 [==============================] - 4s 66us/sample - loss: 1.3807 - acc: 0.5409\n",
            "Validation accuracy: 54.09\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JT_j1efHRpGa"
      },
      "source": [
        "#### NN model, relu activations, adam optimizer, changing learning rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "j6CfiDwmRscX",
        "outputId": "0a8946e3-1264-414c-cbee-218b948805fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%time\n",
        "print('NN model with relu activations and adam optimizer'); print('--'*40)\n",
        "# compiling the neural network classifier, adam optimizer\n",
        "adam = optimizers.Adam(lr = 0.001)\n",
        "model2.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# Fitting the neural network for training\n",
        "history = model2.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3 µs, sys: 2 µs, total: 5 µs\n",
            "Wall time: 9.3 µs\n",
            "NN model with relu activations and adam optimizer\n",
            "--------------------------------------------------------------------------------\n",
            "Train on 42000 samples, validate on 60000 samples\n",
            "Epoch 1/100\n",
            "42000/42000 [==============================] - 2s 38us/sample - loss: 1.3522 - acc: 0.5522 - val_loss: 1.3521 - val_acc: 0.5541\n",
            "Epoch 2/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.3476 - acc: 0.5532 - val_loss: 1.3462 - val_acc: 0.5562\n",
            "Epoch 3/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3460 - acc: 0.5537 - val_loss: 1.3492 - val_acc: 0.5551\n",
            "Epoch 4/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3471 - acc: 0.5536 - val_loss: 1.3479 - val_acc: 0.5548\n",
            "Epoch 5/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3470 - acc: 0.5546 - val_loss: 1.3447 - val_acc: 0.5564\n",
            "Epoch 6/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3448 - acc: 0.5553 - val_loss: 1.3467 - val_acc: 0.5547\n",
            "Epoch 7/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.3442 - acc: 0.5550 - val_loss: 1.3442 - val_acc: 0.5550\n",
            "Epoch 8/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3455 - acc: 0.5554 - val_loss: 1.3536 - val_acc: 0.5521\n",
            "Epoch 9/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3436 - acc: 0.5556 - val_loss: 1.3475 - val_acc: 0.5548\n",
            "Epoch 10/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3448 - acc: 0.5569 - val_loss: 1.3526 - val_acc: 0.5504\n",
            "Epoch 11/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3439 - acc: 0.5542 - val_loss: 1.3466 - val_acc: 0.5548\n",
            "Epoch 12/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3425 - acc: 0.5574 - val_loss: 1.3424 - val_acc: 0.5564\n",
            "Epoch 13/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3420 - acc: 0.5553 - val_loss: 1.3532 - val_acc: 0.5510\n",
            "Epoch 14/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3425 - acc: 0.5560 - val_loss: 1.3601 - val_acc: 0.5476\n",
            "Epoch 15/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 1.3413 - acc: 0.5572 - val_loss: 1.3627 - val_acc: 0.5468\n",
            "Epoch 16/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3415 - acc: 0.5551 - val_loss: 1.3442 - val_acc: 0.5556\n",
            "Epoch 17/100\n",
            "42000/42000 [==============================] - 1s 36us/sample - loss: 1.3420 - acc: 0.5554 - val_loss: 1.3547 - val_acc: 0.5501\n",
            "Epoch 18/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.3446 - acc: 0.5558 - val_loss: 1.3442 - val_acc: 0.5546\n",
            "Epoch 19/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3423 - acc: 0.5552 - val_loss: 1.3509 - val_acc: 0.5528\n",
            "Epoch 20/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3415 - acc: 0.5564 - val_loss: 1.3455 - val_acc: 0.5550\n",
            "Epoch 21/100\n",
            "42000/42000 [==============================] - 1s 36us/sample - loss: 1.3405 - acc: 0.5553 - val_loss: 1.3470 - val_acc: 0.5536\n",
            "Epoch 22/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.3417 - acc: 0.5553 - val_loss: 1.3438 - val_acc: 0.5553\n",
            "Epoch 23/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3414 - acc: 0.5567 - val_loss: 1.3415 - val_acc: 0.5582\n",
            "Epoch 24/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.3428 - acc: 0.5551 - val_loss: 1.3432 - val_acc: 0.5559\n",
            "Epoch 25/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3403 - acc: 0.5559 - val_loss: 1.3478 - val_acc: 0.5526\n",
            "Epoch 26/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3402 - acc: 0.5552 - val_loss: 1.3427 - val_acc: 0.5554\n",
            "Epoch 27/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3405 - acc: 0.5569 - val_loss: 1.3418 - val_acc: 0.5562\n",
            "Epoch 28/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3391 - acc: 0.5572 - val_loss: 1.3474 - val_acc: 0.5537\n",
            "Epoch 29/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3385 - acc: 0.5585 - val_loss: 1.3466 - val_acc: 0.5551\n",
            "Epoch 30/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3402 - acc: 0.5557 - val_loss: 1.3425 - val_acc: 0.5568\n",
            "Epoch 31/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.3392 - acc: 0.5565 - val_loss: 1.3397 - val_acc: 0.5579\n",
            "Epoch 32/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.3383 - acc: 0.5570 - val_loss: 1.3509 - val_acc: 0.5523\n",
            "Epoch 33/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3384 - acc: 0.5590 - val_loss: 1.3389 - val_acc: 0.5571\n",
            "Epoch 34/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3371 - acc: 0.5588 - val_loss: 1.3588 - val_acc: 0.5472\n",
            "Epoch 35/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3402 - acc: 0.5565 - val_loss: 1.3395 - val_acc: 0.5576\n",
            "Epoch 36/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3374 - acc: 0.5581 - val_loss: 1.3495 - val_acc: 0.5535\n",
            "Epoch 37/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3384 - acc: 0.5564 - val_loss: 1.3473 - val_acc: 0.5538\n",
            "Epoch 38/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3378 - acc: 0.5572 - val_loss: 1.3417 - val_acc: 0.5563\n",
            "Epoch 39/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3382 - acc: 0.5560 - val_loss: 1.3413 - val_acc: 0.5568\n",
            "Epoch 40/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3384 - acc: 0.5570 - val_loss: 1.3389 - val_acc: 0.5578\n",
            "Epoch 41/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3361 - acc: 0.5593 - val_loss: 1.3428 - val_acc: 0.5555\n",
            "Epoch 42/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3373 - acc: 0.5576 - val_loss: 1.3418 - val_acc: 0.5571\n",
            "Epoch 43/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3368 - acc: 0.5565 - val_loss: 1.3439 - val_acc: 0.5551\n",
            "Epoch 44/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3378 - acc: 0.5573 - val_loss: 1.3432 - val_acc: 0.5569\n",
            "Epoch 45/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3386 - acc: 0.5559 - val_loss: 1.3533 - val_acc: 0.5508\n",
            "Epoch 46/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3383 - acc: 0.5587 - val_loss: 1.3372 - val_acc: 0.5581\n",
            "Epoch 47/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3370 - acc: 0.5576 - val_loss: 1.3434 - val_acc: 0.5540\n",
            "Epoch 48/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3370 - acc: 0.5569 - val_loss: 1.3473 - val_acc: 0.5531\n",
            "Epoch 49/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3374 - acc: 0.5564 - val_loss: 1.3440 - val_acc: 0.5563\n",
            "Epoch 50/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3382 - acc: 0.5569 - val_loss: 1.3425 - val_acc: 0.5567\n",
            "Epoch 51/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.3355 - acc: 0.5570 - val_loss: 1.3420 - val_acc: 0.5552\n",
            "Epoch 52/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3371 - acc: 0.5566 - val_loss: 1.3422 - val_acc: 0.5559\n",
            "Epoch 53/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.3375 - acc: 0.5573 - val_loss: 1.3481 - val_acc: 0.5536\n",
            "Epoch 54/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3367 - acc: 0.5586 - val_loss: 1.3465 - val_acc: 0.5551\n",
            "Epoch 55/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3364 - acc: 0.5560 - val_loss: 1.3373 - val_acc: 0.5582\n",
            "Epoch 56/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3381 - acc: 0.5570 - val_loss: 1.3496 - val_acc: 0.5526\n",
            "Epoch 57/100\n",
            "42000/42000 [==============================] - 1s 36us/sample - loss: 1.3362 - acc: 0.5566 - val_loss: 1.3449 - val_acc: 0.5551\n",
            "Epoch 58/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3371 - acc: 0.5578 - val_loss: 1.3360 - val_acc: 0.5588\n",
            "Epoch 59/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3349 - acc: 0.5578 - val_loss: 1.3396 - val_acc: 0.5570\n",
            "Epoch 60/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3354 - acc: 0.5576 - val_loss: 1.3376 - val_acc: 0.5584\n",
            "Epoch 61/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3362 - acc: 0.5577 - val_loss: 1.3453 - val_acc: 0.5545\n",
            "Epoch 62/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3373 - acc: 0.5578 - val_loss: 1.3377 - val_acc: 0.5581\n",
            "Epoch 63/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3366 - acc: 0.5573 - val_loss: 1.3387 - val_acc: 0.5573\n",
            "Epoch 64/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 1.3350 - acc: 0.5576 - val_loss: 1.3387 - val_acc: 0.5573\n",
            "Epoch 65/100\n",
            "42000/42000 [==============================] - 1s 36us/sample - loss: 1.3354 - acc: 0.5572 - val_loss: 1.3442 - val_acc: 0.5550\n",
            "Epoch 66/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3378 - acc: 0.5563 - val_loss: 1.3417 - val_acc: 0.5563\n",
            "Epoch 67/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 1.3351 - acc: 0.5583 - val_loss: 1.3379 - val_acc: 0.5577\n",
            "Epoch 68/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3359 - acc: 0.5566 - val_loss: 1.3423 - val_acc: 0.5573\n",
            "Epoch 69/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3347 - acc: 0.5595 - val_loss: 1.3372 - val_acc: 0.5579\n",
            "Epoch 70/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3358 - acc: 0.5568 - val_loss: 1.3410 - val_acc: 0.5555\n",
            "Epoch 71/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3354 - acc: 0.5584 - val_loss: 1.3406 - val_acc: 0.5577\n",
            "Epoch 72/100\n",
            "42000/42000 [==============================] - 1s 36us/sample - loss: 1.3336 - acc: 0.5587 - val_loss: 1.3495 - val_acc: 0.5518\n",
            "Epoch 73/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3345 - acc: 0.5580 - val_loss: 1.3361 - val_acc: 0.5585\n",
            "Epoch 74/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3323 - acc: 0.5585 - val_loss: 1.3394 - val_acc: 0.5566\n",
            "Epoch 75/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3355 - acc: 0.5570 - val_loss: 1.3368 - val_acc: 0.5581\n",
            "Epoch 76/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.3340 - acc: 0.5588 - val_loss: 1.3433 - val_acc: 0.5555\n",
            "Epoch 77/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.3361 - acc: 0.5580 - val_loss: 1.3389 - val_acc: 0.5579\n",
            "Epoch 78/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.3340 - acc: 0.5580 - val_loss: 1.3357 - val_acc: 0.5583\n",
            "Epoch 79/100\n",
            "42000/42000 [==============================] - 1s 36us/sample - loss: 1.3339 - acc: 0.5588 - val_loss: 1.3486 - val_acc: 0.5534\n",
            "Epoch 80/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3343 - acc: 0.5588 - val_loss: 1.3359 - val_acc: 0.5587\n",
            "Epoch 81/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3356 - acc: 0.5577 - val_loss: 1.3570 - val_acc: 0.5501\n",
            "Epoch 82/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.3333 - acc: 0.5573 - val_loss: 1.3494 - val_acc: 0.5537\n",
            "Epoch 83/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3330 - acc: 0.5573 - val_loss: 1.3386 - val_acc: 0.5569\n",
            "Epoch 84/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3342 - acc: 0.5577 - val_loss: 1.3379 - val_acc: 0.5571\n",
            "Epoch 85/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3370 - acc: 0.5560 - val_loss: 1.3461 - val_acc: 0.5541\n",
            "Epoch 86/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 1.3335 - acc: 0.5585 - val_loss: 1.3383 - val_acc: 0.5574\n",
            "Epoch 87/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3343 - acc: 0.5586 - val_loss: 1.3423 - val_acc: 0.5551\n",
            "Epoch 88/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3338 - acc: 0.5586 - val_loss: 1.3471 - val_acc: 0.5533\n",
            "Epoch 89/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3349 - acc: 0.5581 - val_loss: 1.3450 - val_acc: 0.5548\n",
            "Epoch 90/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3338 - acc: 0.5571 - val_loss: 1.3530 - val_acc: 0.5506\n",
            "Epoch 91/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3354 - acc: 0.5582 - val_loss: 1.3427 - val_acc: 0.5562\n",
            "Epoch 92/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3375 - acc: 0.5573 - val_loss: 1.3394 - val_acc: 0.5573\n",
            "Epoch 93/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3347 - acc: 0.5585 - val_loss: 1.3461 - val_acc: 0.5551\n",
            "Epoch 94/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.3328 - acc: 0.5584 - val_loss: 1.3555 - val_acc: 0.5504\n",
            "Epoch 95/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3350 - acc: 0.5584 - val_loss: 1.3389 - val_acc: 0.5571\n",
            "Epoch 96/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3339 - acc: 0.5589 - val_loss: 1.3338 - val_acc: 0.5597\n",
            "Epoch 97/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3338 - acc: 0.5575 - val_loss: 1.3391 - val_acc: 0.5582\n",
            "Epoch 98/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3345 - acc: 0.5572 - val_loss: 1.3437 - val_acc: 0.5547\n",
            "Epoch 99/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.3322 - acc: 0.5594 - val_loss: 1.3393 - val_acc: 0.5582\n",
            "Epoch 100/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3333 - acc: 0.5582 - val_loss: 1.3405 - val_acc: 0.5556\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sERp232wRvAN",
        "outputId": "4f7b0344-096b-4eb7-deb7-ddae473ae905",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "print('Evaluate NN model with relu activations'); print('--'*40)\n",
        "results2 = model2.evaluate(X_val, y_val)\n",
        "print('Validation accuracy: {}'.format(round(results2[1]*100, 2), '%'))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluate NN model with relu activations\n",
            "--------------------------------------------------------------------------------\n",
            "60000/60000 [==============================] - 4s 67us/sample - loss: 1.3405 - acc: 0.5556\n",
            "Validation accuracy: 55.56\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HKt6irM6ownA"
      },
      "source": [
        "<a id='o4'></a>\n",
        "##### Observation 4 - NN model with relu activations\n",
        "* Improves the scores considerably.\n",
        "* Best accuracy achieved till now is using relu activations, SGD optimizer, changing learning rate to 0.001.\n",
        "* Next, let's try and change the number of activators and see if the score improves."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "17b7kG5oR7J_"
      },
      "source": [
        "#### NN model, relu activations, changing number of activators, SGD optimizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "58Iskt9pKB3L",
        "outputId": "76a7276b-7887-4454-99a0-e567a1d09998",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print('NN model with relu activations and changing number of activators'); print('--'*40)\n",
        "# Initialize the neural network classifier\n",
        "model3 = Sequential()\n",
        "\n",
        "# Input Layer - adding input layer and activation functions relu\n",
        "model3.add(Dense(256, input_shape = (1024, )))\n",
        "# Adding activation function\n",
        "model3.add(Activation('relu'))\n",
        "\n",
        "#Hidden Layer 1 - adding first hidden layer\n",
        "model3.add(Dense(128))\n",
        "# Adding activation function\n",
        "model3.add(Activation('relu'))\n",
        "\n",
        "#Hidden Layer 2 - Adding second hidden layer\n",
        "model3.add(Dense(64))\n",
        "# Adding activation function\n",
        "model3.add(Activation('relu'))\n",
        "\n",
        "# Output Layer - adding output layer which is of 10 nodes (digits)\n",
        "model3.add(Dense(10))\n",
        "# Adding activation function - softmax for multiclass classification\n",
        "model3.add(Activation('softmax'))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NN model with relu activations and changing number of activators\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2q5A5rfawr08",
        "outputId": "1e903c35-6608-4e96-bde3-5f818cc35046",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        }
      },
      "source": [
        "model3.summary()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_6 (Dense)              (None, 256)               262400    \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 10)                650       \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 304,202\n",
            "Trainable params: 304,202\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WbApDf7hTG87",
        "outputId": "b08d2e3f-fdb2-4ddd-ace5-ea914b780994",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# compiling the neural network classifier, sgd optimizer\n",
        "sgd = optimizers.SGD(lr = 0.01)\n",
        "model3.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# Fitting the neural network for training\n",
        "history = model3.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 42000 samples, validate on 60000 samples\n",
            "Epoch 1/100\n",
            "42000/42000 [==============================] - 2s 39us/sample - loss: 2.2971 - acc: 0.1348 - val_loss: 2.2870 - val_acc: 0.1343\n",
            "Epoch 2/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 2.2803 - acc: 0.1766 - val_loss: 2.2722 - val_acc: 0.1969\n",
            "Epoch 3/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 2.2644 - acc: 0.2014 - val_loss: 2.2541 - val_acc: 0.2232\n",
            "Epoch 4/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 2.2432 - acc: 0.2322 - val_loss: 2.2293 - val_acc: 0.2510\n",
            "Epoch 5/100\n",
            "42000/42000 [==============================] - 1s 36us/sample - loss: 2.2141 - acc: 0.2630 - val_loss: 2.1947 - val_acc: 0.2808\n",
            "Epoch 6/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 2.1738 - acc: 0.2918 - val_loss: 2.1487 - val_acc: 0.3090\n",
            "Epoch 7/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 2.1227 - acc: 0.3162 - val_loss: 2.0912 - val_acc: 0.3305\n",
            "Epoch 8/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 2.0607 - acc: 0.3426 - val_loss: 2.0228 - val_acc: 0.3652\n",
            "Epoch 9/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.9881 - acc: 0.3706 - val_loss: 1.9453 - val_acc: 0.3896\n",
            "Epoch 10/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.9074 - acc: 0.3970 - val_loss: 1.8640 - val_acc: 0.4117\n",
            "Epoch 11/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.8247 - acc: 0.4254 - val_loss: 1.7863 - val_acc: 0.4253\n",
            "Epoch 12/100\n",
            "42000/42000 [==============================] - 1s 36us/sample - loss: 1.7461 - acc: 0.4464 - val_loss: 1.7033 - val_acc: 0.4672\n",
            "Epoch 13/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.6788 - acc: 0.4669 - val_loss: 1.6271 - val_acc: 0.4860\n",
            "Epoch 14/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.6151 - acc: 0.4887 - val_loss: 1.6673 - val_acc: 0.4426\n",
            "Epoch 15/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.5632 - acc: 0.5065 - val_loss: 1.5023 - val_acc: 0.5419\n",
            "Epoch 16/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.5055 - acc: 0.5312 - val_loss: 1.4550 - val_acc: 0.5602\n",
            "Epoch 17/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.4557 - acc: 0.5487 - val_loss: 1.4694 - val_acc: 0.5396\n",
            "Epoch 18/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.4163 - acc: 0.5634 - val_loss: 1.3754 - val_acc: 0.5867\n",
            "Epoch 19/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3806 - acc: 0.5766 - val_loss: 1.3544 - val_acc: 0.5814\n",
            "Epoch 20/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3388 - acc: 0.5929 - val_loss: 1.3661 - val_acc: 0.5665\n",
            "Epoch 21/100\n",
            "42000/42000 [==============================] - 1s 36us/sample - loss: 1.3037 - acc: 0.6048 - val_loss: 1.2641 - val_acc: 0.6218\n",
            "Epoch 22/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.2712 - acc: 0.6168 - val_loss: 1.2447 - val_acc: 0.6239\n",
            "Epoch 23/100\n",
            "42000/42000 [==============================] - 1s 36us/sample - loss: 1.2475 - acc: 0.6222 - val_loss: 1.1982 - val_acc: 0.6481\n",
            "Epoch 24/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.2231 - acc: 0.6292 - val_loss: 1.1737 - val_acc: 0.6548\n",
            "Epoch 25/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.1897 - acc: 0.6423 - val_loss: 1.1865 - val_acc: 0.6379\n",
            "Epoch 26/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.1686 - acc: 0.6472 - val_loss: 1.1496 - val_acc: 0.6539\n",
            "Epoch 27/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.1403 - acc: 0.6568 - val_loss: 1.1074 - val_acc: 0.6696\n",
            "Epoch 28/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.1182 - acc: 0.6614 - val_loss: 1.0946 - val_acc: 0.6697\n",
            "Epoch 29/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 1.1002 - acc: 0.6697 - val_loss: 1.1338 - val_acc: 0.6492\n",
            "Epoch 30/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.0829 - acc: 0.6715 - val_loss: 1.0493 - val_acc: 0.6874\n",
            "Epoch 31/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.0650 - acc: 0.6785 - val_loss: 1.0319 - val_acc: 0.6941\n",
            "Epoch 32/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.0456 - acc: 0.6840 - val_loss: 1.0387 - val_acc: 0.6820\n",
            "Epoch 33/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.0282 - acc: 0.6882 - val_loss: 1.0167 - val_acc: 0.6975\n",
            "Epoch 34/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.0157 - acc: 0.6943 - val_loss: 1.0013 - val_acc: 0.7009\n",
            "Epoch 35/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.9993 - acc: 0.6974 - val_loss: 0.9835 - val_acc: 0.7017\n",
            "Epoch 36/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.9854 - acc: 0.7029 - val_loss: 0.9762 - val_acc: 0.7069\n",
            "Epoch 37/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.9699 - acc: 0.7089 - val_loss: 0.9501 - val_acc: 0.7146\n",
            "Epoch 38/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.9595 - acc: 0.7095 - val_loss: 0.9601 - val_acc: 0.7098\n",
            "Epoch 39/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.9467 - acc: 0.7146 - val_loss: 0.9147 - val_acc: 0.7300\n",
            "Epoch 40/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.9414 - acc: 0.7146 - val_loss: 0.9287 - val_acc: 0.7193\n",
            "Epoch 41/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.9232 - acc: 0.7215 - val_loss: 0.9044 - val_acc: 0.7283\n",
            "Epoch 42/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.9147 - acc: 0.7246 - val_loss: 0.9301 - val_acc: 0.7131\n",
            "Epoch 43/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.9051 - acc: 0.7267 - val_loss: 0.9101 - val_acc: 0.7236\n",
            "Epoch 44/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.8950 - acc: 0.7304 - val_loss: 0.8743 - val_acc: 0.7396\n",
            "Epoch 45/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.8819 - acc: 0.7341 - val_loss: 0.9134 - val_acc: 0.7196\n",
            "Epoch 46/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.8695 - acc: 0.7355 - val_loss: 0.8589 - val_acc: 0.7431\n",
            "Epoch 47/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.8645 - acc: 0.7384 - val_loss: 0.8986 - val_acc: 0.7271\n",
            "Epoch 48/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.8536 - acc: 0.7420 - val_loss: 0.8435 - val_acc: 0.7464\n",
            "Epoch 49/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.8476 - acc: 0.7444 - val_loss: 0.8481 - val_acc: 0.7441\n",
            "Epoch 50/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.8372 - acc: 0.7481 - val_loss: 0.8292 - val_acc: 0.7523\n",
            "Epoch 51/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.8332 - acc: 0.7479 - val_loss: 0.8224 - val_acc: 0.7542\n",
            "Epoch 52/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.8257 - acc: 0.7513 - val_loss: 0.8375 - val_acc: 0.7448\n",
            "Epoch 53/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.8190 - acc: 0.7531 - val_loss: 0.8462 - val_acc: 0.7441\n",
            "Epoch 54/100\n",
            "42000/42000 [==============================] - 2s 38us/sample - loss: 0.8078 - acc: 0.7561 - val_loss: 0.7974 - val_acc: 0.7604\n",
            "Epoch 55/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.8019 - acc: 0.7576 - val_loss: 0.8065 - val_acc: 0.7567\n",
            "Epoch 56/100\n",
            "42000/42000 [==============================] - 1s 36us/sample - loss: 0.7974 - acc: 0.7583 - val_loss: 0.8102 - val_acc: 0.7524\n",
            "Epoch 57/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.7907 - acc: 0.7584 - val_loss: 0.8346 - val_acc: 0.7445\n",
            "Epoch 58/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.7838 - acc: 0.7622 - val_loss: 0.7901 - val_acc: 0.7609\n",
            "Epoch 59/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.7754 - acc: 0.7655 - val_loss: 0.7732 - val_acc: 0.7673\n",
            "Epoch 60/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.7656 - acc: 0.7680 - val_loss: 0.7578 - val_acc: 0.7732\n",
            "Epoch 61/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.7630 - acc: 0.7689 - val_loss: 0.7438 - val_acc: 0.7788\n",
            "Epoch 62/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.7531 - acc: 0.7701 - val_loss: 0.7491 - val_acc: 0.7752\n",
            "Epoch 63/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.7487 - acc: 0.7730 - val_loss: 0.7342 - val_acc: 0.7807\n",
            "Epoch 64/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.7455 - acc: 0.7732 - val_loss: 0.8003 - val_acc: 0.7614\n",
            "Epoch 65/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.7409 - acc: 0.7770 - val_loss: 0.7806 - val_acc: 0.7641\n",
            "Epoch 66/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.7315 - acc: 0.7775 - val_loss: 0.7438 - val_acc: 0.7762\n",
            "Epoch 67/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.7295 - acc: 0.7794 - val_loss: 0.7174 - val_acc: 0.7878\n",
            "Epoch 68/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.7245 - acc: 0.7797 - val_loss: 0.7301 - val_acc: 0.7798\n",
            "Epoch 69/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.7181 - acc: 0.7828 - val_loss: 0.7280 - val_acc: 0.7807\n",
            "Epoch 70/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.7127 - acc: 0.7836 - val_loss: 0.7063 - val_acc: 0.7903\n",
            "Epoch 71/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.7074 - acc: 0.7841 - val_loss: 0.7415 - val_acc: 0.7769\n",
            "Epoch 72/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.7022 - acc: 0.7873 - val_loss: 0.7047 - val_acc: 0.7906\n",
            "Epoch 73/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.7002 - acc: 0.7868 - val_loss: 0.7204 - val_acc: 0.7825\n",
            "Epoch 74/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.6929 - acc: 0.7906 - val_loss: 0.7135 - val_acc: 0.7852\n",
            "Epoch 75/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6864 - acc: 0.7917 - val_loss: 0.7134 - val_acc: 0.7851\n",
            "Epoch 76/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.6824 - acc: 0.7939 - val_loss: 0.7167 - val_acc: 0.7855\n",
            "Epoch 77/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6745 - acc: 0.7964 - val_loss: 0.7154 - val_acc: 0.7851\n",
            "Epoch 78/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6744 - acc: 0.7954 - val_loss: 0.6742 - val_acc: 0.7997\n",
            "Epoch 79/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.6720 - acc: 0.7978 - val_loss: 0.7034 - val_acc: 0.7910\n",
            "Epoch 80/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6677 - acc: 0.7979 - val_loss: 0.6693 - val_acc: 0.8003\n",
            "Epoch 81/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.6615 - acc: 0.8001 - val_loss: 0.6585 - val_acc: 0.8037\n",
            "Epoch 82/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.6594 - acc: 0.7998 - val_loss: 0.6924 - val_acc: 0.7903\n",
            "Epoch 83/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.6528 - acc: 0.8021 - val_loss: 0.6805 - val_acc: 0.7943\n",
            "Epoch 84/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.6511 - acc: 0.8019 - val_loss: 0.6511 - val_acc: 0.8063\n",
            "Epoch 85/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.6454 - acc: 0.8053 - val_loss: 0.6857 - val_acc: 0.7926\n",
            "Epoch 86/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.6354 - acc: 0.8087 - val_loss: 0.6615 - val_acc: 0.8022\n",
            "Epoch 87/100\n",
            "42000/42000 [==============================] - 1s 36us/sample - loss: 0.6405 - acc: 0.8068 - val_loss: 0.6828 - val_acc: 0.7932\n",
            "Epoch 88/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.6322 - acc: 0.8088 - val_loss: 0.7059 - val_acc: 0.7843\n",
            "Epoch 89/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.6325 - acc: 0.8089 - val_loss: 0.6495 - val_acc: 0.8053\n",
            "Epoch 90/100\n",
            "42000/42000 [==============================] - 1s 36us/sample - loss: 0.6248 - acc: 0.8103 - val_loss: 0.6311 - val_acc: 0.8123\n",
            "Epoch 91/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6233 - acc: 0.8115 - val_loss: 0.7214 - val_acc: 0.7767\n",
            "Epoch 92/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.6218 - acc: 0.8116 - val_loss: 0.6365 - val_acc: 0.8086\n",
            "Epoch 93/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.6147 - acc: 0.8154 - val_loss: 0.6161 - val_acc: 0.8164\n",
            "Epoch 94/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.6120 - acc: 0.8143 - val_loss: 0.6909 - val_acc: 0.7889\n",
            "Epoch 95/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.6129 - acc: 0.8150 - val_loss: 0.6230 - val_acc: 0.8136\n",
            "Epoch 96/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.6056 - acc: 0.8176 - val_loss: 0.6372 - val_acc: 0.8074\n",
            "Epoch 97/100\n",
            "42000/42000 [==============================] - 1s 36us/sample - loss: 0.6025 - acc: 0.8194 - val_loss: 0.6502 - val_acc: 0.8031\n",
            "Epoch 98/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.6000 - acc: 0.8194 - val_loss: 0.6267 - val_acc: 0.8099\n",
            "Epoch 99/100\n",
            "42000/42000 [==============================] - 1s 34us/sample - loss: 0.5923 - acc: 0.8203 - val_loss: 0.6251 - val_acc: 0.8139\n",
            "Epoch 100/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.5899 - acc: 0.8230 - val_loss: 0.6461 - val_acc: 0.8047\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4nU-fCy5KbwM",
        "outputId": "7a1728af-cf85-42b1-c32d-5cd755cc0c43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "print('Evaluate NN model with relu activations and changing the number of activators'); print('--'*40)\n",
        "results3 = model3.evaluate(X_val, y_val)\n",
        "print('Validation accuracy: {}'.format(round(results3[1]*100, 2), '%'))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluate NN model with relu activations and changing the number of activators\n",
            "--------------------------------------------------------------------------------\n",
            "60000/60000 [==============================] - 4s 68us/sample - loss: 0.6461 - acc: 0.8047\n",
            "Validation accuracy: 80.47\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dCjOofsRTN1K"
      },
      "source": [
        "#### NN model, relu activations, changing number of activators, Adam optimizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DhnK4y6VTQAC",
        "outputId": "56257ffc-403f-4ccf-8c81-eeb250555402",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# compiling the neural network classifier, adam optimizer\n",
        "adam = optimizers.Adam(lr = 0.001)\n",
        "model3.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# Fitting the neural network for training\n",
        "history = model3.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 42000 samples, validate on 60000 samples\n",
            "Epoch 1/100\n",
            "42000/42000 [==============================] - 2s 40us/sample - loss: 0.8560 - acc: 0.7366 - val_loss: 0.7600 - val_acc: 0.7680\n",
            "Epoch 2/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.7566 - acc: 0.7669 - val_loss: 0.7506 - val_acc: 0.7710\n",
            "Epoch 3/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.7321 - acc: 0.7747 - val_loss: 0.7005 - val_acc: 0.7874\n",
            "Epoch 4/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.7160 - acc: 0.7818 - val_loss: 0.7662 - val_acc: 0.7647\n",
            "Epoch 5/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.7061 - acc: 0.7847 - val_loss: 0.6672 - val_acc: 0.7981\n",
            "Epoch 6/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.6960 - acc: 0.7849 - val_loss: 0.7240 - val_acc: 0.7784\n",
            "Epoch 7/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.6779 - acc: 0.7921 - val_loss: 0.7075 - val_acc: 0.7831\n",
            "Epoch 8/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.6691 - acc: 0.7937 - val_loss: 0.6973 - val_acc: 0.7841\n",
            "Epoch 9/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.6502 - acc: 0.8010 - val_loss: 0.6480 - val_acc: 0.8041\n",
            "Epoch 10/100\n",
            "42000/42000 [==============================] - 1s 36us/sample - loss: 0.6337 - acc: 0.8062 - val_loss: 0.6482 - val_acc: 0.8043\n",
            "Epoch 11/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.6166 - acc: 0.8131 - val_loss: 0.6188 - val_acc: 0.8135\n",
            "Epoch 12/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.6207 - acc: 0.8089 - val_loss: 0.6153 - val_acc: 0.8172\n",
            "Epoch 13/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.6043 - acc: 0.8131 - val_loss: 0.6500 - val_acc: 0.7992\n",
            "Epoch 14/100\n",
            "42000/42000 [==============================] - 1s 36us/sample - loss: 0.6020 - acc: 0.8143 - val_loss: 0.7279 - val_acc: 0.7765\n",
            "Epoch 15/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.5864 - acc: 0.8210 - val_loss: 0.6229 - val_acc: 0.8104\n",
            "Epoch 16/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.5783 - acc: 0.8212 - val_loss: 0.5822 - val_acc: 0.8256\n",
            "Epoch 17/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.5544 - acc: 0.8289 - val_loss: 0.5754 - val_acc: 0.8290\n",
            "Epoch 18/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.5682 - acc: 0.8262 - val_loss: 0.5458 - val_acc: 0.8370\n",
            "Epoch 19/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.5423 - acc: 0.8330 - val_loss: 0.5254 - val_acc: 0.8439\n",
            "Epoch 20/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.5376 - acc: 0.8354 - val_loss: 0.5557 - val_acc: 0.8342\n",
            "Epoch 21/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.5254 - acc: 0.8398 - val_loss: 0.5603 - val_acc: 0.8298\n",
            "Epoch 22/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.5243 - acc: 0.8388 - val_loss: 0.5422 - val_acc: 0.8369\n",
            "Epoch 23/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.5160 - acc: 0.8417 - val_loss: 0.5439 - val_acc: 0.8375\n",
            "Epoch 24/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.4969 - acc: 0.8457 - val_loss: 0.6135 - val_acc: 0.8110\n",
            "Epoch 25/100\n",
            "42000/42000 [==============================] - 1s 36us/sample - loss: 0.4971 - acc: 0.8466 - val_loss: 0.5487 - val_acc: 0.8338\n",
            "Epoch 26/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.4916 - acc: 0.8487 - val_loss: 0.6003 - val_acc: 0.8166\n",
            "Epoch 27/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.4868 - acc: 0.8494 - val_loss: 0.5156 - val_acc: 0.8446\n",
            "Epoch 28/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.4864 - acc: 0.8491 - val_loss: 0.6135 - val_acc: 0.8138\n",
            "Epoch 29/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.4776 - acc: 0.8505 - val_loss: 0.5312 - val_acc: 0.8383\n",
            "Epoch 30/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.4702 - acc: 0.8525 - val_loss: 0.4893 - val_acc: 0.8542\n",
            "Epoch 31/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.4638 - acc: 0.8557 - val_loss: 0.5628 - val_acc: 0.8329\n",
            "Epoch 32/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.4635 - acc: 0.8540 - val_loss: 0.4987 - val_acc: 0.8498\n",
            "Epoch 33/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.4568 - acc: 0.8577 - val_loss: 0.4964 - val_acc: 0.8514\n",
            "Epoch 34/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.4492 - acc: 0.8608 - val_loss: 0.5004 - val_acc: 0.8500\n",
            "Epoch 35/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.4331 - acc: 0.8653 - val_loss: 0.4663 - val_acc: 0.8612\n",
            "Epoch 36/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.4441 - acc: 0.8605 - val_loss: 0.5042 - val_acc: 0.8452\n",
            "Epoch 37/100\n",
            "42000/42000 [==============================] - 1s 36us/sample - loss: 0.4306 - acc: 0.8649 - val_loss: 0.4601 - val_acc: 0.8626\n",
            "Epoch 38/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.4238 - acc: 0.8678 - val_loss: 0.4492 - val_acc: 0.8673\n",
            "Epoch 39/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.4209 - acc: 0.8687 - val_loss: 0.4386 - val_acc: 0.8694\n",
            "Epoch 40/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.4200 - acc: 0.8680 - val_loss: 0.5270 - val_acc: 0.8412\n",
            "Epoch 41/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.4136 - acc: 0.8704 - val_loss: 0.4692 - val_acc: 0.8581\n",
            "Epoch 42/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.4037 - acc: 0.8725 - val_loss: 0.5324 - val_acc: 0.8386\n",
            "Epoch 43/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.4136 - acc: 0.8685 - val_loss: 0.4254 - val_acc: 0.8738\n",
            "Epoch 44/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.4069 - acc: 0.8695 - val_loss: 0.4548 - val_acc: 0.8644\n",
            "Epoch 45/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.3889 - acc: 0.8760 - val_loss: 0.4943 - val_acc: 0.8503\n",
            "Epoch 46/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.3955 - acc: 0.8755 - val_loss: 0.4492 - val_acc: 0.8670\n",
            "Epoch 47/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.3772 - acc: 0.8803 - val_loss: 0.4727 - val_acc: 0.8576\n",
            "Epoch 48/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.3842 - acc: 0.8774 - val_loss: 0.4516 - val_acc: 0.8639\n",
            "Epoch 49/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.3690 - acc: 0.8829 - val_loss: 0.4341 - val_acc: 0.8712\n",
            "Epoch 50/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.3706 - acc: 0.8825 - val_loss: 0.4676 - val_acc: 0.8621\n",
            "Epoch 51/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.3784 - acc: 0.8794 - val_loss: 0.4190 - val_acc: 0.8765\n",
            "Epoch 52/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.3670 - acc: 0.8838 - val_loss: 0.4550 - val_acc: 0.8640\n",
            "Epoch 53/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.3562 - acc: 0.8872 - val_loss: 0.4819 - val_acc: 0.8533\n",
            "Epoch 54/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.3674 - acc: 0.8832 - val_loss: 0.4846 - val_acc: 0.8527\n",
            "Epoch 55/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.3572 - acc: 0.8846 - val_loss: 0.4362 - val_acc: 0.8702\n",
            "Epoch 56/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.3565 - acc: 0.8847 - val_loss: 0.4600 - val_acc: 0.8626\n",
            "Epoch 57/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.3491 - acc: 0.8896 - val_loss: 0.4006 - val_acc: 0.8826\n",
            "Epoch 58/100\n",
            "42000/42000 [==============================] - 1s 36us/sample - loss: 0.3557 - acc: 0.8857 - val_loss: 0.4267 - val_acc: 0.8742\n",
            "Epoch 59/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.3565 - acc: 0.8859 - val_loss: 0.4122 - val_acc: 0.8800\n",
            "Epoch 60/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.3335 - acc: 0.8939 - val_loss: 0.4032 - val_acc: 0.8838\n",
            "Epoch 61/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.3403 - acc: 0.8921 - val_loss: 0.3993 - val_acc: 0.8857\n",
            "Epoch 62/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.3270 - acc: 0.8948 - val_loss: 0.4020 - val_acc: 0.8828\n",
            "Epoch 63/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.3334 - acc: 0.8942 - val_loss: 0.4219 - val_acc: 0.8760\n",
            "Epoch 64/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.3125 - acc: 0.9000 - val_loss: 0.3825 - val_acc: 0.8907\n",
            "Epoch 65/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.3213 - acc: 0.8976 - val_loss: 0.4443 - val_acc: 0.8691\n",
            "Epoch 66/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.3410 - acc: 0.8895 - val_loss: 0.4317 - val_acc: 0.8741\n",
            "Epoch 67/100\n",
            "42000/42000 [==============================] - 1s 36us/sample - loss: 0.3236 - acc: 0.8954 - val_loss: 0.4230 - val_acc: 0.8784\n",
            "Epoch 68/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.3161 - acc: 0.8966 - val_loss: 0.3964 - val_acc: 0.8840\n",
            "Epoch 69/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.3112 - acc: 0.8997 - val_loss: 0.4235 - val_acc: 0.8764\n",
            "Epoch 70/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.3053 - acc: 0.9011 - val_loss: 0.3702 - val_acc: 0.8947\n",
            "Epoch 71/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.3129 - acc: 0.8990 - val_loss: 0.3975 - val_acc: 0.8844\n",
            "Epoch 72/100\n",
            "42000/42000 [==============================] - 1s 36us/sample - loss: 0.2969 - acc: 0.9040 - val_loss: 0.4597 - val_acc: 0.8644\n",
            "Epoch 73/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.2963 - acc: 0.9036 - val_loss: 0.4065 - val_acc: 0.8827\n",
            "Epoch 74/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.2966 - acc: 0.9037 - val_loss: 0.3928 - val_acc: 0.8896\n",
            "Epoch 75/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.2993 - acc: 0.9030 - val_loss: 0.4068 - val_acc: 0.8826\n",
            "Epoch 76/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.2809 - acc: 0.9094 - val_loss: 0.3733 - val_acc: 0.8928\n",
            "Epoch 77/100\n",
            "42000/42000 [==============================] - 1s 36us/sample - loss: 0.2856 - acc: 0.9076 - val_loss: 0.4177 - val_acc: 0.8802\n",
            "Epoch 78/100\n",
            "42000/42000 [==============================] - 1s 36us/sample - loss: 0.2835 - acc: 0.9092 - val_loss: 0.3845 - val_acc: 0.8904\n",
            "Epoch 79/100\n",
            "42000/42000 [==============================] - 1s 36us/sample - loss: 0.2847 - acc: 0.9072 - val_loss: 0.3826 - val_acc: 0.8921\n",
            "Epoch 80/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.2835 - acc: 0.9064 - val_loss: 0.3959 - val_acc: 0.8862\n",
            "Epoch 81/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.2872 - acc: 0.9047 - val_loss: 0.4433 - val_acc: 0.8705\n",
            "Epoch 82/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.2836 - acc: 0.9075 - val_loss: 0.3823 - val_acc: 0.8930\n",
            "Epoch 83/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.2651 - acc: 0.9137 - val_loss: 0.3860 - val_acc: 0.8918\n",
            "Epoch 84/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.2785 - acc: 0.9091 - val_loss: 0.3920 - val_acc: 0.8905\n",
            "Epoch 85/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.2683 - acc: 0.9114 - val_loss: 0.3569 - val_acc: 0.9026\n",
            "Epoch 86/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.2648 - acc: 0.9127 - val_loss: 0.4547 - val_acc: 0.8730\n",
            "Epoch 87/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.2858 - acc: 0.9063 - val_loss: 0.4077 - val_acc: 0.8845\n",
            "Epoch 88/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.2627 - acc: 0.9140 - val_loss: 0.4040 - val_acc: 0.8879\n",
            "Epoch 89/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.2744 - acc: 0.9098 - val_loss: 0.3628 - val_acc: 0.9004\n",
            "Epoch 90/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.2505 - acc: 0.9182 - val_loss: 0.3896 - val_acc: 0.8914\n",
            "Epoch 91/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.2641 - acc: 0.9135 - val_loss: 0.3993 - val_acc: 0.8901\n",
            "Epoch 92/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.2560 - acc: 0.9161 - val_loss: 0.3912 - val_acc: 0.8911\n",
            "Epoch 93/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.2603 - acc: 0.9141 - val_loss: 0.3710 - val_acc: 0.9015\n",
            "Epoch 94/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.2552 - acc: 0.9171 - val_loss: 0.3726 - val_acc: 0.8977\n",
            "Epoch 95/100\n",
            "42000/42000 [==============================] - 1s 36us/sample - loss: 0.2525 - acc: 0.9170 - val_loss: 0.3666 - val_acc: 0.8994\n",
            "Epoch 96/100\n",
            "42000/42000 [==============================] - 1s 36us/sample - loss: 0.2347 - acc: 0.9229 - val_loss: 0.3523 - val_acc: 0.9050\n",
            "Epoch 97/100\n",
            "42000/42000 [==============================] - 1s 36us/sample - loss: 0.2421 - acc: 0.9207 - val_loss: 0.4097 - val_acc: 0.8842\n",
            "Epoch 98/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.2455 - acc: 0.9190 - val_loss: 0.3973 - val_acc: 0.8890\n",
            "Epoch 99/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.2280 - acc: 0.9257 - val_loss: 0.3621 - val_acc: 0.9050\n",
            "Epoch 100/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.2210 - acc: 0.9280 - val_loss: 0.3567 - val_acc: 0.9074\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sHt5CH17TYgP",
        "outputId": "bc2a35ad-882d-4a07-9463-1b6529728193",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "print('Evaluate NN model with relu activations and changing the number of activators'); print('--'*40)\n",
        "results3 = model3.evaluate(X_val, y_val)\n",
        "print('Validation accuracy: {}'.format(round(results3[1]*100, 2), '%'))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluate NN model with relu activations and changing the number of activators\n",
            "--------------------------------------------------------------------------------\n",
            "60000/60000 [==============================] - 4s 69us/sample - loss: 0.3567 - acc: 0.9074\n",
            "Validation accuracy: 90.74\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xMH4JH0Aw1Xz"
      },
      "source": [
        "<a id='o5'></a>\n",
        "##### Observation 5 - NN model with relu activations and changing activators\n",
        "* Adding relu activations and changing activators results in improvement of score.\n",
        "* Best accuracy achieved till now is using relu activations, changing number of activators and Adam optimizers with a learning rate of 0.001\n",
        "* Next, let's try adding weight initilization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kermiDSyKAPQ"
      },
      "source": [
        "<a id='Weight'></a>\n",
        "### With Weight Initializers\n",
        "Changing weight initialization scheme can significantly improve training of the model by preventing vanishing gradient problem up to some degree."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bdMa80vmVAfU"
      },
      "source": [
        "#### NN model, relu activations, SGD optimizers with weight initializers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jLn1T1HTE5ms",
        "outputId": "a02211f0-90d5-4f94-df73-6205e42546f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print('NN model with weight initializers'); print('--'*40)\n",
        "# Initialize the neural network classifier\n",
        "model4 = Sequential()\n",
        "\n",
        "# Input Layer - adding input layer and activation functions relu and weight initializer\n",
        "model4.add(Dense(256, input_shape = (1024, ), kernel_initializer = 'he_normal'))\n",
        "# Adding activation function\n",
        "model4.add(Activation('relu'))\n",
        "\n",
        "#Hidden Layer 1 - adding first hidden layer\n",
        "model4.add(Dense(128, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\n",
        "# Adding activation function\n",
        "model4.add(Activation('relu'))\n",
        "\n",
        "#Hidden Layer 2 - adding second hidden layer\n",
        "model4.add(Dense(64, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\n",
        "# Adding activation function\n",
        "model4.add(Activation('relu'))\n",
        "\n",
        "#Hidden Layer 3 - adding third hidden layer\n",
        "model4.add(Dense(32, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\n",
        "# Adding activation function\n",
        "model4.add(Activation('relu'))\n",
        "\n",
        "# Output Layer - adding output layer which is of 10 nodes (digits)\n",
        "model4.add(Dense(10, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\n",
        "# Adding activation function\n",
        "model4.add(Activation('softmax'))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NN model with weight initializers\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "j4Ll9EKlU0Ep",
        "outputId": "795e0c32-0439-49bf-8c55-68662796d408",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        }
      },
      "source": [
        "model4.summary()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_10 (Dense)             (None, 256)               262400    \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 10)                330       \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 305,962\n",
            "Trainable params: 305,962\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W8Np2U7LU2vN",
        "outputId": "864a3475-ad2a-4320-c8f5-28b5459d1334",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# compiling the neural network classifier, sgd optimizer\n",
        "sgd = optimizers.SGD(lr = 0.01)\n",
        "# Adding activation function - softmax for multiclass classification\n",
        "model4.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# Fitting the neural network for training\n",
        "history = model4.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 42000 samples, validate on 60000 samples\n",
            "Epoch 1/100\n",
            "42000/42000 [==============================] - 2s 40us/sample - loss: 2.3032 - acc: 0.1220 - val_loss: 2.2766 - val_acc: 0.1479\n",
            "Epoch 2/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 2.2593 - acc: 0.1564 - val_loss: 2.2479 - val_acc: 0.1493\n",
            "Epoch 3/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 2.2025 - acc: 0.2008 - val_loss: 2.1706 - val_acc: 0.1955\n",
            "Epoch 4/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 2.1193 - acc: 0.2475 - val_loss: 2.0668 - val_acc: 0.2684\n",
            "Epoch 5/100\n",
            "42000/42000 [==============================] - 1s 36us/sample - loss: 2.0142 - acc: 0.3005 - val_loss: 1.9387 - val_acc: 0.3428\n",
            "Epoch 6/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.9019 - acc: 0.3520 - val_loss: 1.8582 - val_acc: 0.3440\n",
            "Epoch 7/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.8125 - acc: 0.3945 - val_loss: 1.7317 - val_acc: 0.4288\n",
            "Epoch 8/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.7157 - acc: 0.4343 - val_loss: 1.6669 - val_acc: 0.4636\n",
            "Epoch 9/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.6360 - acc: 0.4695 - val_loss: 1.5768 - val_acc: 0.5022\n",
            "Epoch 10/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 1.5762 - acc: 0.4897 - val_loss: 1.4754 - val_acc: 0.5413\n",
            "Epoch 11/100\n",
            "42000/42000 [==============================] - 1s 36us/sample - loss: 1.4939 - acc: 0.5247 - val_loss: 1.4534 - val_acc: 0.5402\n",
            "Epoch 12/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.4341 - acc: 0.5463 - val_loss: 1.3619 - val_acc: 0.5843\n",
            "Epoch 13/100\n",
            "42000/42000 [==============================] - 1s 36us/sample - loss: 1.3745 - acc: 0.5719 - val_loss: 1.3150 - val_acc: 0.5903\n",
            "Epoch 14/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.3162 - acc: 0.5941 - val_loss: 1.2622 - val_acc: 0.6181\n",
            "Epoch 15/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.2681 - acc: 0.6081 - val_loss: 1.2847 - val_acc: 0.5886\n",
            "Epoch 16/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 1.2275 - acc: 0.6203 - val_loss: 1.2462 - val_acc: 0.6063\n",
            "Epoch 17/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 1.1877 - acc: 0.6327 - val_loss: 1.1345 - val_acc: 0.6596\n",
            "Epoch 18/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.1592 - acc: 0.6426 - val_loss: 1.1910 - val_acc: 0.6270\n",
            "Epoch 19/100\n",
            "42000/42000 [==============================] - 1s 36us/sample - loss: 1.1290 - acc: 0.6512 - val_loss: 1.1114 - val_acc: 0.6600\n",
            "Epoch 20/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.1009 - acc: 0.6619 - val_loss: 1.0585 - val_acc: 0.6792\n",
            "Epoch 21/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.0820 - acc: 0.6665 - val_loss: 1.0362 - val_acc: 0.6863\n",
            "Epoch 22/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.0543 - acc: 0.6751 - val_loss: 1.0628 - val_acc: 0.6706\n",
            "Epoch 23/100\n",
            "42000/42000 [==============================] - 1s 36us/sample - loss: 1.0326 - acc: 0.6839 - val_loss: 1.0669 - val_acc: 0.6677\n",
            "Epoch 24/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 1.0241 - acc: 0.6844 - val_loss: 1.0353 - val_acc: 0.6777\n",
            "Epoch 25/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 1.0044 - acc: 0.6922 - val_loss: 0.9648 - val_acc: 0.7077\n",
            "Epoch 26/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.9933 - acc: 0.6961 - val_loss: 0.9997 - val_acc: 0.6943\n",
            "Epoch 27/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.9744 - acc: 0.7009 - val_loss: 0.9504 - val_acc: 0.7114\n",
            "Epoch 28/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.9660 - acc: 0.7026 - val_loss: 0.9794 - val_acc: 0.6966\n",
            "Epoch 29/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.9457 - acc: 0.7105 - val_loss: 0.9575 - val_acc: 0.7041\n",
            "Epoch 30/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.9384 - acc: 0.7114 - val_loss: 0.9222 - val_acc: 0.7166\n",
            "Epoch 31/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.9236 - acc: 0.7179 - val_loss: 0.9512 - val_acc: 0.7035\n",
            "Epoch 32/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.9137 - acc: 0.7206 - val_loss: 0.8832 - val_acc: 0.7328\n",
            "Epoch 33/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.9031 - acc: 0.7236 - val_loss: 0.9688 - val_acc: 0.6965\n",
            "Epoch 34/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.8892 - acc: 0.7271 - val_loss: 0.9520 - val_acc: 0.7041\n",
            "Epoch 35/100\n",
            "42000/42000 [==============================] - 1s 36us/sample - loss: 0.8837 - acc: 0.7292 - val_loss: 0.8835 - val_acc: 0.7326\n",
            "Epoch 36/100\n",
            "42000/42000 [==============================] - 1s 36us/sample - loss: 0.8689 - acc: 0.7329 - val_loss: 0.8705 - val_acc: 0.7321\n",
            "Epoch 37/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.8572 - acc: 0.7359 - val_loss: 1.0164 - val_acc: 0.6725\n",
            "Epoch 38/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.8529 - acc: 0.7385 - val_loss: 0.8419 - val_acc: 0.7438\n",
            "Epoch 39/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.8395 - acc: 0.7423 - val_loss: 0.8501 - val_acc: 0.7389\n",
            "Epoch 40/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.8345 - acc: 0.7444 - val_loss: 0.8019 - val_acc: 0.7590\n",
            "Epoch 41/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.8250 - acc: 0.7452 - val_loss: 0.8045 - val_acc: 0.7569\n",
            "Epoch 42/100\n",
            "42000/42000 [==============================] - 1s 36us/sample - loss: 0.8176 - acc: 0.7497 - val_loss: 0.8065 - val_acc: 0.7556\n",
            "Epoch 43/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.8117 - acc: 0.7503 - val_loss: 0.9099 - val_acc: 0.7208\n",
            "Epoch 44/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.7960 - acc: 0.7557 - val_loss: 0.7841 - val_acc: 0.7607\n",
            "Epoch 45/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.7900 - acc: 0.7575 - val_loss: 0.7838 - val_acc: 0.7614\n",
            "Epoch 46/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.7802 - acc: 0.7621 - val_loss: 0.7608 - val_acc: 0.7709\n",
            "Epoch 47/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.7751 - acc: 0.7627 - val_loss: 0.7984 - val_acc: 0.7575\n",
            "Epoch 48/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.7642 - acc: 0.7668 - val_loss: 0.7878 - val_acc: 0.7601\n",
            "Epoch 49/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.7579 - acc: 0.7679 - val_loss: 0.7565 - val_acc: 0.7706\n",
            "Epoch 50/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.7528 - acc: 0.7679 - val_loss: 0.7611 - val_acc: 0.7688\n",
            "Epoch 51/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.7443 - acc: 0.7722 - val_loss: 0.7359 - val_acc: 0.7757\n",
            "Epoch 52/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.7372 - acc: 0.7736 - val_loss: 0.7190 - val_acc: 0.7837\n",
            "Epoch 53/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.7320 - acc: 0.7762 - val_loss: 0.7374 - val_acc: 0.7764\n",
            "Epoch 54/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.7254 - acc: 0.7769 - val_loss: 0.7159 - val_acc: 0.7857\n",
            "Epoch 55/100\n",
            "42000/42000 [==============================] - 2s 38us/sample - loss: 0.7192 - acc: 0.7800 - val_loss: 0.7076 - val_acc: 0.7864\n",
            "Epoch 56/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.7122 - acc: 0.7821 - val_loss: 0.7515 - val_acc: 0.7713\n",
            "Epoch 57/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.7093 - acc: 0.7838 - val_loss: 0.7156 - val_acc: 0.7832\n",
            "Epoch 58/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.7003 - acc: 0.7860 - val_loss: 0.6988 - val_acc: 0.7907\n",
            "Epoch 59/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.6942 - acc: 0.7887 - val_loss: 0.7272 - val_acc: 0.7778\n",
            "Epoch 60/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.6901 - acc: 0.7888 - val_loss: 0.7166 - val_acc: 0.7849\n",
            "Epoch 61/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.6813 - acc: 0.7926 - val_loss: 0.7136 - val_acc: 0.7841\n",
            "Epoch 62/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.6825 - acc: 0.7911 - val_loss: 0.7425 - val_acc: 0.7741\n",
            "Epoch 63/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.6738 - acc: 0.7936 - val_loss: 0.6845 - val_acc: 0.7939\n",
            "Epoch 64/100\n",
            "42000/42000 [==============================] - 1s 36us/sample - loss: 0.6641 - acc: 0.7972 - val_loss: 0.7320 - val_acc: 0.7749\n",
            "Epoch 65/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.6656 - acc: 0.7956 - val_loss: 0.6782 - val_acc: 0.7961\n",
            "Epoch 66/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.6604 - acc: 0.7990 - val_loss: 0.6616 - val_acc: 0.7996\n",
            "Epoch 67/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.6528 - acc: 0.7999 - val_loss: 0.6816 - val_acc: 0.7956\n",
            "Epoch 68/100\n",
            "42000/42000 [==============================] - 1s 36us/sample - loss: 0.6496 - acc: 0.8018 - val_loss: 0.6516 - val_acc: 0.8046\n",
            "Epoch 69/100\n",
            "42000/42000 [==============================] - 1s 36us/sample - loss: 0.6448 - acc: 0.8025 - val_loss: 0.6597 - val_acc: 0.8022\n",
            "Epoch 70/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.6392 - acc: 0.8043 - val_loss: 0.7368 - val_acc: 0.7720\n",
            "Epoch 71/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.6311 - acc: 0.8070 - val_loss: 0.6456 - val_acc: 0.8044\n",
            "Epoch 72/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.6278 - acc: 0.8075 - val_loss: 0.6400 - val_acc: 0.8091\n",
            "Epoch 73/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.6252 - acc: 0.8109 - val_loss: 0.6385 - val_acc: 0.8077\n",
            "Epoch 74/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.6221 - acc: 0.8115 - val_loss: 0.6566 - val_acc: 0.8035\n",
            "Epoch 75/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.6125 - acc: 0.8109 - val_loss: 0.6481 - val_acc: 0.8038\n",
            "Epoch 76/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.6125 - acc: 0.8136 - val_loss: 0.6320 - val_acc: 0.8103\n",
            "Epoch 77/100\n",
            "42000/42000 [==============================] - 1s 36us/sample - loss: 0.6084 - acc: 0.8144 - val_loss: 0.6190 - val_acc: 0.8138\n",
            "Epoch 78/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.6005 - acc: 0.8178 - val_loss: 0.6261 - val_acc: 0.8112\n",
            "Epoch 79/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.5990 - acc: 0.8172 - val_loss: 0.6469 - val_acc: 0.8031\n",
            "Epoch 80/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.5907 - acc: 0.8193 - val_loss: 0.6206 - val_acc: 0.8129\n",
            "Epoch 81/100\n",
            "42000/42000 [==============================] - 1s 36us/sample - loss: 0.5891 - acc: 0.8212 - val_loss: 0.6701 - val_acc: 0.7942\n",
            "Epoch 82/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.5901 - acc: 0.8200 - val_loss: 0.6616 - val_acc: 0.7951\n",
            "Epoch 83/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.5853 - acc: 0.8213 - val_loss: 0.6126 - val_acc: 0.8147\n",
            "Epoch 84/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.5735 - acc: 0.8249 - val_loss: 0.6234 - val_acc: 0.8120\n",
            "Epoch 85/100\n",
            "42000/42000 [==============================] - 1s 36us/sample - loss: 0.5769 - acc: 0.8230 - val_loss: 0.6273 - val_acc: 0.8094\n",
            "Epoch 86/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.5741 - acc: 0.8258 - val_loss: 0.6028 - val_acc: 0.8183\n",
            "Epoch 87/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.5646 - acc: 0.8280 - val_loss: 0.5802 - val_acc: 0.8270\n",
            "Epoch 88/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.5636 - acc: 0.8281 - val_loss: 0.5965 - val_acc: 0.8207\n",
            "Epoch 89/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.5570 - acc: 0.8305 - val_loss: 0.5827 - val_acc: 0.8250\n",
            "Epoch 90/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.5552 - acc: 0.8309 - val_loss: 0.5941 - val_acc: 0.8210\n",
            "Epoch 91/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.5504 - acc: 0.8318 - val_loss: 0.6291 - val_acc: 0.8088\n",
            "Epoch 92/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.5444 - acc: 0.8332 - val_loss: 0.6393 - val_acc: 0.8010\n",
            "Epoch 93/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.5438 - acc: 0.8340 - val_loss: 0.5697 - val_acc: 0.8288\n",
            "Epoch 94/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.5418 - acc: 0.8352 - val_loss: 0.6405 - val_acc: 0.8066\n",
            "Epoch 95/100\n",
            "42000/42000 [==============================] - 1s 35us/sample - loss: 0.5413 - acc: 0.8360 - val_loss: 0.5643 - val_acc: 0.8313\n",
            "Epoch 96/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.5337 - acc: 0.8376 - val_loss: 0.5483 - val_acc: 0.8382\n",
            "Epoch 97/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.5324 - acc: 0.8366 - val_loss: 0.5496 - val_acc: 0.8363\n",
            "Epoch 98/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.5245 - acc: 0.8416 - val_loss: 0.5745 - val_acc: 0.8271\n",
            "Epoch 99/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.5228 - acc: 0.8405 - val_loss: 0.5695 - val_acc: 0.8281\n",
            "Epoch 100/100\n",
            "42000/42000 [==============================] - 1s 36us/sample - loss: 0.5234 - acc: 0.8402 - val_loss: 0.5641 - val_acc: 0.8300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xPbRGkQ7NKyW",
        "outputId": "2114c95f-bc74-4a24-973c-6129752adc5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "print('NN with weight initializers'); print('--'*40)\n",
        "results4 = model4.evaluate(X_val, y_val)\n",
        "print('Validation accuracy: {}'.format(round(results4[1]*100, 2), '%'))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NN with weight initializers\n",
            "--------------------------------------------------------------------------------\n",
            "60000/60000 [==============================] - 4s 70us/sample - loss: 0.5641 - acc: 0.8300\n",
            "Validation accuracy: 83.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BSWZvLlSVgq5"
      },
      "source": [
        "#### NN model, relu activations, Adam optimizers with weight initializers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ysyBxxvgVjWk",
        "outputId": "4e73f187-24d0-4804-e589-97a797edf80d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# compiling the neural network classifier, adam optimizer\n",
        "adam = optimizers.Adam(lr = 0.001)\n",
        "# Adding activation function - softmax for multiclass classification\n",
        "model4.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# Fitting the neural network for training\n",
        "history = model4.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 42000 samples, validate on 60000 samples\n",
            "Epoch 1/100\n",
            "42000/42000 [==============================] - 2s 43us/sample - loss: 0.9805 - acc: 0.7098 - val_loss: 0.7760 - val_acc: 0.7625\n",
            "Epoch 2/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.7694 - acc: 0.7611 - val_loss: 0.7600 - val_acc: 0.7638\n",
            "Epoch 3/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.7597 - acc: 0.7637 - val_loss: 0.7708 - val_acc: 0.7621\n",
            "Epoch 4/100\n",
            "42000/42000 [==============================] - 2s 38us/sample - loss: 0.7100 - acc: 0.7803 - val_loss: 0.7272 - val_acc: 0.7761\n",
            "Epoch 5/100\n",
            "42000/42000 [==============================] - 2s 38us/sample - loss: 0.7006 - acc: 0.7841 - val_loss: 0.7313 - val_acc: 0.7795\n",
            "Epoch 6/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.6722 - acc: 0.7945 - val_loss: 0.6705 - val_acc: 0.7955\n",
            "Epoch 7/100\n",
            "42000/42000 [==============================] - 2s 38us/sample - loss: 0.6632 - acc: 0.7970 - val_loss: 0.6610 - val_acc: 0.7960\n",
            "Epoch 8/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.6605 - acc: 0.7966 - val_loss: 0.6784 - val_acc: 0.7908\n",
            "Epoch 9/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.6424 - acc: 0.8011 - val_loss: 0.7090 - val_acc: 0.7836\n",
            "Epoch 10/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.6424 - acc: 0.7998 - val_loss: 0.5825 - val_acc: 0.8252\n",
            "Epoch 11/100\n",
            "42000/42000 [==============================] - 2s 38us/sample - loss: 0.6094 - acc: 0.8136 - val_loss: 0.6558 - val_acc: 0.7954\n",
            "Epoch 12/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.5890 - acc: 0.8197 - val_loss: 0.5736 - val_acc: 0.8266\n",
            "Epoch 13/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.5885 - acc: 0.8178 - val_loss: 0.6498 - val_acc: 0.7999\n",
            "Epoch 14/100\n",
            "42000/42000 [==============================] - 2s 38us/sample - loss: 0.5969 - acc: 0.8142 - val_loss: 0.6711 - val_acc: 0.7962\n",
            "Epoch 15/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.5660 - acc: 0.8254 - val_loss: 0.5637 - val_acc: 0.8293\n",
            "Epoch 16/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.5683 - acc: 0.8230 - val_loss: 0.5797 - val_acc: 0.8222\n",
            "Epoch 17/100\n",
            "42000/42000 [==============================] - 2s 38us/sample - loss: 0.5577 - acc: 0.8263 - val_loss: 0.5637 - val_acc: 0.8308\n",
            "Epoch 18/100\n",
            "42000/42000 [==============================] - 2s 38us/sample - loss: 0.5404 - acc: 0.8332 - val_loss: 0.5412 - val_acc: 0.8362\n",
            "Epoch 19/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.5370 - acc: 0.8321 - val_loss: 0.5645 - val_acc: 0.8282\n",
            "Epoch 20/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.5207 - acc: 0.8369 - val_loss: 0.5529 - val_acc: 0.8309\n",
            "Epoch 21/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.5120 - acc: 0.8406 - val_loss: 0.5370 - val_acc: 0.8388\n",
            "Epoch 22/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.4975 - acc: 0.8459 - val_loss: 0.5308 - val_acc: 0.8393\n",
            "Epoch 23/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.5225 - acc: 0.8344 - val_loss: 0.5050 - val_acc: 0.8462\n",
            "Epoch 24/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.5005 - acc: 0.8443 - val_loss: 0.5352 - val_acc: 0.8349\n",
            "Epoch 25/100\n",
            "42000/42000 [==============================] - 2s 38us/sample - loss: 0.4827 - acc: 0.8491 - val_loss: 0.5216 - val_acc: 0.8418\n",
            "Epoch 26/100\n",
            "42000/42000 [==============================] - 2s 38us/sample - loss: 0.4738 - acc: 0.8534 - val_loss: 0.5172 - val_acc: 0.8457\n",
            "Epoch 27/100\n",
            "42000/42000 [==============================] - 2s 38us/sample - loss: 0.4823 - acc: 0.8502 - val_loss: 0.5290 - val_acc: 0.8391\n",
            "Epoch 28/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.4771 - acc: 0.8504 - val_loss: 0.5686 - val_acc: 0.8241\n",
            "Epoch 29/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.4662 - acc: 0.8545 - val_loss: 0.5139 - val_acc: 0.8438\n",
            "Epoch 30/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.4582 - acc: 0.8578 - val_loss: 0.5117 - val_acc: 0.8443\n",
            "Epoch 31/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.4641 - acc: 0.8549 - val_loss: 0.4912 - val_acc: 0.8506\n",
            "Epoch 32/100\n",
            "42000/42000 [==============================] - 2s 38us/sample - loss: 0.4515 - acc: 0.8585 - val_loss: 0.5024 - val_acc: 0.8479\n",
            "Epoch 33/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.4352 - acc: 0.8636 - val_loss: 0.4866 - val_acc: 0.8510\n",
            "Epoch 34/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.4421 - acc: 0.8598 - val_loss: 0.4964 - val_acc: 0.8478\n",
            "Epoch 35/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.4381 - acc: 0.8618 - val_loss: 0.4830 - val_acc: 0.8545\n",
            "Epoch 36/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.4375 - acc: 0.8621 - val_loss: 0.4619 - val_acc: 0.8598\n",
            "Epoch 37/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.4277 - acc: 0.8641 - val_loss: 0.4942 - val_acc: 0.8483\n",
            "Epoch 38/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.4097 - acc: 0.8704 - val_loss: 0.4586 - val_acc: 0.8624\n",
            "Epoch 39/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.4243 - acc: 0.8654 - val_loss: 0.4405 - val_acc: 0.8677\n",
            "Epoch 40/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.4140 - acc: 0.8692 - val_loss: 0.4683 - val_acc: 0.8575\n",
            "Epoch 41/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.4152 - acc: 0.8668 - val_loss: 0.4191 - val_acc: 0.8741\n",
            "Epoch 42/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.3964 - acc: 0.8748 - val_loss: 0.4686 - val_acc: 0.8596\n",
            "Epoch 43/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.3933 - acc: 0.8734 - val_loss: 0.4412 - val_acc: 0.8687\n",
            "Epoch 44/100\n",
            "42000/42000 [==============================] - 2s 38us/sample - loss: 0.4098 - acc: 0.8704 - val_loss: 0.5064 - val_acc: 0.8454\n",
            "Epoch 45/100\n",
            "42000/42000 [==============================] - 2s 38us/sample - loss: 0.4000 - acc: 0.8715 - val_loss: 0.4721 - val_acc: 0.8601\n",
            "Epoch 46/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.3715 - acc: 0.8818 - val_loss: 0.4284 - val_acc: 0.8729\n",
            "Epoch 47/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.3821 - acc: 0.8775 - val_loss: 0.4342 - val_acc: 0.8699\n",
            "Epoch 48/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.3829 - acc: 0.8766 - val_loss: 0.4411 - val_acc: 0.8671\n",
            "Epoch 49/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.3780 - acc: 0.8800 - val_loss: 0.4422 - val_acc: 0.8662\n",
            "Epoch 50/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.3782 - acc: 0.8790 - val_loss: 0.4281 - val_acc: 0.8712\n",
            "Epoch 51/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.3682 - acc: 0.8819 - val_loss: 0.4046 - val_acc: 0.8816\n",
            "Epoch 52/100\n",
            "42000/42000 [==============================] - 2s 38us/sample - loss: 0.3749 - acc: 0.8806 - val_loss: 0.4067 - val_acc: 0.8787\n",
            "Epoch 53/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.3539 - acc: 0.8869 - val_loss: 0.4040 - val_acc: 0.8800\n",
            "Epoch 54/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.3516 - acc: 0.8864 - val_loss: 0.4083 - val_acc: 0.8801\n",
            "Epoch 55/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.3535 - acc: 0.8874 - val_loss: 0.4342 - val_acc: 0.8716\n",
            "Epoch 56/100\n",
            "42000/42000 [==============================] - 2s 38us/sample - loss: 0.3434 - acc: 0.8906 - val_loss: 0.4524 - val_acc: 0.8647\n",
            "Epoch 57/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.3553 - acc: 0.8851 - val_loss: 0.4870 - val_acc: 0.8530\n",
            "Epoch 58/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.3438 - acc: 0.8884 - val_loss: 0.4562 - val_acc: 0.8639\n",
            "Epoch 59/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.3427 - acc: 0.8899 - val_loss: 0.4782 - val_acc: 0.8587\n",
            "Epoch 60/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.3379 - acc: 0.8910 - val_loss: 0.4492 - val_acc: 0.8669\n",
            "Epoch 61/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.3410 - acc: 0.8893 - val_loss: 0.4527 - val_acc: 0.8632\n",
            "Epoch 62/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.3497 - acc: 0.8874 - val_loss: 0.4173 - val_acc: 0.8763\n",
            "Epoch 63/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.3292 - acc: 0.8934 - val_loss: 0.4370 - val_acc: 0.8691\n",
            "Epoch 64/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.3230 - acc: 0.8956 - val_loss: 0.4512 - val_acc: 0.8664\n",
            "Epoch 65/100\n",
            "42000/42000 [==============================] - 2s 38us/sample - loss: 0.3304 - acc: 0.8918 - val_loss: 0.4039 - val_acc: 0.8824\n",
            "Epoch 66/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.3129 - acc: 0.8982 - val_loss: 0.4105 - val_acc: 0.8812\n",
            "Epoch 67/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.3170 - acc: 0.8973 - val_loss: 0.4499 - val_acc: 0.8660\n",
            "Epoch 68/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.3182 - acc: 0.8971 - val_loss: 0.3795 - val_acc: 0.8917\n",
            "Epoch 69/100\n",
            "42000/42000 [==============================] - 2s 38us/sample - loss: 0.3156 - acc: 0.8979 - val_loss: 0.4131 - val_acc: 0.8808\n",
            "Epoch 70/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.3100 - acc: 0.8998 - val_loss: 0.4329 - val_acc: 0.8744\n",
            "Epoch 71/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.3151 - acc: 0.8980 - val_loss: 0.3876 - val_acc: 0.8897\n",
            "Epoch 72/100\n",
            "42000/42000 [==============================] - 2s 38us/sample - loss: 0.3127 - acc: 0.8961 - val_loss: 0.4197 - val_acc: 0.8771\n",
            "Epoch 73/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.2998 - acc: 0.9030 - val_loss: 0.3864 - val_acc: 0.8868\n",
            "Epoch 74/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.3043 - acc: 0.8993 - val_loss: 0.3910 - val_acc: 0.8862\n",
            "Epoch 75/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.2955 - acc: 0.9034 - val_loss: 0.4085 - val_acc: 0.8819\n",
            "Epoch 76/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.2957 - acc: 0.9036 - val_loss: 0.3923 - val_acc: 0.8877\n",
            "Epoch 77/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.3058 - acc: 0.9006 - val_loss: 0.3672 - val_acc: 0.8973\n",
            "Epoch 78/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.2831 - acc: 0.9071 - val_loss: 0.4116 - val_acc: 0.8825\n",
            "Epoch 79/100\n",
            "42000/42000 [==============================] - 2s 38us/sample - loss: 0.2885 - acc: 0.9057 - val_loss: 0.3999 - val_acc: 0.8855\n",
            "Epoch 80/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.3035 - acc: 0.9000 - val_loss: 0.3684 - val_acc: 0.8970\n",
            "Epoch 81/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.2782 - acc: 0.9097 - val_loss: 0.4482 - val_acc: 0.8694\n",
            "Epoch 82/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.2841 - acc: 0.9070 - val_loss: 0.4195 - val_acc: 0.8804\n",
            "Epoch 83/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.2848 - acc: 0.9062 - val_loss: 0.3658 - val_acc: 0.8975\n",
            "Epoch 84/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.2701 - acc: 0.9115 - val_loss: 0.3790 - val_acc: 0.8939\n",
            "Epoch 85/100\n",
            "42000/42000 [==============================] - 2s 38us/sample - loss: 0.2699 - acc: 0.9111 - val_loss: 0.3817 - val_acc: 0.8939\n",
            "Epoch 86/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.2766 - acc: 0.9088 - val_loss: 0.3944 - val_acc: 0.8878\n",
            "Epoch 87/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.2716 - acc: 0.9109 - val_loss: 0.3775 - val_acc: 0.8963\n",
            "Epoch 88/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.2674 - acc: 0.9133 - val_loss: 0.3872 - val_acc: 0.8908\n",
            "Epoch 89/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.2607 - acc: 0.9160 - val_loss: 0.3959 - val_acc: 0.8890\n",
            "Epoch 90/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.2645 - acc: 0.9145 - val_loss: 0.3904 - val_acc: 0.8915\n",
            "Epoch 91/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.2680 - acc: 0.9132 - val_loss: 0.3808 - val_acc: 0.8946\n",
            "Epoch 92/100\n",
            "42000/42000 [==============================] - 2s 38us/sample - loss: 0.2761 - acc: 0.9088 - val_loss: 0.4326 - val_acc: 0.8786\n",
            "Epoch 93/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.2520 - acc: 0.9165 - val_loss: 0.3775 - val_acc: 0.8973\n",
            "Epoch 94/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.2516 - acc: 0.9176 - val_loss: 0.3683 - val_acc: 0.8985\n",
            "Epoch 95/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.2510 - acc: 0.9166 - val_loss: 0.4064 - val_acc: 0.8871\n",
            "Epoch 96/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.2508 - acc: 0.9173 - val_loss: 0.4152 - val_acc: 0.8857\n",
            "Epoch 97/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.2579 - acc: 0.9153 - val_loss: 0.3552 - val_acc: 0.9037\n",
            "Epoch 98/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.2482 - acc: 0.9188 - val_loss: 0.3660 - val_acc: 0.9001\n",
            "Epoch 99/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.2410 - acc: 0.9201 - val_loss: 0.3962 - val_acc: 0.8918\n",
            "Epoch 100/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.2491 - acc: 0.9179 - val_loss: 0.3784 - val_acc: 0.8960\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "95vBMvCvVrg0",
        "outputId": "4280c1ad-f443-42cf-a85f-773b4971f724",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "print('NN with weight initializers'); print('--'*40)\n",
        "results4 = model4.evaluate(X_val, y_val)\n",
        "print('Validation accuracy: {}'.format(round(results4[1]*100, 2), '%'))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NN with weight initializers\n",
            "--------------------------------------------------------------------------------\n",
            "60000/60000 [==============================] - 4s 69us/sample - loss: 0.3784 - acc: 0.8960\n",
            "Validation accuracy: 89.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gjy8FCIQz_s4"
      },
      "source": [
        "<a id='o6'></a>\n",
        "##### Observation 6 - Weight initializers\n",
        "* Adding weight initialiers didn't result in improvement of score.\n",
        "* relu activations, changing number of activators, Adam optimizers gives the best score out of the ones tried as of now.\n",
        "* Next, let's try batch normalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nlLYOhj_TNcf"
      },
      "source": [
        "<a id='Batch'></a>\n",
        "### Batch Normalization\n",
        "Batch Normalization, one of the methods to prevent the \"internal covariance shift\" problem, has proven to be highly effective. Normalize each mini-batch before nonlinearity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "D8QVHk2dWbgK"
      },
      "source": [
        "#### NN model, relu activations, SGD optimizers with weight initializers and batch normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BCATllTeUghF",
        "outputId": "fcfae444-61fa-448d-e29e-064cbdd92470",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print('NN model with batch normalization'); print('--'*40)\n",
        "# Initialize the neural network classifier\n",
        "model5 = Sequential()\n",
        "\n",
        "# Input Layer - adding input layer and activation functions relu and weight initializer\n",
        "model5.add(Dense(256, input_shape = (1024, ), kernel_initializer = 'he_normal'))\n",
        "# Adding batch normalization\n",
        "model5.add(BatchNormalization())\n",
        "# Adding activation function\n",
        "model5.add(Activation('relu'))\n",
        "\n",
        "#Hidden Layer 1 - adding first hidden layer\n",
        "model5.add(Dense(128, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\n",
        "# Adding batch normalization\n",
        "model5.add(BatchNormalization())\n",
        "# Adding activation function\n",
        "model5.add(Activation('relu'))\n",
        "\n",
        "#Hidden Layer 2 - adding second hidden layer\n",
        "model5.add(Dense(64, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\n",
        "# Adding batch normalization\n",
        "model5.add(BatchNormalization())\n",
        "# Adding activation function\n",
        "model5.add(Activation('relu'))\n",
        "\n",
        "#Hidden Layer 3 - adding third hidden layer\n",
        "model5.add(Dense(32, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\n",
        "# Adding batch normalization\n",
        "model5.add(BatchNormalization())\n",
        "# Adding activation function\n",
        "model5.add(Activation('relu'))\n",
        "\n",
        "# Output Layer - adding output layer which is of 10 nodes (digits)\n",
        "model5.add(Dense(10, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\n",
        "# Adding activation function\n",
        "model5.add(Activation('softmax'))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NN model with batch normalization\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3Yz93UDo557M",
        "outputId": "faa28d6c-28c5-4ac8-d814-d04cc335fe56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        }
      },
      "source": [
        "model5.summary()"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_15 (Dense)             (None, 256)               262400    \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_18 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 10)                330       \n",
            "_________________________________________________________________\n",
            "activation_19 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 307,882\n",
            "Trainable params: 306,922\n",
            "Non-trainable params: 960\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lvXUI89PWUcr",
        "outputId": "6b0de816-f1b4-430a-ec5b-e2e7a4a11fa8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# compiling the neural network classifier, sgd optimizer\n",
        "sgd = optimizers.SGD(lr = 0.01)\n",
        "# Adding activation function - softmax for multiclass classification\n",
        "model5.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# Fitting the neural network for training\n",
        "history = model5.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 42000 samples, validate on 60000 samples\n",
            "Epoch 1/100\n",
            "42000/42000 [==============================] - 3s 77us/sample - loss: 2.3234 - acc: 0.1780 - val_loss: 2.2228 - val_acc: 0.1757\n",
            "Epoch 2/100\n",
            "42000/42000 [==============================] - 3s 63us/sample - loss: 1.8996 - acc: 0.3592 - val_loss: 1.8867 - val_acc: 0.3779\n",
            "Epoch 3/100\n",
            "42000/42000 [==============================] - 3s 61us/sample - loss: 1.6415 - acc: 0.4802 - val_loss: 1.6200 - val_acc: 0.4840\n",
            "Epoch 4/100\n",
            "42000/42000 [==============================] - 3s 62us/sample - loss: 1.4517 - acc: 0.5570 - val_loss: 1.4120 - val_acc: 0.5671\n",
            "Epoch 5/100\n",
            "42000/42000 [==============================] - 3s 63us/sample - loss: 1.3057 - acc: 0.6064 - val_loss: 1.3220 - val_acc: 0.5995\n",
            "Epoch 6/100\n",
            "42000/42000 [==============================] - 3s 61us/sample - loss: 1.1924 - acc: 0.6426 - val_loss: 1.2006 - val_acc: 0.6257\n",
            "Epoch 7/100\n",
            "42000/42000 [==============================] - 3s 64us/sample - loss: 1.1002 - acc: 0.6690 - val_loss: 1.1663 - val_acc: 0.6418\n",
            "Epoch 8/100\n",
            "42000/42000 [==============================] - 3s 63us/sample - loss: 1.0227 - acc: 0.6919 - val_loss: 1.1370 - val_acc: 0.6437\n",
            "Epoch 9/100\n",
            "42000/42000 [==============================] - 3s 64us/sample - loss: 0.9628 - acc: 0.7081 - val_loss: 1.0144 - val_acc: 0.6826\n",
            "Epoch 10/100\n",
            "42000/42000 [==============================] - 3s 64us/sample - loss: 0.9112 - acc: 0.7222 - val_loss: 1.0450 - val_acc: 0.6693\n",
            "Epoch 11/100\n",
            "42000/42000 [==============================] - 3s 62us/sample - loss: 0.8679 - acc: 0.7343 - val_loss: 1.0679 - val_acc: 0.6675\n",
            "Epoch 12/100\n",
            "42000/42000 [==============================] - 3s 63us/sample - loss: 0.8261 - acc: 0.7470 - val_loss: 0.9399 - val_acc: 0.7081\n",
            "Epoch 13/100\n",
            "42000/42000 [==============================] - 3s 64us/sample - loss: 0.7946 - acc: 0.7568 - val_loss: 0.9326 - val_acc: 0.7086\n",
            "Epoch 14/100\n",
            "42000/42000 [==============================] - 3s 62us/sample - loss: 0.7654 - acc: 0.7650 - val_loss: 0.9487 - val_acc: 0.6993\n",
            "Epoch 15/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.7357 - acc: 0.7742 - val_loss: 0.9913 - val_acc: 0.6836\n",
            "Epoch 16/100\n",
            "42000/42000 [==============================] - 3s 62us/sample - loss: 0.7094 - acc: 0.7811 - val_loss: 0.8588 - val_acc: 0.7287\n",
            "Epoch 17/100\n",
            "42000/42000 [==============================] - 3s 62us/sample - loss: 0.6904 - acc: 0.7859 - val_loss: 0.9032 - val_acc: 0.7131\n",
            "Epoch 18/100\n",
            "42000/42000 [==============================] - 3s 63us/sample - loss: 0.6683 - acc: 0.7940 - val_loss: 0.8685 - val_acc: 0.7243\n",
            "Epoch 19/100\n",
            "42000/42000 [==============================] - 3s 62us/sample - loss: 0.6470 - acc: 0.8007 - val_loss: 0.7315 - val_acc: 0.7746\n",
            "Epoch 20/100\n",
            "42000/42000 [==============================] - 3s 67us/sample - loss: 0.6282 - acc: 0.8050 - val_loss: 0.8133 - val_acc: 0.7434\n",
            "Epoch 21/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.6132 - acc: 0.8107 - val_loss: 0.7516 - val_acc: 0.7617\n",
            "Epoch 22/100\n",
            "42000/42000 [==============================] - 3s 63us/sample - loss: 0.5923 - acc: 0.8167 - val_loss: 0.7642 - val_acc: 0.7565\n",
            "Epoch 23/100\n",
            "42000/42000 [==============================] - 3s 63us/sample - loss: 0.5759 - acc: 0.8220 - val_loss: 0.7859 - val_acc: 0.7540\n",
            "Epoch 24/100\n",
            "42000/42000 [==============================] - 3s 63us/sample - loss: 0.5691 - acc: 0.8228 - val_loss: 0.6948 - val_acc: 0.7833\n",
            "Epoch 25/100\n",
            "42000/42000 [==============================] - 3s 64us/sample - loss: 0.5505 - acc: 0.8289 - val_loss: 0.7514 - val_acc: 0.7641\n",
            "Epoch 26/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.5414 - acc: 0.8317 - val_loss: 0.7297 - val_acc: 0.7681\n",
            "Epoch 27/100\n",
            "42000/42000 [==============================] - 3s 66us/sample - loss: 0.5269 - acc: 0.8368 - val_loss: 0.9060 - val_acc: 0.7217\n",
            "Epoch 28/100\n",
            "42000/42000 [==============================] - 3s 66us/sample - loss: 0.5183 - acc: 0.8390 - val_loss: 0.7717 - val_acc: 0.7580\n",
            "Epoch 29/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.5057 - acc: 0.8430 - val_loss: 0.8219 - val_acc: 0.7415\n",
            "Epoch 30/100\n",
            "42000/42000 [==============================] - 3s 63us/sample - loss: 0.4991 - acc: 0.8438 - val_loss: 0.9936 - val_acc: 0.7181\n",
            "Epoch 31/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.4840 - acc: 0.8500 - val_loss: 0.6378 - val_acc: 0.8000\n",
            "Epoch 32/100\n",
            "42000/42000 [==============================] - 3s 63us/sample - loss: 0.4767 - acc: 0.8523 - val_loss: 0.7094 - val_acc: 0.7746\n",
            "Epoch 33/100\n",
            "42000/42000 [==============================] - 3s 62us/sample - loss: 0.4684 - acc: 0.8557 - val_loss: 0.7660 - val_acc: 0.7534\n",
            "Epoch 34/100\n",
            "42000/42000 [==============================] - 3s 61us/sample - loss: 0.4619 - acc: 0.8551 - val_loss: 0.8836 - val_acc: 0.7265\n",
            "Epoch 35/100\n",
            "42000/42000 [==============================] - 3s 64us/sample - loss: 0.4498 - acc: 0.8602 - val_loss: 0.8428 - val_acc: 0.7469\n",
            "Epoch 36/100\n",
            "42000/42000 [==============================] - 3s 62us/sample - loss: 0.4396 - acc: 0.8635 - val_loss: 0.8254 - val_acc: 0.7418\n",
            "Epoch 37/100\n",
            "42000/42000 [==============================] - 3s 64us/sample - loss: 0.4356 - acc: 0.8630 - val_loss: 0.6705 - val_acc: 0.7897\n",
            "Epoch 38/100\n",
            "42000/42000 [==============================] - 3s 63us/sample - loss: 0.4254 - acc: 0.8667 - val_loss: 0.7170 - val_acc: 0.7798\n",
            "Epoch 39/100\n",
            "42000/42000 [==============================] - 3s 63us/sample - loss: 0.4188 - acc: 0.8697 - val_loss: 0.7011 - val_acc: 0.7768\n",
            "Epoch 40/100\n",
            "42000/42000 [==============================] - 3s 63us/sample - loss: 0.4114 - acc: 0.8721 - val_loss: 0.6808 - val_acc: 0.7901\n",
            "Epoch 41/100\n",
            "42000/42000 [==============================] - 3s 62us/sample - loss: 0.4033 - acc: 0.8749 - val_loss: 0.7064 - val_acc: 0.7816\n",
            "Epoch 42/100\n",
            "42000/42000 [==============================] - 3s 62us/sample - loss: 0.3926 - acc: 0.8781 - val_loss: 0.6125 - val_acc: 0.8108\n",
            "Epoch 43/100\n",
            "42000/42000 [==============================] - 3s 63us/sample - loss: 0.3918 - acc: 0.8793 - val_loss: 0.6624 - val_acc: 0.7938\n",
            "Epoch 44/100\n",
            "42000/42000 [==============================] - 3s 63us/sample - loss: 0.3824 - acc: 0.8813 - val_loss: 0.6474 - val_acc: 0.8004\n",
            "Epoch 45/100\n",
            "42000/42000 [==============================] - 3s 63us/sample - loss: 0.3811 - acc: 0.8813 - val_loss: 0.7913 - val_acc: 0.7560\n",
            "Epoch 46/100\n",
            "42000/42000 [==============================] - 3s 62us/sample - loss: 0.3667 - acc: 0.8868 - val_loss: 0.6802 - val_acc: 0.7897\n",
            "Epoch 47/100\n",
            "42000/42000 [==============================] - 3s 63us/sample - loss: 0.3594 - acc: 0.8892 - val_loss: 0.8482 - val_acc: 0.7442\n",
            "Epoch 48/100\n",
            "42000/42000 [==============================] - 3s 63us/sample - loss: 0.3571 - acc: 0.8895 - val_loss: 0.6355 - val_acc: 0.8004\n",
            "Epoch 49/100\n",
            "42000/42000 [==============================] - 3s 63us/sample - loss: 0.3500 - acc: 0.8911 - val_loss: 0.9108 - val_acc: 0.7391\n",
            "Epoch 50/100\n",
            "42000/42000 [==============================] - 3s 62us/sample - loss: 0.3474 - acc: 0.8924 - val_loss: 0.6688 - val_acc: 0.7942\n",
            "Epoch 51/100\n",
            "42000/42000 [==============================] - 3s 61us/sample - loss: 0.3383 - acc: 0.8962 - val_loss: 0.6044 - val_acc: 0.8142\n",
            "Epoch 52/100\n",
            "42000/42000 [==============================] - 3s 61us/sample - loss: 0.3361 - acc: 0.8979 - val_loss: 0.6482 - val_acc: 0.8070\n",
            "Epoch 53/100\n",
            "42000/42000 [==============================] - 3s 66us/sample - loss: 0.3297 - acc: 0.8979 - val_loss: 0.6855 - val_acc: 0.7899\n",
            "Epoch 54/100\n",
            "42000/42000 [==============================] - 3s 63us/sample - loss: 0.3273 - acc: 0.8981 - val_loss: 0.8575 - val_acc: 0.7533\n",
            "Epoch 55/100\n",
            "42000/42000 [==============================] - 3s 62us/sample - loss: 0.3246 - acc: 0.8995 - val_loss: 0.6156 - val_acc: 0.8069\n",
            "Epoch 56/100\n",
            "42000/42000 [==============================] - 3s 64us/sample - loss: 0.3145 - acc: 0.9026 - val_loss: 0.6679 - val_acc: 0.7993\n",
            "Epoch 57/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.3116 - acc: 0.9040 - val_loss: 0.7383 - val_acc: 0.7768\n",
            "Epoch 58/100\n",
            "42000/42000 [==============================] - 3s 62us/sample - loss: 0.3082 - acc: 0.9047 - val_loss: 0.6761 - val_acc: 0.7964\n",
            "Epoch 59/100\n",
            "42000/42000 [==============================] - 3s 63us/sample - loss: 0.3052 - acc: 0.9052 - val_loss: 0.5768 - val_acc: 0.8229\n",
            "Epoch 60/100\n",
            "42000/42000 [==============================] - 3s 63us/sample - loss: 0.2959 - acc: 0.9081 - val_loss: 0.5638 - val_acc: 0.8266\n",
            "Epoch 61/100\n",
            "42000/42000 [==============================] - 3s 66us/sample - loss: 0.2863 - acc: 0.9117 - val_loss: 0.8516 - val_acc: 0.7625\n",
            "Epoch 62/100\n",
            "42000/42000 [==============================] - 3s 63us/sample - loss: 0.2893 - acc: 0.9099 - val_loss: 0.7917 - val_acc: 0.7680\n",
            "Epoch 63/100\n",
            "42000/42000 [==============================] - 3s 62us/sample - loss: 0.2927 - acc: 0.9090 - val_loss: 0.7751 - val_acc: 0.7752\n",
            "Epoch 64/100\n",
            "42000/42000 [==============================] - 3s 62us/sample - loss: 0.2790 - acc: 0.9140 - val_loss: 0.6319 - val_acc: 0.8099\n",
            "Epoch 65/100\n",
            "42000/42000 [==============================] - 3s 63us/sample - loss: 0.2793 - acc: 0.9140 - val_loss: 0.6617 - val_acc: 0.8038\n",
            "Epoch 66/100\n",
            "42000/42000 [==============================] - 3s 63us/sample - loss: 0.2705 - acc: 0.9163 - val_loss: 0.6717 - val_acc: 0.8018\n",
            "Epoch 67/100\n",
            "42000/42000 [==============================] - 3s 62us/sample - loss: 0.2702 - acc: 0.9168 - val_loss: 0.6127 - val_acc: 0.8145\n",
            "Epoch 68/100\n",
            "42000/42000 [==============================] - 3s 62us/sample - loss: 0.2698 - acc: 0.9164 - val_loss: 0.5330 - val_acc: 0.8379\n",
            "Epoch 69/100\n",
            "42000/42000 [==============================] - 3s 62us/sample - loss: 0.2537 - acc: 0.9222 - val_loss: 0.6395 - val_acc: 0.8125\n",
            "Epoch 70/100\n",
            "42000/42000 [==============================] - 3s 62us/sample - loss: 0.2582 - acc: 0.9201 - val_loss: 0.7212 - val_acc: 0.7952\n",
            "Epoch 71/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.2591 - acc: 0.9203 - val_loss: 0.5817 - val_acc: 0.8296\n",
            "Epoch 72/100\n",
            "42000/42000 [==============================] - 3s 63us/sample - loss: 0.2504 - acc: 0.9227 - val_loss: 0.5859 - val_acc: 0.8225\n",
            "Epoch 73/100\n",
            "42000/42000 [==============================] - 3s 64us/sample - loss: 0.2512 - acc: 0.9222 - val_loss: 0.6042 - val_acc: 0.8188\n",
            "Epoch 74/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.2446 - acc: 0.9262 - val_loss: 0.8377 - val_acc: 0.7675\n",
            "Epoch 75/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.2449 - acc: 0.9245 - val_loss: 0.5830 - val_acc: 0.8258\n",
            "Epoch 76/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.2407 - acc: 0.9257 - val_loss: 0.6422 - val_acc: 0.8128\n",
            "Epoch 77/100\n",
            "42000/42000 [==============================] - 3s 63us/sample - loss: 0.2286 - acc: 0.9305 - val_loss: 0.5586 - val_acc: 0.8349\n",
            "Epoch 78/100\n",
            "42000/42000 [==============================] - 3s 61us/sample - loss: 0.2332 - acc: 0.9282 - val_loss: 0.5152 - val_acc: 0.8450\n",
            "Epoch 79/100\n",
            "42000/42000 [==============================] - 3s 63us/sample - loss: 0.2269 - acc: 0.9302 - val_loss: 0.5985 - val_acc: 0.8167\n",
            "Epoch 80/100\n",
            "42000/42000 [==============================] - 3s 64us/sample - loss: 0.2257 - acc: 0.9309 - val_loss: 0.5380 - val_acc: 0.8428\n",
            "Epoch 81/100\n",
            "42000/42000 [==============================] - 3s 64us/sample - loss: 0.2232 - acc: 0.9319 - val_loss: 0.7234 - val_acc: 0.8012\n",
            "Epoch 82/100\n",
            "42000/42000 [==============================] - 3s 63us/sample - loss: 0.2196 - acc: 0.9322 - val_loss: 0.4917 - val_acc: 0.8552\n",
            "Epoch 83/100\n",
            "42000/42000 [==============================] - 3s 64us/sample - loss: 0.2123 - acc: 0.9365 - val_loss: 0.5485 - val_acc: 0.8435\n",
            "Epoch 84/100\n",
            "42000/42000 [==============================] - 3s 62us/sample - loss: 0.2155 - acc: 0.9349 - val_loss: 0.6994 - val_acc: 0.8029\n",
            "Epoch 85/100\n",
            "42000/42000 [==============================] - 3s 62us/sample - loss: 0.2149 - acc: 0.9340 - val_loss: 0.6950 - val_acc: 0.8033\n",
            "Epoch 86/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.2095 - acc: 0.9349 - val_loss: 0.6448 - val_acc: 0.8078\n",
            "Epoch 87/100\n",
            "42000/42000 [==============================] - 3s 63us/sample - loss: 0.2099 - acc: 0.9349 - val_loss: 0.5351 - val_acc: 0.8445\n",
            "Epoch 88/100\n",
            "42000/42000 [==============================] - 3s 63us/sample - loss: 0.1993 - acc: 0.9384 - val_loss: 0.8796 - val_acc: 0.7639\n",
            "Epoch 89/100\n",
            "42000/42000 [==============================] - 3s 66us/sample - loss: 0.2020 - acc: 0.9378 - val_loss: 0.6963 - val_acc: 0.8056\n",
            "Epoch 90/100\n",
            "42000/42000 [==============================] - 3s 62us/sample - loss: 0.1983 - acc: 0.9388 - val_loss: 0.4969 - val_acc: 0.8547\n",
            "Epoch 91/100\n",
            "42000/42000 [==============================] - 3s 63us/sample - loss: 0.1945 - acc: 0.9405 - val_loss: 0.6388 - val_acc: 0.8138\n",
            "Epoch 92/100\n",
            "42000/42000 [==============================] - 3s 63us/sample - loss: 0.1937 - acc: 0.9418 - val_loss: 0.5592 - val_acc: 0.8413\n",
            "Epoch 93/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.1852 - acc: 0.9440 - val_loss: 0.6383 - val_acc: 0.8191\n",
            "Epoch 94/100\n",
            "42000/42000 [==============================] - 3s 63us/sample - loss: 0.1827 - acc: 0.9440 - val_loss: 0.7105 - val_acc: 0.8070\n",
            "Epoch 95/100\n",
            "42000/42000 [==============================] - 3s 64us/sample - loss: 0.1847 - acc: 0.9431 - val_loss: 0.5914 - val_acc: 0.8304\n",
            "Epoch 96/100\n",
            "42000/42000 [==============================] - 3s 63us/sample - loss: 0.1831 - acc: 0.9443 - val_loss: 0.6348 - val_acc: 0.8174\n",
            "Epoch 97/100\n",
            "42000/42000 [==============================] - 3s 63us/sample - loss: 0.1782 - acc: 0.9454 - val_loss: 0.5460 - val_acc: 0.8459\n",
            "Epoch 98/100\n",
            "42000/42000 [==============================] - 3s 64us/sample - loss: 0.1852 - acc: 0.9439 - val_loss: 0.9829 - val_acc: 0.7572\n",
            "Epoch 99/100\n",
            "42000/42000 [==============================] - 3s 64us/sample - loss: 0.1788 - acc: 0.9453 - val_loss: 0.6424 - val_acc: 0.8253\n",
            "Epoch 100/100\n",
            "42000/42000 [==============================] - 3s 62us/sample - loss: 0.1760 - acc: 0.9459 - val_loss: 0.8595 - val_acc: 0.7915\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8jMKdIGlU0eG",
        "outputId": "fe5c8cf7-a161-4d8e-ed56-042d893a8003",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "print('NN with batch normalization'); print('--'*40)\n",
        "results5 = model5.evaluate(X_val, y_val)\n",
        "print('Validation accuracy: {}'.format(round(results5[1]*100, 2), '%'))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NN with batch normalization\n",
            "--------------------------------------------------------------------------------\n",
            "60000/60000 [==============================] - 5s 88us/sample - loss: 0.8595 - acc: 0.7915\n",
            "Validation accuracy: 79.15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hvExZp5EXHFf"
      },
      "source": [
        "#### NN model, relu activations, Adam optimizers with weight initializers and batch normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rws18ePgW5HZ",
        "outputId": "bb6f39a4-b7b1-400a-acfa-4c6b5e77a8b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# compiling the neural network classifier, adam optimizer\n",
        "adam = optimizers.Adam(lr = 0.001)\n",
        "# Adding activation function - softmax for multiclass classification\n",
        "model5.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# Fitting the neural network for training\n",
        "history = model5.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 42000 samples, validate on 60000 samples\n",
            "Epoch 1/100\n",
            "42000/42000 [==============================] - 3s 79us/sample - loss: 0.7501 - acc: 0.7704 - val_loss: 2.4543 - val_acc: 0.4754\n",
            "Epoch 2/100\n",
            "42000/42000 [==============================] - 3s 66us/sample - loss: 0.5667 - acc: 0.8168 - val_loss: 1.4869 - val_acc: 0.5458\n",
            "Epoch 3/100\n",
            "42000/42000 [==============================] - 3s 66us/sample - loss: 0.5155 - acc: 0.8313 - val_loss: 1.9794 - val_acc: 0.4523\n",
            "Epoch 4/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.4891 - acc: 0.8413 - val_loss: 1.3766 - val_acc: 0.6015\n",
            "Epoch 5/100\n",
            "42000/42000 [==============================] - 3s 66us/sample - loss: 0.4514 - acc: 0.8530 - val_loss: 1.5246 - val_acc: 0.5731\n",
            "Epoch 6/100\n",
            "42000/42000 [==============================] - 3s 66us/sample - loss: 0.4250 - acc: 0.8605 - val_loss: 1.8862 - val_acc: 0.5433\n",
            "Epoch 7/100\n",
            "42000/42000 [==============================] - 3s 64us/sample - loss: 0.4199 - acc: 0.8631 - val_loss: 1.4728 - val_acc: 0.5887\n",
            "Epoch 8/100\n",
            "42000/42000 [==============================] - 3s 66us/sample - loss: 0.3944 - acc: 0.8729 - val_loss: 1.3193 - val_acc: 0.6013\n",
            "Epoch 9/100\n",
            "42000/42000 [==============================] - 3s 63us/sample - loss: 0.3969 - acc: 0.8708 - val_loss: 1.6581 - val_acc: 0.5684\n",
            "Epoch 10/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.3705 - acc: 0.8792 - val_loss: 0.9797 - val_acc: 0.7071\n",
            "Epoch 11/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.3573 - acc: 0.8840 - val_loss: 1.8560 - val_acc: 0.5414\n",
            "Epoch 12/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.3576 - acc: 0.8844 - val_loss: 1.1642 - val_acc: 0.6559\n",
            "Epoch 13/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.3495 - acc: 0.8852 - val_loss: 1.3048 - val_acc: 0.6464\n",
            "Epoch 14/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.3260 - acc: 0.8936 - val_loss: 0.8004 - val_acc: 0.7573\n",
            "Epoch 15/100\n",
            "42000/42000 [==============================] - 3s 66us/sample - loss: 0.3177 - acc: 0.8965 - val_loss: 1.2515 - val_acc: 0.6508\n",
            "Epoch 16/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.2967 - acc: 0.9023 - val_loss: 1.2577 - val_acc: 0.6730\n",
            "Epoch 17/100\n",
            "42000/42000 [==============================] - 3s 64us/sample - loss: 0.2923 - acc: 0.9052 - val_loss: 1.1884 - val_acc: 0.6591\n",
            "Epoch 18/100\n",
            "42000/42000 [==============================] - 3s 64us/sample - loss: 0.2989 - acc: 0.9030 - val_loss: 1.2248 - val_acc: 0.6817\n",
            "Epoch 19/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.2869 - acc: 0.9067 - val_loss: 1.5443 - val_acc: 0.6074\n",
            "Epoch 20/100\n",
            "42000/42000 [==============================] - 3s 64us/sample - loss: 0.2771 - acc: 0.9094 - val_loss: 1.2172 - val_acc: 0.6152\n",
            "Epoch 21/100\n",
            "42000/42000 [==============================] - 3s 66us/sample - loss: 0.2657 - acc: 0.9141 - val_loss: 1.3088 - val_acc: 0.6470\n",
            "Epoch 22/100\n",
            "42000/42000 [==============================] - 3s 64us/sample - loss: 0.2657 - acc: 0.9134 - val_loss: 1.1061 - val_acc: 0.6729\n",
            "Epoch 23/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.2528 - acc: 0.9172 - val_loss: 1.0059 - val_acc: 0.7270\n",
            "Epoch 24/100\n",
            "42000/42000 [==============================] - 3s 63us/sample - loss: 0.2435 - acc: 0.9200 - val_loss: 0.9932 - val_acc: 0.7220\n",
            "Epoch 25/100\n",
            "42000/42000 [==============================] - 3s 66us/sample - loss: 0.2334 - acc: 0.9243 - val_loss: 1.2261 - val_acc: 0.6726\n",
            "Epoch 26/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.2398 - acc: 0.9204 - val_loss: 1.3507 - val_acc: 0.6767\n",
            "Epoch 27/100\n",
            "42000/42000 [==============================] - 3s 64us/sample - loss: 0.2287 - acc: 0.9238 - val_loss: 0.8911 - val_acc: 0.7574\n",
            "Epoch 28/100\n",
            "42000/42000 [==============================] - 3s 66us/sample - loss: 0.2320 - acc: 0.9224 - val_loss: 0.9818 - val_acc: 0.7198\n",
            "Epoch 29/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.2243 - acc: 0.9253 - val_loss: 1.8397 - val_acc: 0.5835\n",
            "Epoch 30/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.2227 - acc: 0.9264 - val_loss: 1.2558 - val_acc: 0.6849\n",
            "Epoch 31/100\n",
            "42000/42000 [==============================] - 3s 67us/sample - loss: 0.2102 - acc: 0.9306 - val_loss: 1.1127 - val_acc: 0.7275\n",
            "Epoch 32/100\n",
            "42000/42000 [==============================] - 3s 66us/sample - loss: 0.2053 - acc: 0.9311 - val_loss: 1.1266 - val_acc: 0.7268\n",
            "Epoch 33/100\n",
            "42000/42000 [==============================] - 3s 66us/sample - loss: 0.1960 - acc: 0.9360 - val_loss: 1.1528 - val_acc: 0.7200\n",
            "Epoch 34/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.1978 - acc: 0.9339 - val_loss: 0.6734 - val_acc: 0.7985\n",
            "Epoch 35/100\n",
            "42000/42000 [==============================] - 3s 66us/sample - loss: 0.1937 - acc: 0.9356 - val_loss: 1.1420 - val_acc: 0.7191\n",
            "Epoch 36/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.1893 - acc: 0.9381 - val_loss: 0.9390 - val_acc: 0.7623\n",
            "Epoch 37/100\n",
            "42000/42000 [==============================] - 3s 66us/sample - loss: 0.1820 - acc: 0.9400 - val_loss: 1.0614 - val_acc: 0.7217\n",
            "Epoch 38/100\n",
            "42000/42000 [==============================] - 3s 67us/sample - loss: 0.1946 - acc: 0.9355 - val_loss: 0.9769 - val_acc: 0.7559\n",
            "Epoch 39/100\n",
            "42000/42000 [==============================] - 3s 68us/sample - loss: 0.1784 - acc: 0.9402 - val_loss: 1.3603 - val_acc: 0.6765\n",
            "Epoch 40/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.1788 - acc: 0.9394 - val_loss: 1.0956 - val_acc: 0.7303\n",
            "Epoch 41/100\n",
            "42000/42000 [==============================] - 3s 68us/sample - loss: 0.1720 - acc: 0.9430 - val_loss: 0.9339 - val_acc: 0.7618\n",
            "Epoch 42/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.1734 - acc: 0.9424 - val_loss: 0.9732 - val_acc: 0.7532\n",
            "Epoch 43/100\n",
            "42000/42000 [==============================] - 3s 64us/sample - loss: 0.1648 - acc: 0.9455 - val_loss: 1.0509 - val_acc: 0.7418\n",
            "Epoch 44/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.1540 - acc: 0.9485 - val_loss: 1.2638 - val_acc: 0.7117\n",
            "Epoch 45/100\n",
            "42000/42000 [==============================] - 3s 66us/sample - loss: 0.1611 - acc: 0.9460 - val_loss: 1.5270 - val_acc: 0.6760\n",
            "Epoch 46/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.1548 - acc: 0.9480 - val_loss: 1.4450 - val_acc: 0.6741\n",
            "Epoch 47/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.1528 - acc: 0.9487 - val_loss: 0.9232 - val_acc: 0.7623\n",
            "Epoch 48/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.1467 - acc: 0.9525 - val_loss: 1.3228 - val_acc: 0.6948\n",
            "Epoch 49/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.1552 - acc: 0.9472 - val_loss: 1.0695 - val_acc: 0.7351\n",
            "Epoch 50/100\n",
            "42000/42000 [==============================] - 3s 66us/sample - loss: 0.1462 - acc: 0.9508 - val_loss: 0.8127 - val_acc: 0.8031\n",
            "Epoch 51/100\n",
            "42000/42000 [==============================] - 3s 66us/sample - loss: 0.1415 - acc: 0.9519 - val_loss: 1.5355 - val_acc: 0.6971\n",
            "Epoch 52/100\n",
            "42000/42000 [==============================] - 3s 67us/sample - loss: 0.1357 - acc: 0.9557 - val_loss: 0.9170 - val_acc: 0.7715\n",
            "Epoch 53/100\n",
            "42000/42000 [==============================] - 3s 64us/sample - loss: 0.1434 - acc: 0.9526 - val_loss: 0.7560 - val_acc: 0.8090\n",
            "Epoch 54/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.1398 - acc: 0.9524 - val_loss: 1.5943 - val_acc: 0.6957\n",
            "Epoch 55/100\n",
            "42000/42000 [==============================] - 3s 66us/sample - loss: 0.1346 - acc: 0.9547 - val_loss: 0.8926 - val_acc: 0.7776\n",
            "Epoch 56/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.1376 - acc: 0.9538 - val_loss: 1.4721 - val_acc: 0.7048\n",
            "Epoch 57/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.1198 - acc: 0.9600 - val_loss: 0.9479 - val_acc: 0.7727\n",
            "Epoch 58/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.1300 - acc: 0.9562 - val_loss: 0.9254 - val_acc: 0.7834\n",
            "Epoch 59/100\n",
            "42000/42000 [==============================] - 3s 64us/sample - loss: 0.1284 - acc: 0.9573 - val_loss: 0.7995 - val_acc: 0.8104\n",
            "Epoch 60/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.1216 - acc: 0.9596 - val_loss: 0.9438 - val_acc: 0.7658\n",
            "Epoch 61/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.1218 - acc: 0.9585 - val_loss: 0.6773 - val_acc: 0.8342\n",
            "Epoch 62/100\n",
            "42000/42000 [==============================] - 3s 66us/sample - loss: 0.1156 - acc: 0.9614 - val_loss: 0.8906 - val_acc: 0.7794\n",
            "Epoch 63/100\n",
            "42000/42000 [==============================] - 3s 66us/sample - loss: 0.1121 - acc: 0.9634 - val_loss: 2.0552 - val_acc: 0.6696\n",
            "Epoch 64/100\n",
            "42000/42000 [==============================] - 3s 66us/sample - loss: 0.1185 - acc: 0.9596 - val_loss: 1.0139 - val_acc: 0.7754\n",
            "Epoch 65/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.1160 - acc: 0.9606 - val_loss: 0.7695 - val_acc: 0.8166\n",
            "Epoch 66/100\n",
            "42000/42000 [==============================] - 3s 64us/sample - loss: 0.1103 - acc: 0.9624 - val_loss: 0.8604 - val_acc: 0.7983\n",
            "Epoch 67/100\n",
            "42000/42000 [==============================] - 3s 66us/sample - loss: 0.1139 - acc: 0.9619 - val_loss: 0.7182 - val_acc: 0.8271\n",
            "Epoch 68/100\n",
            "42000/42000 [==============================] - 3s 63us/sample - loss: 0.1179 - acc: 0.9603 - val_loss: 0.8480 - val_acc: 0.7884\n",
            "Epoch 69/100\n",
            "42000/42000 [==============================] - 3s 66us/sample - loss: 0.1142 - acc: 0.9609 - val_loss: 0.7106 - val_acc: 0.8338\n",
            "Epoch 70/100\n",
            "42000/42000 [==============================] - 3s 63us/sample - loss: 0.1030 - acc: 0.9653 - val_loss: 0.8789 - val_acc: 0.8041\n",
            "Epoch 71/100\n",
            "42000/42000 [==============================] - 3s 64us/sample - loss: 0.1067 - acc: 0.9650 - val_loss: 1.0059 - val_acc: 0.7801\n",
            "Epoch 72/100\n",
            "42000/42000 [==============================] - 3s 66us/sample - loss: 0.1034 - acc: 0.9654 - val_loss: 0.9165 - val_acc: 0.7782\n",
            "Epoch 73/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.1103 - acc: 0.9626 - val_loss: 1.3353 - val_acc: 0.7551\n",
            "Epoch 74/100\n",
            "42000/42000 [==============================] - 3s 64us/sample - loss: 0.0987 - acc: 0.9668 - val_loss: 0.8948 - val_acc: 0.8014\n",
            "Epoch 75/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.0961 - acc: 0.9679 - val_loss: 1.1170 - val_acc: 0.7694\n",
            "Epoch 76/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.0990 - acc: 0.9675 - val_loss: 0.8021 - val_acc: 0.8204\n",
            "Epoch 77/100\n",
            "42000/42000 [==============================] - 3s 64us/sample - loss: 0.0994 - acc: 0.9663 - val_loss: 1.7298 - val_acc: 0.7091\n",
            "Epoch 78/100\n",
            "42000/42000 [==============================] - 3s 64us/sample - loss: 0.1022 - acc: 0.9658 - val_loss: 1.4603 - val_acc: 0.7199\n",
            "Epoch 79/100\n",
            "42000/42000 [==============================] - 3s 64us/sample - loss: 0.0935 - acc: 0.9691 - val_loss: 0.8817 - val_acc: 0.7941\n",
            "Epoch 80/100\n",
            "42000/42000 [==============================] - 3s 64us/sample - loss: 0.0964 - acc: 0.9666 - val_loss: 0.8550 - val_acc: 0.8088\n",
            "Epoch 81/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.0938 - acc: 0.9681 - val_loss: 1.0485 - val_acc: 0.7884\n",
            "Epoch 82/100\n",
            "42000/42000 [==============================] - 3s 63us/sample - loss: 0.0877 - acc: 0.9698 - val_loss: 1.1834 - val_acc: 0.7602\n",
            "Epoch 83/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.0907 - acc: 0.9687 - val_loss: 0.8447 - val_acc: 0.8078\n",
            "Epoch 84/100\n",
            "42000/42000 [==============================] - 3s 66us/sample - loss: 0.0893 - acc: 0.9695 - val_loss: 0.8145 - val_acc: 0.8118\n",
            "Epoch 85/100\n",
            "42000/42000 [==============================] - 3s 64us/sample - loss: 0.0923 - acc: 0.9689 - val_loss: 0.8339 - val_acc: 0.8188\n",
            "Epoch 86/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.0831 - acc: 0.9718 - val_loss: 0.7384 - val_acc: 0.8298\n",
            "Epoch 87/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.0921 - acc: 0.9687 - val_loss: 1.3851 - val_acc: 0.7625\n",
            "Epoch 88/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.0889 - acc: 0.9707 - val_loss: 0.9632 - val_acc: 0.7982\n",
            "Epoch 89/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.0852 - acc: 0.9704 - val_loss: 0.7480 - val_acc: 0.8310\n",
            "Epoch 90/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.0865 - acc: 0.9716 - val_loss: 0.8332 - val_acc: 0.8228\n",
            "Epoch 91/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.0852 - acc: 0.9717 - val_loss: 1.5404 - val_acc: 0.7298\n",
            "Epoch 92/100\n",
            "42000/42000 [==============================] - 3s 64us/sample - loss: 0.0798 - acc: 0.9736 - val_loss: 1.0373 - val_acc: 0.7971\n",
            "Epoch 93/100\n",
            "42000/42000 [==============================] - 3s 64us/sample - loss: 0.0865 - acc: 0.9703 - val_loss: 0.8289 - val_acc: 0.8243\n",
            "Epoch 94/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.0749 - acc: 0.9748 - val_loss: 0.8238 - val_acc: 0.8243\n",
            "Epoch 95/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.0844 - acc: 0.9715 - val_loss: 1.0088 - val_acc: 0.7875\n",
            "Epoch 96/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.0789 - acc: 0.9729 - val_loss: 0.7807 - val_acc: 0.8207\n",
            "Epoch 97/100\n",
            "42000/42000 [==============================] - 3s 66us/sample - loss: 0.0747 - acc: 0.9741 - val_loss: 0.9923 - val_acc: 0.8049\n",
            "Epoch 98/100\n",
            "42000/42000 [==============================] - 3s 64us/sample - loss: 0.0815 - acc: 0.9724 - val_loss: 1.0684 - val_acc: 0.7853\n",
            "Epoch 99/100\n",
            "42000/42000 [==============================] - 3s 66us/sample - loss: 0.0803 - acc: 0.9730 - val_loss: 0.7141 - val_acc: 0.8439\n",
            "Epoch 100/100\n",
            "42000/42000 [==============================] - 3s 65us/sample - loss: 0.0752 - acc: 0.9744 - val_loss: 1.0514 - val_acc: 0.7913\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x612tS1vXAfv",
        "outputId": "91fb79e1-158e-48e6-c8c9-037c09a4b839",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "print('NN with batch normalization'); print('--'*40)\n",
        "results5 = model5.evaluate(X_val, y_val)\n",
        "print('Validation accuracy: {}'.format(round(results5[1]*100, 2), '%'))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NN with batch normalization\n",
            "--------------------------------------------------------------------------------\n",
            "60000/60000 [==============================] - 5s 89us/sample - loss: 1.0514 - acc: 0.7913\n",
            "Validation accuracy: 79.13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zlIflBua6Hie"
      },
      "source": [
        "<a id='o7'></a>\n",
        "##### Observation 7 - Batch Normalization\n",
        "* Batch normalization didn't result in improvement of score.\n",
        "* Relu activations, changing number of activators, Adam optimizers achieved the best score.\n",
        "* Next, let's try batch normalization with dropout."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "04uo_AzWTq5X"
      },
      "source": [
        "<a id='Dropout'></a>\n",
        "### Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "smSh8h6RZthm"
      },
      "source": [
        "#### NN model, relu activations, SGD optimizers with weight initializers,  batch normalization and dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xq5p41MtNvui",
        "outputId": "29a0f200-96f0-4523-dd50-e1dcf9f62fea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print('NN model with dropout - sgd optimizer'); print('--'*40)\n",
        "# Initialize the neural network classifier\n",
        "model6 = Sequential()\n",
        "# Input Layer - adding input layer and activation functions relu and weight initializer\n",
        "model6.add(Dense(512, input_shape = (1024, ), kernel_initializer = 'he_normal'))\n",
        "# Adding batch normalization\n",
        "model6.add(BatchNormalization()) \n",
        "# Adding activation function\n",
        "model6.add(Activation('relu'))\n",
        "# Adding dropout layer\n",
        "model6.add(Dropout(0.2))\n",
        "\n",
        "#Hidden Layer 1 - adding first hidden layer\n",
        "model6.add(Dense(256, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\n",
        "# Adding batch normalization\n",
        "model6.add(BatchNormalization())\n",
        "# Adding activation function\n",
        "model6.add(Activation('relu'))\n",
        "# Adding dropout layer\n",
        "model6.add(Dropout(0.2))\n",
        "\n",
        "#Hidden Layer 2 - adding second hidden layer\n",
        "model6.add(Dense(128, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\n",
        "# Adding batch normalization\n",
        "model6.add(BatchNormalization())\n",
        "# Adding activation function\n",
        "model6.add(Activation('relu'))\n",
        "# Adding dropout layer\n",
        "model6.add(Dropout(0.2))\n",
        "\n",
        "#Hidden Layer 3 - adding third hidden layer\n",
        "model6.add(Dense(64, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\n",
        "# Adding batch normalization\n",
        "model6.add(BatchNormalization())\n",
        "# Adding activation function\n",
        "model6.add(Activation('relu'))\n",
        "# Adding dropout layer\n",
        "model6.add(Dropout(0.2))\n",
        "\n",
        "#Hidden Layer 4 - adding fourth hidden layer\n",
        "model6.add(Dense(32, kernel_initializer = 'he_normal', bias_initializer = 'he_uniform'))\n",
        "# Adding batch normalization\n",
        "model6.add(BatchNormalization())\n",
        "# Adding activation function\n",
        "model6.add(Activation('relu'))\n",
        "# Adding dropout layer\n",
        "model6.add(Dropout(0.2))\n",
        "\n",
        "# Output Layer - adding output layer which is of 10 nodes (digits)\n",
        "model6.add(Dense(10, kernel_initializer = 'he_normal',bias_initializer = 'he_uniform'))\n",
        "# Adding activation function\n",
        "model6.add(Activation('softmax'))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NN model with dropout - sgd optimizer\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IV_EP0rf6n6A",
        "outputId": "d3b26f75-f249-4715-af7b-9dc5ca1d703f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 890
        }
      },
      "source": [
        "model6.summary()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_20 (Dense)             (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "activation_20 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "activation_21 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "activation_22 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 64)                256       \n",
            "_________________________________________________________________\n",
            "activation_23 (Activation)   (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 32)                128       \n",
            "_________________________________________________________________\n",
            "activation_24 (Activation)   (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 10)                330       \n",
            "_________________________________________________________________\n",
            "activation_25 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 703,658\n",
            "Trainable params: 701,674\n",
            "Non-trainable params: 1,984\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E9TPI3fIZGTI",
        "outputId": "5704422e-590d-49b2-fc58-2b4dd508e5da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# compiling the neural network classifier, sgd optimizer\n",
        "sgd = optimizers.SGD(lr = 0.01)\n",
        "model6.compile(optimizer = sgd, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# Adding activation function - softmax for multiclass classification\n",
        "history = model6.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 42000 samples, validate on 60000 samples\n",
            "Epoch 1/100\n",
            "42000/42000 [==============================] - 4s 90us/sample - loss: 2.6120 - acc: 0.1094 - val_loss: 2.3354 - val_acc: 0.1125\n",
            "Epoch 2/100\n",
            "42000/42000 [==============================] - 3s 77us/sample - loss: 2.4148 - acc: 0.1320 - val_loss: 2.2363 - val_acc: 0.1856\n",
            "Epoch 3/100\n",
            "42000/42000 [==============================] - 3s 75us/sample - loss: 2.3196 - acc: 0.1562 - val_loss: 2.1331 - val_acc: 0.2562\n",
            "Epoch 4/100\n",
            "42000/42000 [==============================] - 3s 73us/sample - loss: 2.2389 - acc: 0.1830 - val_loss: 2.0427 - val_acc: 0.3007\n",
            "Epoch 5/100\n",
            "42000/42000 [==============================] - 3s 73us/sample - loss: 2.1694 - acc: 0.2124 - val_loss: 1.9642 - val_acc: 0.3599\n",
            "Epoch 6/100\n",
            "42000/42000 [==============================] - 3s 76us/sample - loss: 2.0996 - acc: 0.2438 - val_loss: 1.8865 - val_acc: 0.3987\n",
            "Epoch 7/100\n",
            "42000/42000 [==============================] - 3s 73us/sample - loss: 2.0227 - acc: 0.2766 - val_loss: 1.8017 - val_acc: 0.4387\n",
            "Epoch 8/100\n",
            "42000/42000 [==============================] - 3s 75us/sample - loss: 1.9586 - acc: 0.3005 - val_loss: 1.7226 - val_acc: 0.4554\n",
            "Epoch 9/100\n",
            "42000/42000 [==============================] - 3s 75us/sample - loss: 1.8875 - acc: 0.3332 - val_loss: 1.6380 - val_acc: 0.4993\n",
            "Epoch 10/100\n",
            "42000/42000 [==============================] - 3s 76us/sample - loss: 1.8271 - acc: 0.3576 - val_loss: 1.5650 - val_acc: 0.5216\n",
            "Epoch 11/100\n",
            "42000/42000 [==============================] - 3s 73us/sample - loss: 1.7730 - acc: 0.3800 - val_loss: 1.4923 - val_acc: 0.5422\n",
            "Epoch 12/100\n",
            "42000/42000 [==============================] - 3s 72us/sample - loss: 1.7258 - acc: 0.3991 - val_loss: 1.4223 - val_acc: 0.5712\n",
            "Epoch 13/100\n",
            "42000/42000 [==============================] - 3s 72us/sample - loss: 1.6631 - acc: 0.4218 - val_loss: 1.3825 - val_acc: 0.5707\n",
            "Epoch 14/100\n",
            "42000/42000 [==============================] - 3s 72us/sample - loss: 1.6236 - acc: 0.4363 - val_loss: 1.3186 - val_acc: 0.5894\n",
            "Epoch 15/100\n",
            "42000/42000 [==============================] - 3s 74us/sample - loss: 1.5696 - acc: 0.4561 - val_loss: 1.2697 - val_acc: 0.6099\n",
            "Epoch 16/100\n",
            "42000/42000 [==============================] - 3s 73us/sample - loss: 1.5355 - acc: 0.4696 - val_loss: 1.2525 - val_acc: 0.6122\n",
            "Epoch 17/100\n",
            "42000/42000 [==============================] - 3s 74us/sample - loss: 1.4933 - acc: 0.4852 - val_loss: 1.2114 - val_acc: 0.6234\n",
            "Epoch 18/100\n",
            "42000/42000 [==============================] - 3s 74us/sample - loss: 1.4602 - acc: 0.4998 - val_loss: 1.1701 - val_acc: 0.6405\n",
            "Epoch 19/100\n",
            "42000/42000 [==============================] - 3s 74us/sample - loss: 1.4306 - acc: 0.5111 - val_loss: 1.1263 - val_acc: 0.6558\n",
            "Epoch 20/100\n",
            "42000/42000 [==============================] - 3s 73us/sample - loss: 1.4008 - acc: 0.5253 - val_loss: 1.1074 - val_acc: 0.6551\n",
            "Epoch 21/100\n",
            "42000/42000 [==============================] - 3s 73us/sample - loss: 1.3698 - acc: 0.5321 - val_loss: 1.0943 - val_acc: 0.6658\n",
            "Epoch 22/100\n",
            "42000/42000 [==============================] - 3s 74us/sample - loss: 1.3412 - acc: 0.5462 - val_loss: 1.0265 - val_acc: 0.6862\n",
            "Epoch 23/100\n",
            "42000/42000 [==============================] - 3s 73us/sample - loss: 1.3111 - acc: 0.5565 - val_loss: 1.0234 - val_acc: 0.6903\n",
            "Epoch 24/100\n",
            "42000/42000 [==============================] - 3s 72us/sample - loss: 1.2897 - acc: 0.5681 - val_loss: 1.0250 - val_acc: 0.6797\n",
            "Epoch 25/100\n",
            "42000/42000 [==============================] - 3s 72us/sample - loss: 1.2677 - acc: 0.5711 - val_loss: 0.9889 - val_acc: 0.6923\n",
            "Epoch 26/100\n",
            "42000/42000 [==============================] - 3s 76us/sample - loss: 1.2472 - acc: 0.5842 - val_loss: 0.9412 - val_acc: 0.7155\n",
            "Epoch 27/100\n",
            "42000/42000 [==============================] - 3s 74us/sample - loss: 1.2209 - acc: 0.5906 - val_loss: 0.9717 - val_acc: 0.7009\n",
            "Epoch 28/100\n",
            "42000/42000 [==============================] - 3s 74us/sample - loss: 1.2014 - acc: 0.5995 - val_loss: 0.9308 - val_acc: 0.7122\n",
            "Epoch 29/100\n",
            "42000/42000 [==============================] - 3s 73us/sample - loss: 1.1808 - acc: 0.6092 - val_loss: 0.8822 - val_acc: 0.7325\n",
            "Epoch 30/100\n",
            "42000/42000 [==============================] - 3s 75us/sample - loss: 1.1566 - acc: 0.6190 - val_loss: 0.9129 - val_acc: 0.7211\n",
            "Epoch 31/100\n",
            "42000/42000 [==============================] - 3s 75us/sample - loss: 1.1422 - acc: 0.6262 - val_loss: 0.8404 - val_acc: 0.7483\n",
            "Epoch 32/100\n",
            "42000/42000 [==============================] - 3s 74us/sample - loss: 1.1210 - acc: 0.6332 - val_loss: 0.8300 - val_acc: 0.7472\n",
            "Epoch 33/100\n",
            "42000/42000 [==============================] - 3s 72us/sample - loss: 1.1093 - acc: 0.6383 - val_loss: 0.8030 - val_acc: 0.7550\n",
            "Epoch 34/100\n",
            "42000/42000 [==============================] - 3s 75us/sample - loss: 1.0881 - acc: 0.6486 - val_loss: 0.8412 - val_acc: 0.7419\n",
            "Epoch 35/100\n",
            "42000/42000 [==============================] - 3s 76us/sample - loss: 1.0736 - acc: 0.6509 - val_loss: 0.7873 - val_acc: 0.7614\n",
            "Epoch 36/100\n",
            "42000/42000 [==============================] - 3s 75us/sample - loss: 1.0591 - acc: 0.6546 - val_loss: 0.7678 - val_acc: 0.7718\n",
            "Epoch 37/100\n",
            "42000/42000 [==============================] - 3s 75us/sample - loss: 1.0480 - acc: 0.6630 - val_loss: 0.7618 - val_acc: 0.7722\n",
            "Epoch 38/100\n",
            "42000/42000 [==============================] - 3s 73us/sample - loss: 1.0313 - acc: 0.6704 - val_loss: 0.8031 - val_acc: 0.7507\n",
            "Epoch 39/100\n",
            "42000/42000 [==============================] - 3s 74us/sample - loss: 1.0193 - acc: 0.6733 - val_loss: 0.7349 - val_acc: 0.7797\n",
            "Epoch 40/100\n",
            "42000/42000 [==============================] - 3s 73us/sample - loss: 0.9982 - acc: 0.6795 - val_loss: 0.7329 - val_acc: 0.7782\n",
            "Epoch 41/100\n",
            "42000/42000 [==============================] - 3s 75us/sample - loss: 0.9912 - acc: 0.6811 - val_loss: 0.7385 - val_acc: 0.7747\n",
            "Epoch 42/100\n",
            "42000/42000 [==============================] - 3s 74us/sample - loss: 0.9789 - acc: 0.6867 - val_loss: 0.6980 - val_acc: 0.7887\n",
            "Epoch 43/100\n",
            "42000/42000 [==============================] - 3s 75us/sample - loss: 0.9706 - acc: 0.6892 - val_loss: 0.7009 - val_acc: 0.7862\n",
            "Epoch 44/100\n",
            "42000/42000 [==============================] - 3s 79us/sample - loss: 0.9537 - acc: 0.6968 - val_loss: 0.6951 - val_acc: 0.7914\n",
            "Epoch 45/100\n",
            "42000/42000 [==============================] - 3s 75us/sample - loss: 0.9496 - acc: 0.7035 - val_loss: 0.7801 - val_acc: 0.7615\n",
            "Epoch 46/100\n",
            "42000/42000 [==============================] - 3s 76us/sample - loss: 0.9417 - acc: 0.7021 - val_loss: 0.6982 - val_acc: 0.7866\n",
            "Epoch 47/100\n",
            "42000/42000 [==============================] - 3s 74us/sample - loss: 0.9215 - acc: 0.7083 - val_loss: 0.6700 - val_acc: 0.7965\n",
            "Epoch 48/100\n",
            "42000/42000 [==============================] - 3s 72us/sample - loss: 0.9140 - acc: 0.7126 - val_loss: 0.6587 - val_acc: 0.8010\n",
            "Epoch 49/100\n",
            "42000/42000 [==============================] - 3s 74us/sample - loss: 0.9070 - acc: 0.7125 - val_loss: 0.6818 - val_acc: 0.7911\n",
            "Epoch 50/100\n",
            "42000/42000 [==============================] - 3s 73us/sample - loss: 0.8951 - acc: 0.7180 - val_loss: 0.7042 - val_acc: 0.7799\n",
            "Epoch 51/100\n",
            "42000/42000 [==============================] - 3s 75us/sample - loss: 0.8875 - acc: 0.7219 - val_loss: 0.6375 - val_acc: 0.8065\n",
            "Epoch 52/100\n",
            "42000/42000 [==============================] - 3s 74us/sample - loss: 0.8843 - acc: 0.7245 - val_loss: 0.6865 - val_acc: 0.7842\n",
            "Epoch 53/100\n",
            "42000/42000 [==============================] - 3s 73us/sample - loss: 0.8772 - acc: 0.7236 - val_loss: 0.7252 - val_acc: 0.7733\n",
            "Epoch 54/100\n",
            "42000/42000 [==============================] - 3s 72us/sample - loss: 0.8717 - acc: 0.7283 - val_loss: 0.6905 - val_acc: 0.7824\n",
            "Epoch 55/100\n",
            "42000/42000 [==============================] - 3s 76us/sample - loss: 0.8540 - acc: 0.7330 - val_loss: 0.6805 - val_acc: 0.7888\n",
            "Epoch 56/100\n",
            "42000/42000 [==============================] - 3s 75us/sample - loss: 0.8506 - acc: 0.7327 - val_loss: 0.7231 - val_acc: 0.7755\n",
            "Epoch 57/100\n",
            "42000/42000 [==============================] - 3s 72us/sample - loss: 0.8417 - acc: 0.7378 - val_loss: 0.7149 - val_acc: 0.7768\n",
            "Epoch 58/100\n",
            "42000/42000 [==============================] - 3s 75us/sample - loss: 0.8350 - acc: 0.7392 - val_loss: 0.6065 - val_acc: 0.8169\n",
            "Epoch 59/100\n",
            "42000/42000 [==============================] - 3s 74us/sample - loss: 0.8238 - acc: 0.7437 - val_loss: 0.6547 - val_acc: 0.7996\n",
            "Epoch 60/100\n",
            "42000/42000 [==============================] - 3s 75us/sample - loss: 0.8289 - acc: 0.7429 - val_loss: 0.7046 - val_acc: 0.7841\n",
            "Epoch 61/100\n",
            "42000/42000 [==============================] - 3s 73us/sample - loss: 0.8189 - acc: 0.7452 - val_loss: 0.6829 - val_acc: 0.7891\n",
            "Epoch 62/100\n",
            "42000/42000 [==============================] - 3s 75us/sample - loss: 0.8083 - acc: 0.7498 - val_loss: 0.5951 - val_acc: 0.8176\n",
            "Epoch 63/100\n",
            "42000/42000 [==============================] - 3s 73us/sample - loss: 0.8013 - acc: 0.7498 - val_loss: 0.7327 - val_acc: 0.7700\n",
            "Epoch 64/100\n",
            "42000/42000 [==============================] - 3s 74us/sample - loss: 0.7938 - acc: 0.7541 - val_loss: 0.8093 - val_acc: 0.7531\n",
            "Epoch 65/100\n",
            "42000/42000 [==============================] - 3s 73us/sample - loss: 0.7972 - acc: 0.7513 - val_loss: 0.5905 - val_acc: 0.8214\n",
            "Epoch 66/100\n",
            "42000/42000 [==============================] - 3s 75us/sample - loss: 0.7849 - acc: 0.7561 - val_loss: 0.5578 - val_acc: 0.8314\n",
            "Epoch 67/100\n",
            "42000/42000 [==============================] - 3s 74us/sample - loss: 0.7809 - acc: 0.7606 - val_loss: 0.5731 - val_acc: 0.8275\n",
            "Epoch 68/100\n",
            "42000/42000 [==============================] - 3s 75us/sample - loss: 0.7640 - acc: 0.7637 - val_loss: 0.5328 - val_acc: 0.8398\n",
            "Epoch 69/100\n",
            "42000/42000 [==============================] - 3s 74us/sample - loss: 0.7683 - acc: 0.7609 - val_loss: 0.6485 - val_acc: 0.7931\n",
            "Epoch 70/100\n",
            "42000/42000 [==============================] - 3s 73us/sample - loss: 0.7674 - acc: 0.7620 - val_loss: 0.5737 - val_acc: 0.8245\n",
            "Epoch 71/100\n",
            "42000/42000 [==============================] - 3s 72us/sample - loss: 0.7521 - acc: 0.7692 - val_loss: 0.5951 - val_acc: 0.8209\n",
            "Epoch 72/100\n",
            "42000/42000 [==============================] - 3s 75us/sample - loss: 0.7551 - acc: 0.7653 - val_loss: 0.5672 - val_acc: 0.8236\n",
            "Epoch 73/100\n",
            "42000/42000 [==============================] - 3s 73us/sample - loss: 0.7508 - acc: 0.7700 - val_loss: 0.5445 - val_acc: 0.8345\n",
            "Epoch 74/100\n",
            "42000/42000 [==============================] - 3s 72us/sample - loss: 0.7406 - acc: 0.7716 - val_loss: 0.5592 - val_acc: 0.8248\n",
            "Epoch 75/100\n",
            "42000/42000 [==============================] - 3s 74us/sample - loss: 0.7360 - acc: 0.7740 - val_loss: 0.5711 - val_acc: 0.8235\n",
            "Epoch 76/100\n",
            "42000/42000 [==============================] - 3s 73us/sample - loss: 0.7379 - acc: 0.7752 - val_loss: 0.6134 - val_acc: 0.8099\n",
            "Epoch 77/100\n",
            "42000/42000 [==============================] - 3s 74us/sample - loss: 0.7298 - acc: 0.7758 - val_loss: 0.6003 - val_acc: 0.8160\n",
            "Epoch 78/100\n",
            "42000/42000 [==============================] - 3s 73us/sample - loss: 0.7304 - acc: 0.7761 - val_loss: 0.5731 - val_acc: 0.8216\n",
            "Epoch 79/100\n",
            "42000/42000 [==============================] - 3s 73us/sample - loss: 0.7171 - acc: 0.7773 - val_loss: 0.5497 - val_acc: 0.8325\n",
            "Epoch 80/100\n",
            "42000/42000 [==============================] - 3s 74us/sample - loss: 0.7131 - acc: 0.7813 - val_loss: 0.5589 - val_acc: 0.8310\n",
            "Epoch 81/100\n",
            "42000/42000 [==============================] - 3s 73us/sample - loss: 0.7120 - acc: 0.7817 - val_loss: 0.5373 - val_acc: 0.8353\n",
            "Epoch 82/100\n",
            "42000/42000 [==============================] - 3s 74us/sample - loss: 0.7003 - acc: 0.7847 - val_loss: 0.5398 - val_acc: 0.8336\n",
            "Epoch 83/100\n",
            "42000/42000 [==============================] - 3s 75us/sample - loss: 0.7050 - acc: 0.7846 - val_loss: 0.6079 - val_acc: 0.8129\n",
            "Epoch 84/100\n",
            "42000/42000 [==============================] - 3s 73us/sample - loss: 0.7020 - acc: 0.7853 - val_loss: 0.6890 - val_acc: 0.7887\n",
            "Epoch 85/100\n",
            "42000/42000 [==============================] - 3s 74us/sample - loss: 0.7011 - acc: 0.7868 - val_loss: 0.5426 - val_acc: 0.8346\n",
            "Epoch 86/100\n",
            "42000/42000 [==============================] - 3s 74us/sample - loss: 0.6945 - acc: 0.7864 - val_loss: 0.5196 - val_acc: 0.8381\n",
            "Epoch 87/100\n",
            "42000/42000 [==============================] - 3s 74us/sample - loss: 0.6927 - acc: 0.7864 - val_loss: 0.4815 - val_acc: 0.8555\n",
            "Epoch 88/100\n",
            "42000/42000 [==============================] - 3s 74us/sample - loss: 0.6815 - acc: 0.7925 - val_loss: 0.4812 - val_acc: 0.8551\n",
            "Epoch 89/100\n",
            "42000/42000 [==============================] - 3s 74us/sample - loss: 0.6781 - acc: 0.7939 - val_loss: 0.5811 - val_acc: 0.8237\n",
            "Epoch 90/100\n",
            "42000/42000 [==============================] - 3s 74us/sample - loss: 0.6785 - acc: 0.7922 - val_loss: 0.5060 - val_acc: 0.8472\n",
            "Epoch 91/100\n",
            "42000/42000 [==============================] - 3s 73us/sample - loss: 0.6677 - acc: 0.7955 - val_loss: 0.5122 - val_acc: 0.8421\n",
            "Epoch 92/100\n",
            "42000/42000 [==============================] - 3s 73us/sample - loss: 0.6740 - acc: 0.7941 - val_loss: 0.4932 - val_acc: 0.8502\n",
            "Epoch 93/100\n",
            "42000/42000 [==============================] - 3s 74us/sample - loss: 0.6700 - acc: 0.7958 - val_loss: 0.5843 - val_acc: 0.8142\n",
            "Epoch 94/100\n",
            "42000/42000 [==============================] - 3s 75us/sample - loss: 0.6631 - acc: 0.7982 - val_loss: 0.5173 - val_acc: 0.8392\n",
            "Epoch 95/100\n",
            "42000/42000 [==============================] - 3s 76us/sample - loss: 0.6689 - acc: 0.7957 - val_loss: 0.4906 - val_acc: 0.8508\n",
            "Epoch 96/100\n",
            "42000/42000 [==============================] - 3s 75us/sample - loss: 0.6583 - acc: 0.7986 - val_loss: 0.5055 - val_acc: 0.8453\n",
            "Epoch 97/100\n",
            "42000/42000 [==============================] - 3s 74us/sample - loss: 0.6520 - acc: 0.8021 - val_loss: 0.5113 - val_acc: 0.8429\n",
            "Epoch 98/100\n",
            "42000/42000 [==============================] - 3s 73us/sample - loss: 0.6541 - acc: 0.8018 - val_loss: 0.5233 - val_acc: 0.8430\n",
            "Epoch 99/100\n",
            "42000/42000 [==============================] - 3s 73us/sample - loss: 0.6445 - acc: 0.8042 - val_loss: 0.5069 - val_acc: 0.8439\n",
            "Epoch 100/100\n",
            "42000/42000 [==============================] - 3s 73us/sample - loss: 0.6440 - acc: 0.8038 - val_loss: 0.5548 - val_acc: 0.8234\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JpuSGV5WUUJX",
        "outputId": "c8941ab4-0a44-4875-be41-0c7e9ff1976b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "print('NN model with dropout - sgd optimizer'); print('--'*40)\n",
        "results6 = model6.evaluate(X_val, y_val)\n",
        "print('Validation accuracy: {}'.format(round(results6[1]*100, 2), '%'))"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NN model with dropout - sgd optimizer\n",
            "--------------------------------------------------------------------------------\n",
            "60000/60000 [==============================] - 6s 96us/sample - loss: 0.5548 - acc: 0.8234\n",
            "Validation accuracy: 82.34\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "658feR9fZ353",
        "outputId": "437902c0-0698-4730-aba5-016a755aa8e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# compiling the neural network classifier, adam optimizer\n",
        "adam = optimizers.Adam(lr = 0.001)\n",
        "model6.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# Adding activation function - softmax for multiclass classification\n",
        "history = model6.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 42000 samples, validate on 60000 samples\n",
            "Epoch 1/100\n",
            "42000/42000 [==============================] - 4s 94us/sample - loss: 1.0537 - acc: 0.6722 - val_loss: 1.4033 - val_acc: 0.5174\n",
            "Epoch 2/100\n",
            "42000/42000 [==============================] - 3s 77us/sample - loss: 0.9148 - acc: 0.7188 - val_loss: 1.2238 - val_acc: 0.5859\n",
            "Epoch 3/100\n",
            "42000/42000 [==============================] - 3s 75us/sample - loss: 0.8570 - acc: 0.7368 - val_loss: 1.1011 - val_acc: 0.6679\n",
            "Epoch 4/100\n",
            "42000/42000 [==============================] - 3s 75us/sample - loss: 0.8046 - acc: 0.7538 - val_loss: 1.2365 - val_acc: 0.5808\n",
            "Epoch 5/100\n",
            "42000/42000 [==============================] - 3s 76us/sample - loss: 0.7710 - acc: 0.7652 - val_loss: 1.0478 - val_acc: 0.6470\n",
            "Epoch 6/100\n",
            "42000/42000 [==============================] - 3s 78us/sample - loss: 0.7443 - acc: 0.7740 - val_loss: 1.1123 - val_acc: 0.6237\n",
            "Epoch 7/100\n",
            "42000/42000 [==============================] - 3s 75us/sample - loss: 0.7118 - acc: 0.7849 - val_loss: 0.9592 - val_acc: 0.7093\n",
            "Epoch 8/100\n",
            "42000/42000 [==============================] - 3s 78us/sample - loss: 0.6871 - acc: 0.7943 - val_loss: 0.9791 - val_acc: 0.6772\n",
            "Epoch 9/100\n",
            "42000/42000 [==============================] - 3s 78us/sample - loss: 0.6758 - acc: 0.7970 - val_loss: 1.0898 - val_acc: 0.6525\n",
            "Epoch 10/100\n",
            "42000/42000 [==============================] - 3s 75us/sample - loss: 0.6555 - acc: 0.8020 - val_loss: 1.0242 - val_acc: 0.6567\n",
            "Epoch 11/100\n",
            "42000/42000 [==============================] - 3s 76us/sample - loss: 0.6432 - acc: 0.8072 - val_loss: 0.7851 - val_acc: 0.7456\n",
            "Epoch 12/100\n",
            "42000/42000 [==============================] - 3s 75us/sample - loss: 0.6278 - acc: 0.8113 - val_loss: 0.9653 - val_acc: 0.6997\n",
            "Epoch 13/100\n",
            "42000/42000 [==============================] - 3s 75us/sample - loss: 0.6128 - acc: 0.8131 - val_loss: 0.9813 - val_acc: 0.7164\n",
            "Epoch 14/100\n",
            "42000/42000 [==============================] - 3s 77us/sample - loss: 0.6027 - acc: 0.8187 - val_loss: 0.9552 - val_acc: 0.6895\n",
            "Epoch 15/100\n",
            "42000/42000 [==============================] - 3s 78us/sample - loss: 0.5925 - acc: 0.8239 - val_loss: 0.9974 - val_acc: 0.6705\n",
            "Epoch 16/100\n",
            "42000/42000 [==============================] - 3s 77us/sample - loss: 0.5782 - acc: 0.8253 - val_loss: 0.8248 - val_acc: 0.7401\n",
            "Epoch 17/100\n",
            "42000/42000 [==============================] - 3s 76us/sample - loss: 0.5701 - acc: 0.8281 - val_loss: 0.7529 - val_acc: 0.7717\n",
            "Epoch 18/100\n",
            "42000/42000 [==============================] - 3s 77us/sample - loss: 0.5637 - acc: 0.8301 - val_loss: 0.8962 - val_acc: 0.7063\n",
            "Epoch 19/100\n",
            "42000/42000 [==============================] - 3s 77us/sample - loss: 0.5447 - acc: 0.8370 - val_loss: 0.9742 - val_acc: 0.6851\n",
            "Epoch 20/100\n",
            "42000/42000 [==============================] - 3s 74us/sample - loss: 0.5457 - acc: 0.8355 - val_loss: 0.7525 - val_acc: 0.7635\n",
            "Epoch 21/100\n",
            "42000/42000 [==============================] - 3s 76us/sample - loss: 0.5411 - acc: 0.8390 - val_loss: 0.8379 - val_acc: 0.7336\n",
            "Epoch 22/100\n",
            "42000/42000 [==============================] - 3s 77us/sample - loss: 0.5360 - acc: 0.8386 - val_loss: 0.7831 - val_acc: 0.7468\n",
            "Epoch 23/100\n",
            "42000/42000 [==============================] - 3s 76us/sample - loss: 0.5198 - acc: 0.8438 - val_loss: 0.7819 - val_acc: 0.7510\n",
            "Epoch 24/100\n",
            "42000/42000 [==============================] - 3s 75us/sample - loss: 0.5222 - acc: 0.8438 - val_loss: 0.9025 - val_acc: 0.7104\n",
            "Epoch 25/100\n",
            "42000/42000 [==============================] - 3s 75us/sample - loss: 0.5140 - acc: 0.8475 - val_loss: 0.7967 - val_acc: 0.7447\n",
            "Epoch 26/100\n",
            "42000/42000 [==============================] - 3s 75us/sample - loss: 0.4962 - acc: 0.8503 - val_loss: 0.6611 - val_acc: 0.7896\n",
            "Epoch 27/100\n",
            "42000/42000 [==============================] - 3s 77us/sample - loss: 0.5022 - acc: 0.8502 - val_loss: 0.9104 - val_acc: 0.7111\n",
            "Epoch 28/100\n",
            "42000/42000 [==============================] - 3s 76us/sample - loss: 0.4966 - acc: 0.8516 - val_loss: 0.7199 - val_acc: 0.7667\n",
            "Epoch 29/100\n",
            "42000/42000 [==============================] - 3s 79us/sample - loss: 0.4903 - acc: 0.8530 - val_loss: 0.6487 - val_acc: 0.8070\n",
            "Epoch 30/100\n",
            "42000/42000 [==============================] - 3s 80us/sample - loss: 0.4716 - acc: 0.8576 - val_loss: 0.7528 - val_acc: 0.7468\n",
            "Epoch 31/100\n",
            "42000/42000 [==============================] - 3s 79us/sample - loss: 0.4843 - acc: 0.8537 - val_loss: 0.9980 - val_acc: 0.6725\n",
            "Epoch 32/100\n",
            "42000/42000 [==============================] - 3s 77us/sample - loss: 0.4617 - acc: 0.8610 - val_loss: 0.8136 - val_acc: 0.7298\n",
            "Epoch 33/100\n",
            "42000/42000 [==============================] - 3s 76us/sample - loss: 0.4587 - acc: 0.8615 - val_loss: 0.6557 - val_acc: 0.7965\n",
            "Epoch 34/100\n",
            "42000/42000 [==============================] - 3s 78us/sample - loss: 0.4593 - acc: 0.8616 - val_loss: 0.9737 - val_acc: 0.6831\n",
            "Epoch 35/100\n",
            "42000/42000 [==============================] - 3s 76us/sample - loss: 0.4614 - acc: 0.8603 - val_loss: 0.6706 - val_acc: 0.7910\n",
            "Epoch 36/100\n",
            "42000/42000 [==============================] - 3s 75us/sample - loss: 0.4496 - acc: 0.8637 - val_loss: 0.7671 - val_acc: 0.7528\n",
            "Epoch 37/100\n",
            "42000/42000 [==============================] - 3s 74us/sample - loss: 0.4452 - acc: 0.8641 - val_loss: 0.9066 - val_acc: 0.7028\n",
            "Epoch 38/100\n",
            "42000/42000 [==============================] - 3s 78us/sample - loss: 0.4392 - acc: 0.8693 - val_loss: 0.8164 - val_acc: 0.7294\n",
            "Epoch 39/100\n",
            "42000/42000 [==============================] - 3s 80us/sample - loss: 0.4335 - acc: 0.8694 - val_loss: 0.8177 - val_acc: 0.7241\n",
            "Epoch 40/100\n",
            "42000/42000 [==============================] - 3s 79us/sample - loss: 0.4330 - acc: 0.8703 - val_loss: 0.6483 - val_acc: 0.7995\n",
            "Epoch 41/100\n",
            "42000/42000 [==============================] - 3s 76us/sample - loss: 0.4171 - acc: 0.8748 - val_loss: 0.8264 - val_acc: 0.7241\n",
            "Epoch 42/100\n",
            "42000/42000 [==============================] - 3s 76us/sample - loss: 0.4277 - acc: 0.8722 - val_loss: 0.6710 - val_acc: 0.7956\n",
            "Epoch 43/100\n",
            "42000/42000 [==============================] - 3s 77us/sample - loss: 0.4115 - acc: 0.8759 - val_loss: 0.7578 - val_acc: 0.7555\n",
            "Epoch 44/100\n",
            "42000/42000 [==============================] - 3s 76us/sample - loss: 0.4136 - acc: 0.8763 - val_loss: 0.8177 - val_acc: 0.7286\n",
            "Epoch 45/100\n",
            "42000/42000 [==============================] - 3s 77us/sample - loss: 0.4098 - acc: 0.8766 - val_loss: 0.7376 - val_acc: 0.7609\n",
            "Epoch 46/100\n",
            "42000/42000 [==============================] - 3s 74us/sample - loss: 0.4075 - acc: 0.8767 - val_loss: 0.5806 - val_acc: 0.8255\n",
            "Epoch 47/100\n",
            "42000/42000 [==============================] - 3s 74us/sample - loss: 0.4008 - acc: 0.8804 - val_loss: 0.5925 - val_acc: 0.8124\n",
            "Epoch 48/100\n",
            "42000/42000 [==============================] - 3s 79us/sample - loss: 0.3890 - acc: 0.8833 - val_loss: 0.6096 - val_acc: 0.7996\n",
            "Epoch 49/100\n",
            "42000/42000 [==============================] - 3s 74us/sample - loss: 0.3952 - acc: 0.8803 - val_loss: 0.6157 - val_acc: 0.8102\n",
            "Epoch 50/100\n",
            "42000/42000 [==============================] - 3s 75us/sample - loss: 0.3941 - acc: 0.8809 - val_loss: 0.6609 - val_acc: 0.7837\n",
            "Epoch 51/100\n",
            "42000/42000 [==============================] - 3s 75us/sample - loss: 0.3881 - acc: 0.8840 - val_loss: 0.5999 - val_acc: 0.8140\n",
            "Epoch 52/100\n",
            "42000/42000 [==============================] - 3s 75us/sample - loss: 0.3858 - acc: 0.8840 - val_loss: 0.6734 - val_acc: 0.7904\n",
            "Epoch 53/100\n",
            "42000/42000 [==============================] - 3s 76us/sample - loss: 0.3832 - acc: 0.8857 - val_loss: 0.7264 - val_acc: 0.7635\n",
            "Epoch 54/100\n",
            "42000/42000 [==============================] - 3s 75us/sample - loss: 0.3816 - acc: 0.8854 - val_loss: 0.5058 - val_acc: 0.8416\n",
            "Epoch 55/100\n",
            "42000/42000 [==============================] - 3s 76us/sample - loss: 0.3840 - acc: 0.8842 - val_loss: 0.6260 - val_acc: 0.7953\n",
            "Epoch 56/100\n",
            "42000/42000 [==============================] - 3s 74us/sample - loss: 0.3719 - acc: 0.8887 - val_loss: 0.5832 - val_acc: 0.8156\n",
            "Epoch 57/100\n",
            "42000/42000 [==============================] - 3s 77us/sample - loss: 0.3672 - acc: 0.8902 - val_loss: 0.7094 - val_acc: 0.7707\n",
            "Epoch 58/100\n",
            "42000/42000 [==============================] - 3s 76us/sample - loss: 0.3665 - acc: 0.8879 - val_loss: 0.6616 - val_acc: 0.7845\n",
            "Epoch 59/100\n",
            "42000/42000 [==============================] - 3s 74us/sample - loss: 0.3613 - acc: 0.8912 - val_loss: 0.6697 - val_acc: 0.7796\n",
            "Epoch 60/100\n",
            "42000/42000 [==============================] - 3s 76us/sample - loss: 0.3632 - acc: 0.8903 - val_loss: 0.6813 - val_acc: 0.7756\n",
            "Epoch 61/100\n",
            "42000/42000 [==============================] - 3s 74us/sample - loss: 0.3611 - acc: 0.8918 - val_loss: 0.5738 - val_acc: 0.8176\n",
            "Epoch 62/100\n",
            "42000/42000 [==============================] - 3s 77us/sample - loss: 0.3522 - acc: 0.8925 - val_loss: 0.8086 - val_acc: 0.7253\n",
            "Epoch 63/100\n",
            "42000/42000 [==============================] - 3s 76us/sample - loss: 0.3556 - acc: 0.8918 - val_loss: 0.5383 - val_acc: 0.8334\n",
            "Epoch 64/100\n",
            "42000/42000 [==============================] - 3s 74us/sample - loss: 0.3483 - acc: 0.8962 - val_loss: 0.5685 - val_acc: 0.8171\n",
            "Epoch 65/100\n",
            "42000/42000 [==============================] - 3s 75us/sample - loss: 0.3498 - acc: 0.8955 - val_loss: 0.7666 - val_acc: 0.7503\n",
            "Epoch 66/100\n",
            "42000/42000 [==============================] - 3s 76us/sample - loss: 0.3507 - acc: 0.8945 - val_loss: 0.5648 - val_acc: 0.8195\n",
            "Epoch 67/100\n",
            "42000/42000 [==============================] - 3s 74us/sample - loss: 0.3403 - acc: 0.8976 - val_loss: 0.5243 - val_acc: 0.8408\n",
            "Epoch 68/100\n",
            "42000/42000 [==============================] - 3s 76us/sample - loss: 0.3354 - acc: 0.8976 - val_loss: 0.6728 - val_acc: 0.7855\n",
            "Epoch 69/100\n",
            "42000/42000 [==============================] - 3s 77us/sample - loss: 0.3390 - acc: 0.8987 - val_loss: 0.5558 - val_acc: 0.8216\n",
            "Epoch 70/100\n",
            "42000/42000 [==============================] - 3s 75us/sample - loss: 0.3329 - acc: 0.8994 - val_loss: 0.5011 - val_acc: 0.8407\n",
            "Epoch 71/100\n",
            "42000/42000 [==============================] - 3s 76us/sample - loss: 0.3349 - acc: 0.8985 - val_loss: 0.5567 - val_acc: 0.8256\n",
            "Epoch 72/100\n",
            "42000/42000 [==============================] - 3s 76us/sample - loss: 0.3319 - acc: 0.8995 - val_loss: 0.6989 - val_acc: 0.7715\n",
            "Epoch 73/100\n",
            "42000/42000 [==============================] - 3s 77us/sample - loss: 0.3284 - acc: 0.9018 - val_loss: 0.6308 - val_acc: 0.7950\n",
            "Epoch 74/100\n",
            "42000/42000 [==============================] - 3s 76us/sample - loss: 0.3259 - acc: 0.9019 - val_loss: 0.4959 - val_acc: 0.8462\n",
            "Epoch 75/100\n",
            "42000/42000 [==============================] - 3s 75us/sample - loss: 0.3188 - acc: 0.9030 - val_loss: 1.0376 - val_acc: 0.6919\n",
            "Epoch 76/100\n",
            "42000/42000 [==============================] - 3s 77us/sample - loss: 0.3230 - acc: 0.9020 - val_loss: 0.4649 - val_acc: 0.8549\n",
            "Epoch 77/100\n",
            "42000/42000 [==============================] - 3s 75us/sample - loss: 0.3229 - acc: 0.9029 - val_loss: 0.5583 - val_acc: 0.8210\n",
            "Epoch 78/100\n",
            "42000/42000 [==============================] - 3s 76us/sample - loss: 0.3136 - acc: 0.9033 - val_loss: 0.5741 - val_acc: 0.8153\n",
            "Epoch 79/100\n",
            "42000/42000 [==============================] - 3s 76us/sample - loss: 0.3167 - acc: 0.9059 - val_loss: 0.7069 - val_acc: 0.7759\n",
            "Epoch 80/100\n",
            "42000/42000 [==============================] - 3s 76us/sample - loss: 0.3190 - acc: 0.9038 - val_loss: 0.6045 - val_acc: 0.8156\n",
            "Epoch 81/100\n",
            "42000/42000 [==============================] - 3s 75us/sample - loss: 0.3124 - acc: 0.9059 - val_loss: 0.5937 - val_acc: 0.8174\n",
            "Epoch 82/100\n",
            "42000/42000 [==============================] - 3s 77us/sample - loss: 0.3080 - acc: 0.9057 - val_loss: 0.4613 - val_acc: 0.8525\n",
            "Epoch 83/100\n",
            "42000/42000 [==============================] - 3s 75us/sample - loss: 0.3059 - acc: 0.9082 - val_loss: 0.4942 - val_acc: 0.8424\n",
            "Epoch 84/100\n",
            "42000/42000 [==============================] - 3s 74us/sample - loss: 0.3105 - acc: 0.9070 - val_loss: 0.7019 - val_acc: 0.7707\n",
            "Epoch 85/100\n",
            "42000/42000 [==============================] - 3s 76us/sample - loss: 0.3036 - acc: 0.9065 - val_loss: 0.6475 - val_acc: 0.8042\n",
            "Epoch 86/100\n",
            "42000/42000 [==============================] - 3s 74us/sample - loss: 0.3014 - acc: 0.9097 - val_loss: 0.7370 - val_acc: 0.7640\n",
            "Epoch 87/100\n",
            "42000/42000 [==============================] - 3s 75us/sample - loss: 0.3060 - acc: 0.9084 - val_loss: 0.4844 - val_acc: 0.8459\n",
            "Epoch 88/100\n",
            "42000/42000 [==============================] - 3s 76us/sample - loss: 0.2968 - acc: 0.9106 - val_loss: 1.0255 - val_acc: 0.7094\n",
            "Epoch 89/100\n",
            "42000/42000 [==============================] - 3s 74us/sample - loss: 0.2941 - acc: 0.9130 - val_loss: 0.5806 - val_acc: 0.8143\n",
            "Epoch 90/100\n",
            "42000/42000 [==============================] - 3s 75us/sample - loss: 0.2944 - acc: 0.9116 - val_loss: 0.6892 - val_acc: 0.7789\n",
            "Epoch 91/100\n",
            "42000/42000 [==============================] - 3s 74us/sample - loss: 0.2954 - acc: 0.9115 - val_loss: 0.6269 - val_acc: 0.8006\n",
            "Epoch 92/100\n",
            "42000/42000 [==============================] - 3s 77us/sample - loss: 0.2917 - acc: 0.9124 - val_loss: 0.5374 - val_acc: 0.8297\n",
            "Epoch 93/100\n",
            "42000/42000 [==============================] - 3s 74us/sample - loss: 0.2858 - acc: 0.9142 - val_loss: 0.6446 - val_acc: 0.7962\n",
            "Epoch 94/100\n",
            "42000/42000 [==============================] - 3s 77us/sample - loss: 0.2879 - acc: 0.9130 - val_loss: 0.5270 - val_acc: 0.8307\n",
            "Epoch 95/100\n",
            "42000/42000 [==============================] - 3s 76us/sample - loss: 0.2856 - acc: 0.9132 - val_loss: 0.4362 - val_acc: 0.8623\n",
            "Epoch 96/100\n",
            "42000/42000 [==============================] - 3s 75us/sample - loss: 0.2855 - acc: 0.9131 - val_loss: 0.5630 - val_acc: 0.8196\n",
            "Epoch 97/100\n",
            "42000/42000 [==============================] - 3s 75us/sample - loss: 0.2830 - acc: 0.9145 - val_loss: 0.7080 - val_acc: 0.7754\n",
            "Epoch 98/100\n",
            "42000/42000 [==============================] - 3s 74us/sample - loss: 0.2831 - acc: 0.9138 - val_loss: 0.6825 - val_acc: 0.7893\n",
            "Epoch 99/100\n",
            "42000/42000 [==============================] - 3s 74us/sample - loss: 0.2787 - acc: 0.9161 - val_loss: 0.4657 - val_acc: 0.8533\n",
            "Epoch 100/100\n",
            "42000/42000 [==============================] - 3s 74us/sample - loss: 0.2768 - acc: 0.9157 - val_loss: 0.4664 - val_acc: 0.8536\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f-n63MmSN9DQ",
        "outputId": "271fa5ec-ae50-4c0e-d0e3-a205b01587d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "print('NN model with dropout - adam optimizer'); print('--'*40)\n",
        "results6 = model6.evaluate(X_val, y_val)\n",
        "print('Validation accuracy: {}'.format(round(results6[1]*100, 2), '%'))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NN model with dropout - adam optimizer\n",
            "--------------------------------------------------------------------------------\n",
            "60000/60000 [==============================] - 6s 97us/sample - loss: 0.4664 - acc: 0.8536\n",
            "Validation accuracy: 85.36\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YEbd6Ynm6wmp"
      },
      "source": [
        "<a id='o8'></a>\n",
        "##### Observation 8 - Batch Normalization and Dropout\n",
        "* Didn't result in any improvement of score.\n",
        "* NN model, relu activations, SGD optimizers with weight initializers and batch normalization is still the best model.\n",
        "* Next, let's try batch normalization and dropout with adam optimizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "H-x-REsa00x7"
      },
      "source": [
        "<a id='Prediction'></a>\n",
        "### Prediction on test dataset using Model 3 - relu activations, Adam optimizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E3V_MkU-018D",
        "outputId": "0c45e786-8dfc-4b93-f860-7be00e32f52b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print('NN model with relu activations and changing number of activators'); print('--'*40)\n",
        "# Initialize the neural network classifier\n",
        "model3 = Sequential()\n",
        "\n",
        "# Input Layer - adding input layer and activation functions relu\n",
        "model3.add(Dense(256, input_shape = (1024, )))\n",
        "# Adding activation function\n",
        "model3.add(Activation('relu'))\n",
        "\n",
        "#Hidden Layer 1 - adding first hidden layer\n",
        "model3.add(Dense(128))\n",
        "# Adding activation function\n",
        "model3.add(Activation('relu'))\n",
        "\n",
        "#Hidden Layer 2 - Adding second hidden layer\n",
        "model3.add(Dense(64))\n",
        "# Adding activation function\n",
        "model3.add(Activation('relu'))\n",
        "\n",
        "# Output Layer - adding output layer which is of 10 nodes (digits)\n",
        "model3.add(Dense(10))\n",
        "# Adding activation function - softmax for multiclass classification\n",
        "model3.add(Activation('softmax'))"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NN model with relu activations and changing number of activators\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dFUqzgvDL0Gf",
        "outputId": "24e7f705-2d56-4b29-8d02-794ab467d906",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# compiling the neural network classifier, adam optimizer\n",
        "adam = optimizers.Adam(lr = 0.001)\n",
        "model3.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "\n",
        "# Fitting the neural network for training\n",
        "history = model3.fit(X_train, y_train, validation_data = (X_val, y_val), batch_size = 200, epochs = 100, verbose = 1)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 42000 samples, validate on 60000 samples\n",
            "Epoch 1/100\n",
            "42000/42000 [==============================] - 2s 47us/sample - loss: 2.2186 - acc: 0.1635 - val_loss: 1.9248 - val_acc: 0.3210\n",
            "Epoch 2/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 1.6238 - acc: 0.4394 - val_loss: 1.3642 - val_acc: 0.5505\n",
            "Epoch 3/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 1.2903 - acc: 0.5803 - val_loss: 1.2130 - val_acc: 0.6088\n",
            "Epoch 4/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 1.1580 - acc: 0.6350 - val_loss: 1.1313 - val_acc: 0.6453\n",
            "Epoch 5/100\n",
            "42000/42000 [==============================] - 2s 39us/sample - loss: 1.0535 - acc: 0.6729 - val_loss: 1.0103 - val_acc: 0.6860\n",
            "Epoch 6/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.9880 - acc: 0.6935 - val_loss: 0.9377 - val_acc: 0.7111\n",
            "Epoch 7/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.9396 - acc: 0.7109 - val_loss: 0.9256 - val_acc: 0.7147\n",
            "Epoch 8/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.9129 - acc: 0.7181 - val_loss: 0.8904 - val_acc: 0.7280\n",
            "Epoch 9/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.8694 - acc: 0.7314 - val_loss: 0.8750 - val_acc: 0.7346\n",
            "Epoch 10/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.8461 - acc: 0.7388 - val_loss: 0.8414 - val_acc: 0.7420\n",
            "Epoch 11/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.8159 - acc: 0.7490 - val_loss: 0.8106 - val_acc: 0.7511\n",
            "Epoch 12/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.7983 - acc: 0.7534 - val_loss: 0.8013 - val_acc: 0.7548\n",
            "Epoch 13/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.7778 - acc: 0.7590 - val_loss: 0.7755 - val_acc: 0.7620\n",
            "Epoch 14/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.7594 - acc: 0.7691 - val_loss: 0.7490 - val_acc: 0.7708\n",
            "Epoch 15/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.7363 - acc: 0.7731 - val_loss: 0.7375 - val_acc: 0.7751\n",
            "Epoch 16/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.7223 - acc: 0.7787 - val_loss: 0.7302 - val_acc: 0.7769\n",
            "Epoch 17/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.7003 - acc: 0.7853 - val_loss: 0.7193 - val_acc: 0.7811\n",
            "Epoch 18/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.7043 - acc: 0.7833 - val_loss: 0.6909 - val_acc: 0.7889\n",
            "Epoch 19/100\n",
            "42000/42000 [==============================] - 2s 38us/sample - loss: 0.6822 - acc: 0.7881 - val_loss: 0.6684 - val_acc: 0.7972\n",
            "Epoch 20/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.6669 - acc: 0.7948 - val_loss: 0.6865 - val_acc: 0.7892\n",
            "Epoch 21/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.6560 - acc: 0.7980 - val_loss: 0.6343 - val_acc: 0.8081\n",
            "Epoch 22/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.6505 - acc: 0.8000 - val_loss: 0.6390 - val_acc: 0.8079\n",
            "Epoch 23/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.6461 - acc: 0.8014 - val_loss: 0.6496 - val_acc: 0.8033\n",
            "Epoch 24/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.6356 - acc: 0.8035 - val_loss: 0.6657 - val_acc: 0.7958\n",
            "Epoch 25/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.6291 - acc: 0.8073 - val_loss: 0.6447 - val_acc: 0.8019\n",
            "Epoch 26/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.6122 - acc: 0.8118 - val_loss: 0.6135 - val_acc: 0.8141\n",
            "Epoch 27/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.5973 - acc: 0.8159 - val_loss: 0.6616 - val_acc: 0.8000\n",
            "Epoch 28/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.5998 - acc: 0.8153 - val_loss: 0.6001 - val_acc: 0.8182\n",
            "Epoch 29/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.5891 - acc: 0.8197 - val_loss: 0.6149 - val_acc: 0.8137\n",
            "Epoch 30/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.5742 - acc: 0.8234 - val_loss: 0.5808 - val_acc: 0.8246\n",
            "Epoch 31/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.5696 - acc: 0.8232 - val_loss: 0.5778 - val_acc: 0.8245\n",
            "Epoch 32/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.5745 - acc: 0.8226 - val_loss: 0.6068 - val_acc: 0.8133\n",
            "Epoch 33/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.5567 - acc: 0.8272 - val_loss: 0.5667 - val_acc: 0.8275\n",
            "Epoch 34/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.5441 - acc: 0.8320 - val_loss: 0.6144 - val_acc: 0.8120\n",
            "Epoch 35/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.5405 - acc: 0.8319 - val_loss: 0.5705 - val_acc: 0.8274\n",
            "Epoch 36/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.5394 - acc: 0.8333 - val_loss: 0.5588 - val_acc: 0.8294\n",
            "Epoch 37/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.5307 - acc: 0.8359 - val_loss: 0.6015 - val_acc: 0.8169\n",
            "Epoch 38/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.5207 - acc: 0.8386 - val_loss: 0.5612 - val_acc: 0.8277\n",
            "Epoch 39/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.5210 - acc: 0.8379 - val_loss: 0.5615 - val_acc: 0.8302\n",
            "Epoch 40/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.5155 - acc: 0.8396 - val_loss: 0.5625 - val_acc: 0.8296\n",
            "Epoch 41/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.5062 - acc: 0.8416 - val_loss: 0.5581 - val_acc: 0.8293\n",
            "Epoch 42/100\n",
            "42000/42000 [==============================] - 2s 38us/sample - loss: 0.5004 - acc: 0.8448 - val_loss: 0.5450 - val_acc: 0.8347\n",
            "Epoch 43/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.4930 - acc: 0.8467 - val_loss: 0.5588 - val_acc: 0.8300\n",
            "Epoch 44/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.4874 - acc: 0.8491 - val_loss: 0.5749 - val_acc: 0.8262\n",
            "Epoch 45/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.4897 - acc: 0.8470 - val_loss: 0.5431 - val_acc: 0.8338\n",
            "Epoch 46/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.4849 - acc: 0.8476 - val_loss: 0.5372 - val_acc: 0.8375\n",
            "Epoch 47/100\n",
            "42000/42000 [==============================] - 2s 38us/sample - loss: 0.4832 - acc: 0.8492 - val_loss: 0.5235 - val_acc: 0.8389\n",
            "Epoch 48/100\n",
            "42000/42000 [==============================] - 2s 38us/sample - loss: 0.4729 - acc: 0.8508 - val_loss: 0.4879 - val_acc: 0.8515\n",
            "Epoch 49/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.4684 - acc: 0.8545 - val_loss: 0.5765 - val_acc: 0.8234\n",
            "Epoch 50/100\n",
            "42000/42000 [==============================] - 2s 38us/sample - loss: 0.4707 - acc: 0.8544 - val_loss: 0.4880 - val_acc: 0.8522\n",
            "Epoch 51/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.4465 - acc: 0.8600 - val_loss: 0.4978 - val_acc: 0.8496\n",
            "Epoch 52/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.4472 - acc: 0.8603 - val_loss: 0.4974 - val_acc: 0.8506\n",
            "Epoch 53/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.4389 - acc: 0.8633 - val_loss: 0.4529 - val_acc: 0.8662\n",
            "Epoch 54/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.4384 - acc: 0.8632 - val_loss: 0.5015 - val_acc: 0.8496\n",
            "Epoch 55/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.4370 - acc: 0.8634 - val_loss: 0.5062 - val_acc: 0.8478\n",
            "Epoch 56/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.4419 - acc: 0.8604 - val_loss: 0.4749 - val_acc: 0.8570\n",
            "Epoch 57/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.4373 - acc: 0.8606 - val_loss: 0.4670 - val_acc: 0.8611\n",
            "Epoch 58/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.4355 - acc: 0.8624 - val_loss: 0.4806 - val_acc: 0.8541\n",
            "Epoch 59/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.4346 - acc: 0.8645 - val_loss: 0.5012 - val_acc: 0.8476\n",
            "Epoch 60/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.4303 - acc: 0.8645 - val_loss: 0.4638 - val_acc: 0.8612\n",
            "Epoch 61/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.4317 - acc: 0.8630 - val_loss: 0.4974 - val_acc: 0.8484\n",
            "Epoch 62/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.4145 - acc: 0.8694 - val_loss: 0.4596 - val_acc: 0.8613\n",
            "Epoch 63/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.4148 - acc: 0.8693 - val_loss: 0.4723 - val_acc: 0.8578\n",
            "Epoch 64/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.4148 - acc: 0.8683 - val_loss: 0.4912 - val_acc: 0.8519\n",
            "Epoch 65/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.4057 - acc: 0.8721 - val_loss: 0.4887 - val_acc: 0.8515\n",
            "Epoch 66/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.4107 - acc: 0.8704 - val_loss: 0.4300 - val_acc: 0.8717\n",
            "Epoch 67/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.3956 - acc: 0.8748 - val_loss: 0.4840 - val_acc: 0.8546\n",
            "Epoch 68/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.3993 - acc: 0.8745 - val_loss: 0.4516 - val_acc: 0.8658\n",
            "Epoch 69/100\n",
            "42000/42000 [==============================] - 2s 38us/sample - loss: 0.4029 - acc: 0.8720 - val_loss: 0.4353 - val_acc: 0.8707\n",
            "Epoch 70/100\n",
            "42000/42000 [==============================] - 2s 38us/sample - loss: 0.3925 - acc: 0.8757 - val_loss: 0.4442 - val_acc: 0.8676\n",
            "Epoch 71/100\n",
            "42000/42000 [==============================] - 2s 38us/sample - loss: 0.3884 - acc: 0.8779 - val_loss: 0.5119 - val_acc: 0.8472\n",
            "Epoch 72/100\n",
            "42000/42000 [==============================] - 2s 38us/sample - loss: 0.3888 - acc: 0.8759 - val_loss: 0.4722 - val_acc: 0.8575\n",
            "Epoch 73/100\n",
            "42000/42000 [==============================] - 2s 38us/sample - loss: 0.3797 - acc: 0.8790 - val_loss: 0.4765 - val_acc: 0.8550\n",
            "Epoch 74/100\n",
            "42000/42000 [==============================] - 2s 38us/sample - loss: 0.3951 - acc: 0.8726 - val_loss: 0.4332 - val_acc: 0.8717\n",
            "Epoch 75/100\n",
            "42000/42000 [==============================] - 2s 38us/sample - loss: 0.3835 - acc: 0.8769 - val_loss: 0.4680 - val_acc: 0.8590\n",
            "Epoch 76/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.3781 - acc: 0.8797 - val_loss: 0.4740 - val_acc: 0.8578\n",
            "Epoch 77/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.3788 - acc: 0.8795 - val_loss: 0.4579 - val_acc: 0.8621\n",
            "Epoch 78/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.3770 - acc: 0.8797 - val_loss: 0.4405 - val_acc: 0.8691\n",
            "Epoch 79/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.3737 - acc: 0.8800 - val_loss: 0.4627 - val_acc: 0.8601\n",
            "Epoch 80/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.3679 - acc: 0.8837 - val_loss: 0.5108 - val_acc: 0.8475\n",
            "Epoch 81/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.3625 - acc: 0.8841 - val_loss: 0.4471 - val_acc: 0.8662\n",
            "Epoch 82/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.3613 - acc: 0.8843 - val_loss: 0.4357 - val_acc: 0.8715\n",
            "Epoch 83/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.3664 - acc: 0.8824 - val_loss: 0.4189 - val_acc: 0.8776\n",
            "Epoch 84/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.3525 - acc: 0.8867 - val_loss: 0.4432 - val_acc: 0.8673\n",
            "Epoch 85/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.3553 - acc: 0.8859 - val_loss: 0.4818 - val_acc: 0.8543\n",
            "Epoch 86/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.3505 - acc: 0.8885 - val_loss: 0.4358 - val_acc: 0.8711\n",
            "Epoch 87/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.3493 - acc: 0.8874 - val_loss: 0.4198 - val_acc: 0.8752\n",
            "Epoch 88/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.3537 - acc: 0.8858 - val_loss: 0.4301 - val_acc: 0.8727\n",
            "Epoch 89/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.3503 - acc: 0.8880 - val_loss: 0.4464 - val_acc: 0.8676\n",
            "Epoch 90/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.3492 - acc: 0.8875 - val_loss: 0.4119 - val_acc: 0.8795\n",
            "Epoch 91/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.3421 - acc: 0.8903 - val_loss: 0.4181 - val_acc: 0.8783\n",
            "Epoch 92/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.3464 - acc: 0.8899 - val_loss: 0.4488 - val_acc: 0.8684\n",
            "Epoch 93/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.3529 - acc: 0.8845 - val_loss: 0.3993 - val_acc: 0.8834\n",
            "Epoch 94/100\n",
            "42000/42000 [==============================] - 1s 36us/sample - loss: 0.3401 - acc: 0.8915 - val_loss: 0.3983 - val_acc: 0.8829\n",
            "Epoch 95/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.3345 - acc: 0.8915 - val_loss: 0.4123 - val_acc: 0.8779\n",
            "Epoch 96/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.3326 - acc: 0.8922 - val_loss: 0.4275 - val_acc: 0.8741\n",
            "Epoch 97/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.3194 - acc: 0.8974 - val_loss: 0.4024 - val_acc: 0.8822\n",
            "Epoch 98/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.3302 - acc: 0.8936 - val_loss: 0.4861 - val_acc: 0.8573\n",
            "Epoch 99/100\n",
            "42000/42000 [==============================] - 2s 37us/sample - loss: 0.3349 - acc: 0.8905 - val_loss: 0.4733 - val_acc: 0.8619\n",
            "Epoch 100/100\n",
            "42000/42000 [==============================] - 2s 36us/sample - loss: 0.3292 - acc: 0.8934 - val_loss: 0.4105 - val_acc: 0.8798\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2ao6j_Dl1I9j",
        "outputId": "418b8c3b-6ec7-4258-84f5-df44de95b8ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "print('NN with batch normalization'); print('--'*40)\n",
        "results3 = model3.evaluate(X_val, y_val)\n",
        "print('Validation accuracy: {}'.format(round(results3[1]*100, 2), '%'))"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NN with batch normalization\n",
            "--------------------------------------------------------------------------------\n",
            "60000/60000 [==============================] - 4s 72us/sample - loss: 0.4105 - acc: 0.8798\n",
            "Validation accuracy: 87.98\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2VfnxZqX1Mnm",
        "outputId": "d66ba130-0383-4ced-db0e-5deba4a8ee83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "print('Testing the model on test dataset')\n",
        "predictions = model3.predict_classes(X_test)\n",
        "score = model3.evaluate(X_test, y_test)\n",
        "print('Test loss :', score[0])\n",
        "print('Test accuracy :', score[1])"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing the model on test dataset\n",
            "18000/18000 [==============================] - 1s 73us/sample - loss: 0.6690 - acc: 0.8254\n",
            "Test loss : 0.6690111002657149\n",
            "Test accuracy : 0.82544446\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Zg8aVLVu1rkI",
        "outputId": "0b9f7626-1f11-4207-9b45-e97ba59c2fab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "source": [
        "print('Classification Report'); print('--'*40)\n",
        "print(classification_report(y_test_o, predictions))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification Report\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.86      0.86      1814\n",
            "           1       0.85      0.84      0.85      1828\n",
            "           2       0.89      0.81      0.85      1803\n",
            "           3       0.72      0.81      0.76      1719\n",
            "           4       0.89      0.84      0.86      1812\n",
            "           5       0.83      0.79      0.81      1768\n",
            "           6       0.78      0.85      0.81      1832\n",
            "           7       0.83      0.89      0.86      1808\n",
            "           8       0.79      0.77      0.78      1812\n",
            "           9       0.83      0.80      0.82      1804\n",
            "\n",
            "    accuracy                           0.83     18000\n",
            "   macro avg       0.83      0.83      0.83     18000\n",
            "weighted avg       0.83      0.83      0.83     18000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rTLDrm8u3KR0",
        "outputId": "7d9903be-8257-4d02-8d9d-7cab966a44a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        }
      },
      "source": [
        "print('Visualizing the confusion matrix')\n",
        "plt.figure(figsize = (15, 7.2))\n",
        "sns.heatmap(confusion_matrix(y_test_o, predictions), annot = True)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Visualizing the confusion matrix\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f1b3f8a6b00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2oAAAGlCAYAAACLCgVDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\naiIiIiIiIoFRR01ERERERCQw/wWJFFS0sqYOjAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1080x518.4 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "91DnbyzFQCVg",
        "outputId": "040ec1dd-aba2-4eb8-ecfd-861bf8687cf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        }
      },
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('Model3 Accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Train', 'Validation'], loc = 'upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model3 Loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Train', 'Validation'], loc = 'upper left')\n",
        "plt.show()"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaoAAAEXCAYAAAD82wBdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbEAAAEXCAYAAAAjlXpCAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rX4gZs4YB3Kc",
        "outputId": "5f0c1f5c-1e1d-44a6-f23f-4565d3fc3833",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model3.predict_classes(X_test)[5]"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8YADP-ZLB7HA",
        "outputId": "3db7d894-d17b-4ad7-e7c0-b08e828d53ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "#Showing the image\n",
        "plt.imshow(X_test[20].reshape(32, 32), cmap = 'gray')"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f1b39d41470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD1CAYAAAB9TzjVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZnPW2DeGCBf8",
        "outputId": "e54e1008-8f18-432d-a48a-ea67edf64df9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model3.predict_classes(X_test)[20]"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SNYI3rhrCEVv",
        "outputId": "4eea1b8b-a873-4cd1-abfd-d8b675ca95c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "plt.imshow(X_test[10].reshape(32, 32), cmap = 'gray')"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f1b39d22b70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD1CAYAAAB9TzjVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJyRiqOCERAwVnJCI+X8SLdBus2/nIgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "liZmu-GVCPoV",
        "outputId": "6e411bd5-7858-4eb2-cf89-8427359f967c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model3.predict_classes(X_test)[10]"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "k-nx-k4eMKAl"
      },
      "source": [
        "### Conclusion\n",
        "Evaluated and found that Neural Network with relu activations gives 76% accuracy. The final Neural Netowrk achieved best accuracy of 82% on validation set. Also printed the classification report, visualized the confusion matrix and summarized history for accuracy and loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62daDdY2RMKK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}